{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Take-aways\n",
    "\n",
    "After studying this material, we will be able to\n",
    "- explain Gaussian elimination in detail in a computation-oriented way,\n",
    "  - conduct elimination and back substitution for linear systems in a computationally efficient way,\n",
    "  - derive the complexity of the elimination part and back substitution part,\n",
    "  - give the leading term of the complexity for the elimination part and back substitution part,\n",
    "- explain how to solve linear systems using LU decomposition and related facts,\n",
    "  - prove properties of triangular matrices that are related to LU decomposition,\n",
    "  - find LU decomposition of a given matrix when possible,\n",
    "  - solve a linear system when LU decomposition is given,\n",
    "  - give the complexity of LU decomposition,\n",
    "  - explain advantage of LU decomposition compared to plain Gaussian elimination,\n",
    "- explain error amplification via condition numbers of matrices,\n",
    "  - explain and use the notion of vector norm,\n",
    "  - explain and use the notion of matrix norm,\n",
    "  - find condition number of a matrix and what it means,\n",
    "  - explain how error will amplify by examining condition number,\n",
    "- explain how to solve linear systems using LU decomposition with partial pivoting (PA=LU) and related facts,\n",
    "  - TBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation/Settings/Acronyms\n",
    "\n",
    "Common settings\n",
    "\n",
    "| symbol | setting |\n",
    "|---|---|\n",
    "| $n$ | a positive integer |\n",
    "| $A$ | nonsingular $n$-by-$n$ matrix |\n",
    "| $b$ | (column) vector of length $n$ |\n",
    "| $x$ | (column) vector of length $n$ |\n",
    "\n",
    "Acronyms\n",
    "\n",
    "|Abbreviation| meaning|\n",
    "|---|---|\n",
    "| SPD | Symmetric positive definite |\n",
    "\n",
    "Common convention\n",
    "\n",
    "| expression | meaning |\n",
    "|---|---|\n",
    "| $a_{ij}$, $a_{i,j}$, $A_{ij}$, $A_{i,j}$ | $(i,j)$-component of a matrix $A$ ($i$-th row, $j$-th column) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem of interest**\n",
    "\n",
    "Given $A$ and $b$, find $x$ such that\n",
    "\n",
    "$$ Ax = b. $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods\n",
    "\n",
    "- Methods for general matrices\n",
    "   1. Direct methods\n",
    "      - plain Gaussian elimination\n",
    "      - Gaussian elimination using $PA = LU$ decomposition.\n",
    "        - Preliminary: $A=LU$ decomposition\n",
    "   2. Iterative methods\n",
    "      - Jacobi iteration\n",
    "      - Gauss-Seidel iteration\n",
    "- Methods for SPD matrices\n",
    "   1. Direct methods\n",
    "      - Cholesky factorization\n",
    "   2. Iterative methods\n",
    "      - Conjugate gradient method\n",
    "- Framework for improvements\n",
    "   1. Preconditioning\n",
    "\n",
    "**Remark**\n",
    "\n",
    "- Direct method: Gives the exact solution in a finite number of steps. \n",
    "  - Caveat: rounding errors may destroy this nature in practice.\n",
    "- Itervative method: Gives approximate solution every step. \n",
    "  - Theoreicially, true solution is obtained as a limit. \n",
    "  - In practice, a reasonable number of iterations can give a very good approximate solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian elimination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmented matrix\n",
    "\n",
    "Compact rearrangement of a system of linear equations in matrix form\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x+2 y-z & =3 \\\\\n",
    "2 x+y-2 z & =3 \\\\\n",
    "-3 x+y+z & =-6 \n",
    "\\end{aligned}\n",
    "\\leftrightarrow\n",
    "\\left[\\begin{array}{rrr:r}\n",
    "1 & 2 & -1 & 3 \\\\\n",
    "2 & 1 & -2 & 3 \\\\\n",
    "-3 & 1 & 1 & -6\n",
    "\\end{array}\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elimination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Elementary row operations \n",
    "\n",
    "1. Swap one equation (or a row) for another (row): $R_i \\leftrightarrow R_j$.\n",
    "2. Add or subtract a multiple of one equation (or a row) from another (row): $R_i \\gets R_i + c R_j$.\n",
    "3. Multiply an equation (or a row) by a nonzero constant: $R_i \\gets c R_i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Solve the following system of linear equations using the augmented matrix and elementary row operations: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x+2 y-z & =3 \\\\\n",
    "2 x+y-2 z & =3 \\\\\n",
    "-3 x+y+z & =-6 .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "[Example of Gaussian eliminations 1](../images/ex_GaussianElimination1_lp1000.png)\n",
    "\n",
    "[Example of Gaussian eliminations 2](../images/ex_GaussianElimination2_lp1000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back substitution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Remark** (back substitution)\n",
    "\n",
    "- While there can be many creative ways to find the solution, we will following one single way: we are *developing a systematic method*.\n",
    "- Let us call, in this class, the first step *elimination*.\n",
    "- The second step (finding unknowns one by one) is called *back substitution* or *back solving*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complexity of Gaussian elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Step | Complexity (precise) | Complexity (order) | \n",
    "|------|------------|-----|\n",
    "| eliminations | $$ \\frac 2 3 n^3 + \\frac 1 2 n^2 - \\frac 7 6 n $$ | $$=\\mathcal{O}(n^3) $$ | \n",
    "| back substitutions | $$ n^2 $$ |  $$= \\mathcal{O}(n^2) $$ |\n",
    "\n",
    "[Derivation of complexity of eliminations 1](../images/der_ComplexityGaussianEliminations1_lp2000.png)\n",
    "\n",
    "[Derivation of complexity of eliminations 2](../images/der_ComplexityGaussianEliminations2_lp2000.png)\n",
    "\n",
    "[Derivation of complexity of back substitions](../images/der_ComplexityBackSubstitutions_lp2000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A = LU decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition: Gaussian eliminations can be encapsulated in matrix form. (provided there are no issues)\n",
    "\n",
    "- We will see that $L^{-1}$ encodes the elimination while $U$ encodes the result of the elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm** (LU factorization)\n",
    "\n",
    "\n",
    "Algorithm is borrowed from Kincaid and Cheney (2002) p. 155.\n",
    "\n",
    "**Data**\n",
    "\n",
    "- $A=(a_{ij})$: matrix\n",
    "- $n$: size of matrix\n",
    "\n",
    "**Computation**\n",
    "\n",
    "- **for** $k=1$ to $n$ **do**\n",
    "  - $\\ell_{kk} \\gets 1$\n",
    "  - **for** $j=k$ to $n$ **do**\n",
    "    - $u_{k j} \\gets a_{k j}-\\sum_{s=1}^{k-1} \\ell_{k s} u_{s j}$\n",
    "  - **for** $i=k+1$ to $n$ **do**\n",
    "    - $\\ell_{i k} \\leftarrow\\left(a_{i k}-\\sum_{s=1}^{k-1} \\ell_{i s} u_{s k}\\right) / u_{k k}$\n",
    "\n",
    "**Output**\n",
    "\n",
    "- $L=(\\ell_{ij})$\n",
    "- $U=(u_{ij})$\n",
    "\n",
    "**Remark**\n",
    "\n",
    "- This algorithm works only if there is no zero pivot encountered.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Triangular matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Definition**\n",
    "\n",
    "1. A $n$-by-$n$ matrix $L$ is called:*lower triangular* if $\\ell_{ij}=0$ for $i < j$. In addition, if $\\ell_{ij}=1$ for $i = j$, it is called *unit* lower triangular.\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{ccccc}\n",
    "\\ell_{1,1} & & & & 0 \\\\\n",
    "\\ell_{2,1} & \\ell_{2,2} & & & \\\\\n",
    "\\ell_{3,1} & \\ell_{3,2} & \\ddots & & \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\ddots & \\\\\n",
    "\\ell_{n, 1} & \\ell_{n, 2} & \\ldots & \\ell_{n, n-1} & \\ell_{n, n}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "2. A $n$-by-$n$ matrix $U$ is called *upper triangular* if $u_{ij}=0$ for $i > j$. In addition, if $u_{ij}=1$ for $i = j$, it is called *unit* upper triangular.\n",
    "\n",
    "$$\n",
    "U=\\left[\\begin{array}{ccccc}\n",
    "u_{1,1} & u_{1,2} & u_{1,3} & \\ldots & u_{1, n} \\\\\n",
    "& u_{2,2} & u_{2,3} & \\ldots & u_{2, n} \\\\\n",
    "& & \\ddots & \\ddots & \\vdots \\\\\n",
    "& & & \\ddots & u_{n-1, n} \\\\\n",
    "0 & & & & u_{n, n}\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Properties of triangular matrices**\n",
    "\n",
    "**Fact** (From linear algebra)\n",
    "\n",
    "- If an $n$-by-$n$ matrix $A$ is invertible, then the eigenvalues of $A^{-1}$ are precisely the inverse of eigenvalues of $A$. \n",
    "- Determinant of a triangular matrix is the product of its diagonal entries.\n",
    "- The eigenvalues of a lower triangular matrix are precisely its diagonal entries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Theorem** (Triangular matrices and their algebraic structure)\n",
    "\n",
    "Lower triangular shape is preserved under addition, scalar multiplication, matrix multiplication, and inversion. More specifically, \n",
    "\n",
    "1. If $L_1$ and $L_2$ are lower triangular matrices of size $n$-by-$n$, then $L_1 + L_2$ also lower triangular. \n",
    "2. If $L_1$ is a lower triangular matrix of size $n$-by-$n$ and $\\alpha$ is a scalar, then $\\alpha L_1$ is also lower triangular. \n",
    "3. If $L_1$ and $L_2$ are lower triangular matrices of size $n$-by-$n$, then $L_1 L_2$  also lower triangular. Furthermore, $[L_1 L_2]_{ii}=[L_1]_{ii}[L_2]_{ii}$ for $i=1,2,\\cdots,n$\n",
    "   - If $L_1$ and $L_2$ are unit lower triangular matrices of size $n$-by-$n$, then $L_1 L_2$  also unit lower triangular. \n",
    "4. If $L_1$ is a lower triangular matrix of size $n$-by-$n$ and it is invertible, then $L_1^{-1}$ is also lower triangular. Furthermore, $[L_1^{-1}]_{ii}=[L_1]_{ii}^{-1}$.\n",
    "   - If $L_1$ is a unit lower triangular matrix of size $n$-by-$n$ and it is invertible, then $L_1^{-1}$ is also unit lower triangular. \n",
    "\n",
    "The same is true for upper triangular matrices.\n",
    "\n",
    "[Proof of properties of triangular matrices 1](../images/pf_PropTriangularMatrices1_lp2000.png)\n",
    "\n",
    "[Proof of properties of triangular matrices 2](../images/pf_PropTriangularMatrices2_lp2000.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmas for LU decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 1 for LU** (matrix of row subtraction)\n",
    "\n",
    "The elementary row operation $R_{i} \\gets R_{i}+(-c)R_{j}$ can be represented by a matrix multiplication by $L_{ij}(-c)$ on the left, where\n",
    "\n",
    "$$\n",
    "[L_{ij}(-c)]_{k \\ell} = \\begin{cases}\n",
    "1 & (k = \\ell) \\\\\n",
    "-c & (k = i, \\ \\ell = j) \\\\\n",
    "0 & (\\text{otherwise}),\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "or, \n",
    "\n",
    "$$\n",
    "L_{i j}(-c)=\\left[\\begin{array}{ccccccc}\n",
    "1 & & & & & & \\\\\n",
    "& \\ddots & & & & & \\\\\n",
    "& & 1 & & & & \\\\\n",
    "& & & \\ddots & & & \\\\\n",
    "& & -c & & 1 & & \\\\\n",
    "& & & & & \\ddots & \\\\\n",
    "& & & & & & 1\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 2 for LU** (Product of row subtraction)\n",
    "\n",
    "Let $L_{ij}(c_{ij})$ be defined as above. If $j$ is fixed, then, we have \n",
    "\n",
    "$$\n",
    "\\left[\\prod_{i=j+1}^n L_{ij}(c_{ij})\\right]_{k \\ell} = \\begin{cases}\n",
    "1 & (k = \\ell) \\\\\n",
    "c_{ij} & (k = i, \\ \\ell = j) \\\\\n",
    "0 & (\\text{otherwise}),\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "or, \n",
    "\n",
    "$$\n",
    "\\prod_{i=j+1}^n L_{ij}(c_{ij})\n",
    "=\\left[\\begin{array}{ccccccc}\n",
    "1 & & & & & & \\\\\n",
    "& \\ddots & & & & & \\\\\n",
    "& & 1 & & & & \\\\\n",
    "& & c_{j+1,j} & \\ddots & & & \\\\\n",
    "& & c_{j+2,j} & & 1 & & \\\\\n",
    "& & \\vdots & & & \\ddots & \\\\\n",
    "& & c_{n,j} & & & & 1\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "For example in $4$-by-$4$ case with $j=1$, \n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{cccc}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "c_{21} & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{cccc}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "c_{31} & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{cccc}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "c_{41} & 0 & 0 & 1 \\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{cccc}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "c_{21} & 1 & 0 & 0 \\\\\n",
    "c_{31} & 0 & 1 & 0 \\\\\n",
    "c_{41} & 0 & 0 & 1 \\\\\n",
    "\\end{array}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 3 for LU** (Inverse of row reduction)\n",
    "\n",
    "Let $L$ be an $n$-by-$n$ lower triangular matrix whose diagonal elements are all 1, and only one column has nonzero elements below diagonal. Then, $A^{-1}$ is of the same form as $A$ except the signs of elements below diagonal being flipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This script verifies the inversion of a triangular matrix.\n",
    "\n",
    "1. If only one column has nonzero element below the diagonal,\n",
    "    then the matrix inversion is mechanical.\n",
    "2. If more columns has nonzero elements below the diagonal,\n",
    "    then inversion is not that simple. \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "AA = np.eye(3)\n",
    "AA[1, 0] = 2\n",
    "AA[2, 0] = -5\n",
    "# A[2, 1] = 3 # uncomment this line to see case 2\n",
    "\n",
    "B = AA.copy()\n",
    "low_diag_ind = np.tril_indices_from(B, -1)\n",
    "B[low_diag_ind] = - AA[low_diag_ind]\n",
    "\n",
    "print(\"A: \\n\", AA)\n",
    "print(\"\\nB: \\n\", B)\n",
    "print(\"\\nA*B: \\n\", AA@B)\n",
    "print(\"\\nA^-1:\\n\", np.linalg.inv(AA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 4 for LU** (Product of elementary matrix)\n",
    "\n",
    "The following patterns generalizes to any size $n$-by-$n$ as long as\n",
    "\n",
    "1. each matrix is unit lower triangular,\n",
    "2. each matrix has at most one column that is filled with nonzero entries below diagonal, and\n",
    "3. the order is kept, namely, the matrix with a column of smaller index is multiplied more to the left.\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{cccc}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "c_{21} & 1 & 0 & 0 \\\\\n",
    "c_{31} & 0 & 1 & 0 \\\\\n",
    "c_{41} & 0 & 0 & 1 \\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{cccc}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & c_{32} & 1 & 0 \\\\\n",
    "0 & c_{42} & 0 & 1 \\\\\n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array}{cccc}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & c_{43} & 1 \\\\\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{cccc}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "c_{21} & 1 & 0 & 0 \\\\\n",
    "c_{31} & c_{32} & 1 & 0 \\\\\n",
    "c_{41} & c_{42} & c_{43} & 1 \\\\\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This script verifies product of elementary triangular matrices.\n",
    "\n",
    "1. If the matrix with earlier column filled is multiplied more \n",
    "    to the left, the product is as easy as writing out.\n",
    "2. If not, this property is lost.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "AA = np.eye(3)\n",
    "AA[1, 0] = 2\n",
    "AA[2, 0] = -5\n",
    "# A[2, 1] = 3 # uncomment this line to see case 2\n",
    "\n",
    "B = np.eye(3)\n",
    "B[2, 1] = 3\n",
    "\n",
    "\n",
    "print(\"A: \\n\", AA)\n",
    "print(\"\\nB: \\n\", B)\n",
    "print(\"\\nA*B: \\n\", AA@B)\n",
    "print(\"\\nB*A: \\n\", B@AA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- Prove these lemmas very carefully. Or at least verify them carefully. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding LU decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Finding LU decomposition)\n",
    "\n",
    "1. Represent the Gaussian elimination of the following system of linear equations using elementary lower triangular matrices, and\n",
    "2. find $A=LU$ decomposition, where $L$ is lower unit triangular and $U$ is upper triangular.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x+2 y-z & =3 \\\\\n",
    "2 x+y-2 z & =3 \\\\\n",
    "-3 x+y+z & =-6 .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "[Example of LU decomposition 1](../images/ex_LUdecomposition1_lp2000.png)\n",
    "\n",
    "[Example of LU decomposition 2](../images/ex_LUdecomposition2_lp2000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving linear system given LU decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once $A=LU$ is obtained, we solve $Ax=b$ via two steps.\n",
    "\n",
    "1. Solve $Lc = b$ for $c$, then\n",
    "2. solve $Ux = c$ for $x$.\n",
    "\n",
    "Reason\n",
    "\n",
    "From $Ax = LUx = b$, we have $Ux = L^{-1}b =: c$, hence $x=U^{-1}c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- Both steps can be computed efficiently because $L$ and $U$ are both triangular, hence their inversion is nothing but a (back or forward) substibution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Solving a linear system given LU)\n",
    "\n",
    "Given \n",
    "\n",
    "$$\n",
    "A \n",
    "= \\left[\\begin{array}{rrr}\n",
    "1 & 2 & -1 \\\\\n",
    "2 & 1 & -2 \\\\\n",
    "-3 & 1 & 1\n",
    "\\end{array}\\right]\n",
    "=\\left[\\begin{array}{rrr}\n",
    "1 & 0 & 0 \\\\\n",
    "2 & 1 & 0 \\\\\n",
    "-3 & -\\frac{7}{3} & 1\n",
    "\\end{array}\\right]\\left[\\begin{array}{rrr}\n",
    "1 & 2 & -1 \\\\\n",
    "0 & -3 & 0 \\\\\n",
    "0 & 0 & -2\n",
    "\\end{array}\\right]\n",
    "=L U\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "b = [3, 3, -6]^T,\n",
    "$$\n",
    "\n",
    "solve $Ax=b$.\n",
    "\n",
    "[Example of solving linear system given LU](../images/ex_SolvingLinSystemGivenLU_lp2000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complexity comparisons: Gaussian elimination and LU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Though the Gaussian elimination and LU decomposition use the same idea, they are quite different in practical manner. \n",
    "\n",
    "| | Gaussian elimination | LU factorization |\n",
    "|---|---|---|\n",
    "| $b$ | included in the augmented matrix | not included |\n",
    "| approximate complexity <br> for $Ax=b$ | $$ \\frac 2 3 n^3 $$ | $$ \\frac 2 3 n^3$$ |\n",
    "| approximate complexity <br> for multiple problems: <br> $Ax_i=b_i$ <br> ($i=1,2,\\cdots,k$)| $$ \\frac 2 3 k n^3 $$ | $$ \\frac 2 3 n^3 + 2 k n^2$$ |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- The above table summarizes approximate complexity rather than very precise ones. The reason we have $2kn^2$ for multiple problems using LU decomposition (second order term rather than third order compared to a single problem) is that we need to compute two more substitions ($Lc=b$ and $Ux=c$) for each problem. On the other hand, for Gaussian elimination, we have to recompute all over again for each problem, hence complexity $(2/3) kn^3$.\n",
    "- The multiple problems scenario ($Ax_i=b_i$ for $i=1,2,\\cdots,k$) is common in applications because it is often the case $A$ comes from discretizing the integral or differential operator that governs the application and $b$ comes from different data. For example, accoring to Sauer (2017), in structural engineering, $b$ is called *loading vector* and the solution $x$ gives *stress*. We may want to see how stress looks like for many different loading vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source of errors and conditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n",
    "We want to study how errors amplify in the course of finding solutions. For this, we need to clarify what we mean by the size of a vector. Also, can we extract a single number that summarizes the behavior of a matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Vector norms)\n",
    "\n",
    "Given a vector space $(V, +, \\cdot)$ over a field $\\mathbb{F}$ (i.e., $\\mathbb{R}$ or $\\mathbb{C}$), we call a function $\\| \\ \\cdot \\ \\|:V \\to [0,\\infty)$ a *norm* if it satisfies the following. (You may focus only on more concrete setting, say $\\mathbb{R}^{n}$ with usual addition and scalar multiplication over $\\mathbb{R}$.)\n",
    "\n",
    "- $\\| x \\| \\ge 0$ and $\\| x \\| = 0$ only if $x=0$ (zero vector),\n",
    "- for each scalar $\\alpha$ and a vector $x$, we have $\\| \\alpha x \\| = |\\alpha| \\| x \\|$,\n",
    "- for any vectors $x,y$, triangle inequality holds: $\\|x + y \\| \\le \\| x \\| + \\| y \\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Operator norm)\n",
    "\n",
    "Let $A:V \\to W$ be a map, where $V, W$ is a (finite dimensional) vector space that a norm is defined, $\\|\\ \\cdot \\ \\|_V$ and $\\|\\ \\cdot \\ \\|_W$ respectively. Then, the *operator norm* of $A$ is defined by\n",
    "\n",
    "$$\n",
    "\\| A \\| := \\mathrm{max} \\{ \\frac{\\| A x \\|_W }{\\| x\\|_V} \\ : \\ x \\in V, x \\neq 0\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Operator norm of a matrix)\n",
    "\n",
    "- Matrix multiplication can be thought of a map between $\\mathbb{R}^{n}$. Hence, it makes sense that a matrix has a natural norm derived from the operator norm.\n",
    "- Operator norm depends on the vector norm being used.\n",
    "- If $V=W$, we usually choose the same norm even though it is possible to couple two different norms for $\\| A \\|$. In this case, we omit the subscript and write $\\| x \\|$ for $x \\in V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminology** (Consistency of matrix norm)\n",
    "\n",
    "By the definition of the operator norm, we have, for any vector $x$,\n",
    "\n",
    "$$\n",
    "\\| A x \\|_W \\le \\| A \\| \\| x\\|_V,\n",
    "$$\n",
    "\n",
    "or if $A$ maps between the same space $V$ with the same vector norm $\\| \\ \\cdot \\ \\|$,\n",
    "\n",
    "$$\n",
    "\\| A x \\| \\le \\| A \\| \\| x\\|.\n",
    "$$\n",
    "\n",
    "Referring to this property, we say that $\\|\\ A \\|$ are *consistent* with vector norms appearing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Norms on $\\mathbb{R}^{n}$ and square matrices)\n",
    "\n",
    "Given $x=(x_1, x_2, \\cdots, x_n)\\in {\\mathbb{R}^n}$ and a matrix $A \\in {\\mathbb{R}^{n\\times n}}$, \n",
    "\n",
    "- Vector norm\n",
    "\n",
    "$$\n",
    "\\| x \\|_\\infty := \\mathrm{max}\\{ |x_i| \\ : \\ 1 \\le i \\le n \\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\| x \\|_1 := \\sum_{i=1}^n |x_i|,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\| x \\|_2 := \\sqrt{\\sum_{i=1}^n |x_i|^2},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\| x \\|_p := \\left(\\sum_{i=1}^n |x_i|^p \\right)^{1/p}, \\quad \\text{ where } p \\ge 1\n",
    "$$\n",
    "\n",
    "- Matrix norm\n",
    "\n",
    "$$\n",
    "\\| A \\|_\\infty := \\mathrm{max}_{1\\le i \\le n} \\{ \\| r_i \\|_1 \\ : \\ r_i \\text{ is the }i\\text{-th row of } A  \\}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\| A \\|_1 := \\mathrm{max}_{1\\le i \\le n} \\{ \\| c_i \\|_1 \\ : \\ c_i \\text{ is the }i\\text{-th column of } A  \\}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\| A \\|_2 := \\mathrm{max} \\{ \\sqrt{\\lambda} \\ : \\ \\lambda \\text{ is an eigenvalue of } A^T A \\}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- There are many other matrix norms. But we focus on most basic norms.\n",
    "- It can be shown that $\\| A \\|_\\infty$ is the operator norm of $A$ when we use $\\| x \\|_\\infty$ for vector norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error magnification and condition number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation** (Temporary abuse of norm notation)\n",
    "\n",
    "- In the following examples, we are going to use general norm notation, $\\| A \\|$ and $\\| x \\|$ since they indeed work in general norm settings. However, when we carry out concrete calculations, we will compute the infinity norms. The reason for this is pedagogical: the infinity norms are easy to compute whille still conveying the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Residual, backward error, forward error)\n",
    "\n",
    "Given a linear system $Ax=b$ and an approximate solution $\\tilde x$, \n",
    "\n",
    "- $r:=b-A \\tilde x$ is called *residual*,\n",
    "- $\\| r \\| = \\| b-A \\tilde x \\|$ is called *backward error*,\n",
    "- $\\| x - \\tilde x \\|$ is called *forward error*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- Many people say residual when then really mean backward error if there is no confusion. \n",
    "- Many people simply say *the error* when they refer to the forward error.\n",
    "- We can measure the errors and backward errors in various norms. But, in this class, we will use infinity norms in the examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Forward and backward errors)\n",
    "\n",
    "Find the forward and backward errors for the approximate solution $[-1, 3.0001]$ of the system\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1+x_2 & =2 \\\\\n",
    "1.0001 x_1+x_2 & =2.0001.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "[Example: Forward and backward error](../images/ex_ForBackErrLinSystem_lp2000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Relative backward/forward error and error magnification factor)\n",
    "\n",
    "Given a linear system $Ax=b$, an approximate solution $\\tilde x$, the error $e=x-\\tilde x$, and the residual $r=b-A\\tilde x$, \n",
    "\n",
    "- the *relative backward error* is defined by\n",
    "\n",
    "$$\n",
    "\\frac{\\|r\\|}{\\|b\\|}\n",
    "$$\n",
    "\n",
    "- the relative forward error is defined by\n",
    "\n",
    "$$\n",
    "\\frac{\\left\\|e \\right\\|}{\\|x\\|}\n",
    "$$\n",
    "\n",
    "- the *error magnification factor* for $Ax=b$ is defined by\n",
    "\n",
    "$$\n",
    "\\frac{\\text { relative forward error }}{\\text { relative backward error }}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Condition number)\n",
    "\n",
    "The condition number $\\mathbf{cond}(A)$ of a square matrix $A$ is the maximum possible error magnification factor for solving $Ax=b$, over all right hand side $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Condition number)\n",
    "\n",
    "For a square matrix, we have\n",
    "\n",
    "$$\n",
    "\\mathbf{cond}(A)=\\| A \\| \\| A^{-1} \\|.\n",
    "$$\n",
    "\n",
    "[Proof: Condition number 1](../images/pf_CondNumMatrix1_lp3000.png)\n",
    "\n",
    "[Proof: Condition number 2](../images/pf_CondNumMatrix2_lp3000.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** \n",
    "\n",
    "Given the following linear system with the approximate solution $[-1, 3.0001]$, find relative backward error, relative forward error, and error magnification factor.\n",
    "Also, find the largest possible error magnification factor for the following system. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1+x_2 & =2 \\\\\n",
    "1.0001 x_1+x_2 & =2.0001.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "[Example: Relative forward and backward error](../images/ex_RelErrMagnFacLinSystem1_lp2000.png)\n",
    "\n",
    "[Example: Error maginfication factor](../images/ex_RelErrMagnFacLinSystem2_lp2000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Comments on condition numbers)\n",
    "\n",
    "- Fortunately, large condition numbers are unusual. (Sauer (2017) p. 94)\n",
    "- However, in many applications, numerically implemented differential operators often result in a linear system with rapidly growing condition number as the discretization gets finer (on top of larger size of the linear system)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Motivating example**\n",
    "\n",
    "Recall the homework problem, where we solved the following linear system by programming 2D version of Gaussian elimination.\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "10^{-20} x_1+x_2 & =1 \\\\\n",
    "x_1+2 x_2 & =4\n",
    "\\end{aligned}\n",
    "\\right. \n",
    "$$\n",
    "\n",
    "If we programmed \"right,\" it should produce $(x_1, x_2)\\approx(0,1)$, while the package `numpy.linalg.solve` gave $(x_1, x_2)\\approx(2,1)$. \n",
    "\n",
    "What is going on? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Remark**\n",
    "\n",
    "- The source of error is that the first coefficient `A[0,0]` is so small that the computer (using IEEE double precision) \"thinks\" it is 0. \n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "10^{-20} x_1+x_2 & =1 \\\\\n",
    "x_1+2 x_2 & =4\n",
    "\\end{aligned}\n",
    "\\right. \n",
    "\\Rightarrow\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "0 x_1+x_2 & =1 \\\\\n",
    "x_1+2 x_2 & =4\n",
    "\\end{aligned}\n",
    "\\right. \n",
    "$$\n",
    "\n",
    "Then, the first equation leads to $x_2=1$, which in turn, leads to $x_1 = 0$. Hence, we have $(x_1, x_2)\\approx(0,1)$. \n",
    "\n",
    "- However, if we carry the small coefficient $10^{-20}$ and meticulously solve the equation, we obtain $(x_1, x_2)\\approx(2,1)$. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Idea** (Swapping or partial pivoting)\n",
    "\n",
    "- Push down a row whose pivot is small, and pull up the one with the largest coefficient in that column.\n",
    "- This procedure is called *partial pivoting*. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PA = LU decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n",
    "1. Not all matrices have $LU$ factorization.\n",
    "2. One of the main sources of error can be resolved by swapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** ($LU$ may not exist)\n",
    "\n",
    "The following matrix does not have $LU$ factorization.\n",
    "\n",
    "$$\n",
    "A \n",
    "= \\left[\\begin{array}{rr}\n",
    "0 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "[Example of impossibility of LU factorization](../images/ex_LUimpossible_lp2000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea**\n",
    "\n",
    "Search a good pivot candidate and swap the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm** (Partial pivoting)\n",
    "\n",
    "Given an $n$-by-$n$ matrix $A$, the partial pivoting conducted on the $j$-th column reads:\n",
    "\n",
    "1. find $p\\in\\{j, j+1, \\cdots, n\\}$ such that $|a_{pj}| \\ge |a_{ij}|$ for all $i\\in\\{j, j+1, \\cdots, n\\}$.\n",
    "2. Exchange the rows $R_j \\leftrightarrow R_p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm** (LU decomposition with partial pivoting)\n",
    "\n",
    "Given an $n$-by-$n$ matrix $A$,\n",
    "\n",
    "- **for** $j=1,2,\\cdots,n-1$, **do**\n",
    "  - conduct partial pivoting on $j$-th column (and record the rows exchanged)\n",
    "  - eliminate subdiagonal entries of $j$-th column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Consequence of partial pivoting)\n",
    "\n",
    "- The multipliers involved in elimination is always less than or equal to 1 in absolute value: $|c_{ij}|\\le 1$ in $L_{ij}(c_{ij})$ notation of row reduction.  \n",
    "- Equivalently, every entry of $L$ is less than or equal to 1 in absolute value. (Recall that the subdiagonal entries of $L$ are the multipliers of row reduction.)\n",
    "- Partial pivoting also deals with encounters of 0 pivot (by swapping with other row with nonzero entry on the column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** ($PA = LU$ decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def p_pivot(A, j, verbose=True):\n",
    "    \"\"\" Retrun a matrix of partial-pivoted.\n",
    "    \n",
    "    Input:\n",
    "        A: a square matrix\n",
    "        j: the column index to be pivoted. Indexing starts from 0.\n",
    "    Output:\n",
    "        None: the matrix is modified in place due to \"pass by reference\".\n",
    "    \"\"\"\n",
    "\n",
    "    n = A.shape[0]\n",
    "    p = np.argmax(np.abs(A[j:, j])) + j\n",
    "    if p != j:\n",
    "        tmp = A[p].copy()\n",
    "        A[p] = A[j]\n",
    "        A[j] = tmp\n",
    "        if verbose:\n",
    "            print(f\"Rows exchanged: {j} <--> {p}.\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"No row exchange.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elim_col(A, j, verbose=True):\n",
    "    \"\"\" Eliminate the j-th column of A.\n",
    "    \n",
    "    Input:\n",
    "        A: a square matrix\n",
    "        j: the column index to be eliminated. Indexing starts from 0.\n",
    "    Output:\n",
    "        None: the matrix is modified in place due to \"pass by reference\".\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Eliminating column {j}.\")\n",
    "\n",
    "    n = A.shape[0]\n",
    "    for i in range(j+1, n):\n",
    "        m = A[i, j]/A[j, j]\n",
    "        A[i] = A[i] - m*A[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Illustrate Gaussian elimination with partial pivoting.\"\"\"\n",
    "\n",
    "A = np.array([[1, -1, 3], \n",
    "              [-1, 0, -2], \n",
    "              [2, 2, 4]], dtype=np.float64)\n",
    "b = np.array([-3, 1, 0]).reshape(-1, 1)\n",
    "\n",
    "# Augmented matrix\n",
    "M = np.hstack((A, b))\n",
    "print(\"Augmented matrix: \\n\", M)\n",
    "\n",
    "p_pivot(M, 0)\n",
    "print(M)\n",
    "elim_col(M, 0)\n",
    "print(M)\n",
    "\n",
    "p_pivot(M, 1)\n",
    "print(M)\n",
    "elim_col(M, 1)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Permutation matrix)\n",
    "\n",
    "A permutation matrix is a square matrix consisting of all zeros, except for a single 1 in every row and column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Permutation matrix)\n",
    "\n",
    "(2-by-2)\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{ll}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{array}\\right],\\left[\\begin{array}{ll}\n",
    "0 & 1 \\\\\n",
    "1 & 0\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "(3-by-3) \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& {\\left[\\begin{array}{lll}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{array}\\right],\\left[\\begin{array}{lll}\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{array}\\right],\\left[\\begin{array}{lll}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{array}\\right]} \\\\\n",
    "& {\\left[\\begin{array}{lll}\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{array}\\right],\\left[\\begin{array}{lll}\n",
    "0 & 0 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{array}\\right],\\left[\\begin{array}{lll}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{array}\\right]}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Permutation matrix and row exchange)\n",
    "\n",
    "Let $P$ be the $n$-by-$n$ permutation matrix formed by a particular set of row exchanges applied to the $n$-by-$n$ identity matrix. Then, for any $n$-by-$n$ matrix $A$, $P A$ is the matrix obtained by applying exactly the same set of row exchanges to $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Illustrate Gaussian elimination with partial pivoting.\"\"\"\n",
    "\n",
    "A = np.arange(1, 10).reshape(3, 3)\n",
    "I = np.eye(3)\n",
    "\n",
    "P = I.copy()\n",
    "P[1] = I[2]\n",
    "P[2] = I[1]\n",
    "\n",
    "print(\"A: \\n\", A)\n",
    "print(\"\\nP: \\n\", P)\n",
    "print(\"\\nPA: \\n\", P@A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PA = LU factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** ($PA =LU$ factorization)\n",
    "\n",
    "Suppose $A$ is *any* $n$-by-$n$ matrix. Then, there exists a permutation matrix $P$, a unit lower triangular matrix $L$, and an upper triangular matrix $U$ such that\n",
    "\n",
    "$$\n",
    "PA = LU,\n",
    "$$\n",
    "\n",
    "where all matrices appearing are of size $n$-by-$n$. Furthermore, if $A$ is invertible, then $U$ has nonzero diagonal entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Proof of $PA=LU$ factorization; can be skipped in undergraduate class)\n",
    "\n",
    "- We skip the proof. While there are some steps that not not obvious, the main intuition is explain in the algorithm given above.\n",
    "- The proof hinges on the following facts.\n",
    "  - If a permutation matrxi is obtained by exchanging only two rows of identity matrix, it is called a *simple permutation*.\n",
    "  - A simple permutation matrix is its own inverse.\n",
    "  - Partial pivoting involves only simple permutations of rows below diagonals of the column where the elimination are going on.\n",
    "  - These two ensures the inversion of row reduction matrices is kept simple just as in $LU$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Implementation of $PA=LU$)\n",
    "\n",
    "- We only need to store a vector, call it $s$, representing the partial pivoting, not the whole permutation matrix.\n",
    "- We can store $L$ and $U$ in one matrix $\\tilde U = U + L - I$. \n",
    "  - Use the empty space of $U$ to put subdigonal entries of $L$.\n",
    "  - We already know the diagonal entries of $L$ is always 1.\n",
    "- The embedded entries of $L$ move correctly along subsequent swapping if they are inserted upon row reduction done. (See Sauer (2017) pp. 102-103)\n",
    "- We can *read off* $PA=LU$ from $\\tilde U$ and $s$ thanks to the previous properties. (reading off $P$ needed to be checked.)\n",
    "- We can use $s$ when we conduct back substitution to recover the correct order of unknowns.\n",
    "- Following these, all bookkeeping of elimination and pivoting are automatic and contained in the matrix equation $PA=LU$. (Sauer (2017) p. 103)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Finding $PA=LU$ factorization)\n",
    "\n",
    "Find the $PA=LU$ factorization of \n",
    "\n",
    "$$\n",
    "A=\\left[\\begin{array}{rrr}\n",
    "2 & 1 & 5 \\\\\n",
    "4 & 4 & -4 \\\\\n",
    "1 & 3 & 1\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving system from $PA=LU$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to solve $Ax=b$ while we have information $PA=LU$. By multiplying $Ax=b$ by $P$ on the left, we have $P A x=P b$, or\n",
    "\n",
    "$$\n",
    "L U x=P b.\n",
    "$$\n",
    "\n",
    "Hence, we can do the same things as in $LU$ factorization without partial pivoting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm** (Solve $Ax=b$ given $PA=LU$)\n",
    "\n",
    "1. Solve $Lc=Pb$ for $c$.\n",
    "2. Solve $Ux=c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- The first step in the above recovers the correct order from swapping conducted on $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Solving a system using $PA=LU$)\n",
    "\n",
    "Find the solution of $Ax=b$, where\n",
    "\n",
    "$$\n",
    "A=\\left[\\begin{array}{rrr}\n",
    "2 & 1 & 5 \\\\\n",
    "4 & 4 & -4 \\\\\n",
    "1 & 3 & 1\n",
    "\\end{array}\\right], \\quad b=\\left[\\begin{array}{l}\n",
    "5 \\\\\n",
    "0 \\\\\n",
    "6\n",
    "\\end{array}\\right] \n",
    "$$\n",
    "\n",
    "and $PA=LU$ decomposition\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{lll}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{rrr}\n",
    "2 & 1 & 5 \\\\\n",
    "4 & 4 & -4 \\\\\n",
    "1 & 3 & 1\n",
    "\\end{array}\\right]=\\left[\\begin{array}{rrr}\n",
    "1 & 0 & 0 \\\\\n",
    "\\frac{1}{4} & 1 & 0 \\\\\n",
    "\\frac{1}{2} & -\\frac{1}{2} & 1\n",
    "\\end{array}\\right]\\left[\\begin{array}{rrr}\n",
    "4 & 4 & -4 \\\\\n",
    "0 & 2 & 2 \\\\\n",
    "0 & 0 & 8\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Application of LU decomposition\n",
    "\n",
    "- Solving systems of linear equations\n",
    "- Determinant\n",
    "- Inverting matrices\n",
    "\n",
    "Reference: [Wikipedia](https://en.wikipedia.org/wiki/LU_decomposition#Applications)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrch",
   "language": "python",
   "name": "ptrch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
