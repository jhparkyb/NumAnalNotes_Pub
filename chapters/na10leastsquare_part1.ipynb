{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take-aways\n",
    "\n",
    "- Least square\n",
    "  - What we can do with it.\n",
    "  - Why do we care?\n",
    "  - Why it works.\n",
    "  - How to find them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Settings/Notation\n",
    "\n",
    "Common settings\n",
    "\n",
    "| symbol | setting |\n",
    "|---|---|\n",
    "| $m$ | a positive integer; number of observations; number of equations |\n",
    "| $n$ | a positive integer; number of parameters; number of unknowns |\n",
    "| $A$ | full rank $m$-by-$n$ matrix with $m \\gg n$ (tall matrix) |\n",
    "| $b$ | (column) vector of length $m$ |\n",
    "| $x$ | (column) vector of length $n$ |\n",
    "\n",
    "$$\n",
    "\\| x \\| = \\| x \\|_2=\\sqrt{x_1^2 +\\cdots + x_n^2} \\quad \\text{(2-norm)}\n",
    "$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clicker question 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Problem of interest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem** (Least square)\n",
    "\n",
    "> Given an over-determined (or inconsistent) system $Ax=b$, find the \"best\" $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we care?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The least square method gives a new way of answering interpolation problem.\n",
    "- The least square method gives answers to the following situations.\n",
    "  - The \"equality\" conditions that $x$ should satisfy are only within some margin of errors (Sauer 2017)\n",
    "  - Using aggregated data (hence too many conditions to satisfy) to determine a better solution. \n",
    "    - E.g.: Measurements of celestial bodies. In fact, predicting the orbit of an ateroid Ceres was at the heart of the invention of the method. ([Wikipedia page on history of least square](https://en.wikipedia.org/wiki/Least_squares#History))\n",
    "- The least square method inspires many other statistical methods and signal processing.\n",
    "  - Ridge regression, also known as Tikhonov regularization (LS + $\\ell^2$ penalty)\n",
    "  - LASSO (LS + $\\ell^1$ penalty)\n",
    "  - Elastic net (LS + $\\ell^1$ penalty + $\\ell^2$ penalty)\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| ![determine an elliptical orbit using least square](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1f/X33-ellips-1.svg/1920px-X33-ellips-1.svg.png) <br> Figure: determine an elliptical orbit using least square (Wikipedia) | ![fitting parabola model with many data](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Linear_least_squares2.svg/1024px-Linear_least_squares2.svg.png) <br> Figure: fitting parabola model with many data (Wikipedia) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we can do\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting data by least square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose a model $\\longrightarrow$ dimension and meaning of $x$\n",
    "2. Force the model to fit the data $\\longrightarrow$ $Ax=b$ \n",
    "3. Solve the normal equation $\\longrightarrow$ compute $x$ such that $A^T Ax = A^T b$\n",
    "4. Diagnostics $\\longrightarrow$ plots and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main idea of least square method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** \n",
    "\n",
    "Find \"the best\" solution to \n",
    "$$\n",
    "\\begin{aligned}\n",
    "& x_1+x_2=2 \\\\\n",
    "& x_1-x_2=1 \\\\\n",
    "& x_1+x_2=3\n",
    "\\end{aligned} \n",
    "\\Longleftrightarrow\n",
    "\\left[\\begin{array}{rr}\n",
    "1 & 1 \\\\\n",
    "1 & -1 \\\\\n",
    "1 & 1\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
    "2 \\\\\n",
    "1 \\\\\n",
    "3\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation 1**\n",
    "\n",
    "- Change the perspective: row $\\longleftrightarrow$ column.\n",
    "  - Matrix muliplication $\\Longleftrightarrow$ Linear combination of column vectors\n",
    "\n",
    "$$\n",
    "x_1\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array}\\right]+x_2\\left[\\begin{array}{r}\n",
    "1 \\\\\n",
    "-1 \\\\\n",
    "1\n",
    "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
    "2 \\\\\n",
    "1 \\\\\n",
    "3\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $b$ were in the column space (i.e., some linear combination of columns equals $b$), it would have been solvable. But $b$ is outside the column space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation 2**\n",
    "\n",
    "\"The best\" candidate for $x$ is the one that gives the **projection** of $b$ onto the column space.\n",
    "\n",
    "- Projection is always related to smallest distance in some metric.\n",
    "  - Here, the metric is Euclidean distance, i.e., 2-norm.\n",
    "- Now, resulting the projected vector is a possible task while the original r.h.s. vector was impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![geometric intuition behind LS](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Linear_least_squares_geometric_interpretation.png/543px-Linear_least_squares_geometric_interpretation.png)\n",
    "\n",
    "Figure: Geometric intuition behind Least Square method (Wikipedia)\n",
    "\n",
    "Notation adjustments\n",
    "\n",
    "- $\\beta \\gets x$\n",
    "- $A \\gets X$\n",
    "- $y \\gets b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\bar x$ be the vector of unknowns that gives the projection of $b$. Then, we have, from the picture,\n",
    "\n",
    "$$\n",
    "(b-A \\bar{x}) \\perp \\underbrace{\\left\\{A c \\mid c \\in R^n\\right\\}}_{\\text{column space}}\n",
    "$$\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "$$\n",
    "\\underbrace{(A c)^T}_{\\text{lin. comb.\\\\col's}}(b-A \\bar{x})=0 \\text { for all } c \\text { in } R^n\n",
    "$$\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "$$\n",
    "c^T A^T(b-A \\bar{x})=0 \\text { for all } c \\text { in } R^n.\n",
    "$$\n",
    "\n",
    "This leads to the normal equation because $\\vec 0 \\in R^n$ is the only vector that is orthongonal to arbitrary vectors $c$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal equation**\n",
    "\n",
    "$$\n",
    "A^T A \\bar{x}=A^T b\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fact**\n",
    "\n",
    "- $\\bar x = \\mathrm{argmin}_{x\\in\\mathbb{R}^n} \\| b - Ax \\|_2$. In other words, the least square solution minimizes the 2-norm of the residual. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diagnostics\n",
    "\n",
    "| Terminology | Definition |\n",
    "|---|---|\n",
    "| Residual |  $$r=b-A \\bar x=(r_1, \\cdots, r_m)$$ |\n",
    "| Squared error (SE) | $$r_1^2+\\cdots+r_m^2$$ |\n",
    "| Mean squared error (MSE) |  $$\\mathrm{SE} / m= \\left(r_1^2+\\cdots+r_m^2\\right) / m$$ |\n",
    "| root mean squared error (RMSE) |  $$\\sqrt{MSE}=\\sqrt{\\left(r_1^2+\\cdots+r_m^2\\right) / m}$$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Residual and backward errors)\n",
    "\n",
    "- The above quantities are all some measure of **backward** errors $b-A \\bar x$, not the real (i.e., forward) errors $x-\\bar x$. (It seems that the nomenclature used in statistics and mathematics communities are different.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** \n",
    "\n",
    "Find \"the best\" solution to \n",
    "$$\n",
    "\\begin{aligned}\n",
    "& x_1+x_2=2 \\\\\n",
    "& x_1-x_2=1 \\\\\n",
    "& x_1+x_2=3\n",
    "\\end{aligned} \n",
    "\\Longleftrightarrow\n",
    "\\left[\\begin{array}{rr}\n",
    "1 & 1 \\\\\n",
    "1 & -1 \\\\\n",
    "1 & 1\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
    "2 \\\\\n",
    "1 \\\\\n",
    "3\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# System of linear equations\n",
    "A = np.array([[1, 1], [1, -1], [1, 1]], dtype=np.float64)\n",
    "b = np.array([2, 1, 3], dtype=np.float64)\n",
    "\n",
    "# naively try to solve the system\n",
    "# x = np.linalg.solve(A, b)\n",
    "\n",
    "# solve normal equation\n",
    "x = np.linalg.solve(A.T @ A, A.T @ b)\n",
    "\n",
    "print(f\"{'x':<10}{': '}{x}\")\n",
    "print(f\"{'residual':<10}{': '}{b - A @ x}\")\n",
    "print(f\"Dot prodoct of col1 and residual: {A[:,0].T @ (b - A @ x)}\")\n",
    "print(f\"Dot prodoct of col2 and residual: {A[:,1].T @ (b - A @ x)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** \n",
    "\n",
    "Find the best line for the four data points (−1, 1), (0, 0),\n",
    "(1,0),(2,−2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decide a model\n",
    "2. Force the model to fit data: Write $Ax=b$.\n",
    "3. Solve normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data points\n",
    "data = np.array([[-1, 1], [0, 0], [1,0], [2, -2]])#, dtype=np.float64)\n",
    "print(data)\n",
    "print(type(data[0,0]))\n",
    "data[0,0] = 4.5\n",
    "print(data[0,0])\n",
    "\n",
    "# number of data points/observations\n",
    "m = data.shape[0]\n",
    "\n",
    "# vector of unknowns \n",
    "n = 2\n",
    "\n",
    "# matrix of least squares\n",
    "\n",
    "# vector of observations\n",
    "\n",
    "# Solve normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI usage: initial plotting block is doen with copilot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decide a model\n",
    "   - linear ($c_1 + c_2 t$)\n",
    "2. Force the model to fit data: Write $Ax=b$.\n",
    "   - Be clear about settings.\n",
    "3. Solve normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings/Notations** (A subtask in Step 2)\n",
    "\n",
    "- Given\n",
    "  - Generic least square $Ax=b$\n",
    "  - Our problem involvs\n",
    "    - $c=[c_1, c_2]$,\n",
    "    - $(x_i, y_i)$ for $1\\le i \\le 4$, \n",
    "    - $t$\n",
    "\n",
    "1. Write out several equations by imposing conditions.\n",
    "2. Match the roles of symbols.\n",
    "\n",
    "- Settings\n",
    "\n",
    "| Generic LS | Our problem | Shape (math-oriented, not Python) |\n",
    "|---|---|---|\n",
    "| $x$ |  $c=[c_1, c_2]$ <br> (Unknown/Solution vector) | $n\\times 1$ ($n=2$) |\n",
    "| $A$ | $x_i^0$, $x_i^1$, $x_i^2$ <br> (coefficents matrix) | $m\\times n$ ($m=4$, $n=2$) | \n",
    "| $b$ | $y_i$ <br> (r.h.s. vector) | $m\\times 1$ ($m=4$) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data points\n",
    "data = np.array([[-1, 1], [0, 0], [1,0], [2, -2]], dtype=np.float64)\n",
    "\n",
    "# number of data points/observations\n",
    "m = data.shape[0]\n",
    "\n",
    "# vector of unknowns (model 1)\n",
    "n = 2\n",
    "c = np.zeros(n)\n",
    "\n",
    "# matrix of least squares (model 1)\n",
    "A = np.zeros((m, n))\n",
    "ones = np.ones(m)\n",
    "A = np.column_stack((ones, data[:, 0]))\n",
    "\n",
    "# vector of observations\n",
    "b = data[:, 1]\n",
    "\n",
    "# least squares solution (model 1)\n",
    "c = np.linalg.solve(A.T @ A, A.T @ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI usage: initial plotting block is doen with copilot\n",
    "\n",
    "# Calculate the predicted values\n",
    "y_pred = c[0] + c[1] * data[:, 0]\n",
    "\n",
    "# Calculate the root mean squared error\n",
    "rmse = np.sqrt(np.mean((data[:, 1] - y_pred)**2))\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(data[:, 0], data[:, 1], label='Data')\n",
    "\n",
    "# Plot the linear model\n",
    "x = np.linspace(-2, 3, 100)\n",
    "y = c[0] + c[1] * x\n",
    "plt.plot(x, y, color='red', label='Linear Model')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Linear Model with Least Square Solution\\nRoot Mean Squared Error: {rmse:.3g}')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to be careful of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditioning of normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- The condition number $\\mathrm{cond}(A^T A)$ is approximately the square of the original $\\mathrm{cond}(A)$ (Sauer (2017) p. 204)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (ill-conditioned least square problem) Sauer 2017)\n",
    "\n",
    "Let $x_0=2.0, x_2=2.2, x_3=2.4, \\ldots, x_{10}=4.0$ be equally spaced points in $[2,4]$, and set $y_i=1+x_i+x_i^2+x_i^3+x_i^4+x_i^5+x_i^6+x_i^7$ for $0 \\leq i \\leq 10$. Use the normal equations to find the least squares polynomial $P(x)=c_1+c_2 x+\\cdots+c_8 x^7$ fitting the $\\left(x_i, y_i\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose a model $\\longrightarrow$ dimension and meaning of $c$\n",
    "\n",
    "Find best solution among the following (model) with $c=(c_0, \\cdots, c_7)\\in\\mathbb{R}^8$\n",
    "\n",
    "$$\n",
    "P(x)=c_0+c_1 x+\\cdots+c_7 x^7\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Force the model to fit the data $\\longrightarrow$ $Ac=y$ \n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{ccccc}\n",
    "1 & x_0 & x_0^2 & \\cdots & x_0^7 \\\\\n",
    "1 & x_1 & x_1^2 & \\cdots & x_1^7 \\\\\n",
    "\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & x_{10} & x_{10}^2 & \\cdots & x_{10}^7\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "c_0 \\\\\n",
    "c_1 \\\\\n",
    "\\vdots \\\\\n",
    "c_7\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "y_0 \\\\\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_{10}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "where $x_0=2.0, x_2=2.2, x_3=2.4, \\ldots, x_{10}=4.0$ and \n",
    "\n",
    "$$\n",
    "y_i=1+x_i+x_i^2+x_i^3+x_i^4+x_i^5+x_i^6+x_i^7 \\quad (i\\in\\{0,1,\\cdots,10\\})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Solve the normal equation $\\longrightarrow$ compute $x$ such that $A^T Ax = A^T b$\n",
    "\n",
    "Use package\n",
    "\n",
    "4. Diagnostics $\\longrightarrow$ plots and statistics\n",
    "\n",
    "Compare with the true solution since it is known in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from internallib import poly_eval\n",
    "\n",
    "# problem parameters\n",
    "# Suggestions: To check sanity, set n = 2, 3, 4, \n",
    "#   Things get wild soon after n = 5\n",
    "n = 8       # number of unknowns\n",
    "\n",
    "# right-hand side vector\n",
    "x = np.linspace(2., 4., 11)\n",
    "c_true = np.ones(n)\n",
    "y = poly_eval(c_true, x)\n",
    "\n",
    "# matrix\n",
    "pow = np.arange(n)\n",
    "# A = x[:, np.newaxis] ** pow       # broadcasting in effect\n",
    "A = x.reshape(-1, 1) ** pow         # equivalent to the previous line\n",
    "\n",
    "# vector of unknowns\n",
    "c = np.zeros(n)\n",
    "\n",
    "# least squares solution (model 1 and 2)\n",
    "c = np.linalg.solve(A.T @ A, A.T @ y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'c_true':<10}{': '}{c_true}\")\n",
    "print(f\"{'c':<10}{': '}{c}\")\n",
    "print(f\"{'y':<10}{': '}{y}\")\n",
    "print(f\"{'x':<10}{': '}{x}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- This motivates QR factorization, which finds the least square solution with better conditioning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survey of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (revisited)\n",
    "\n",
    "Find the best best **parabola** for the four data points (−1, 1), (0, 0),\n",
    "(1,0),(2,−2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decide a model\n",
    "   - linear ($c_1 + c_2 t$)\n",
    "   - **parabola** ($c_1 + c_2 t + c_3 t^2$)\n",
    "2. Force the model to fit data: Write $Ax=b$.\n",
    "   - Be clear about settings.\n",
    "3. Solve normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings/Notations** (A subtask in Step 2)\n",
    "\n",
    "- Settings\n",
    "\n",
    "| Generic LS | Our problem | Shape (math-oriented, not Python) |\n",
    "|---|---|---|\n",
    "| $x$ | $[c_1, c_2, c_3]$ <br> (Unknown/Solution vector) | $n\\times 1$ ($n=3$) |\n",
    "| $A$ | $x_i^0$, $x_i^1$, $x_i^2$ <br> (coefficents matrix) | $m\\times n$ ($m=4$, $n=3$) | \n",
    "| $b$ | $y_i$ <br> (r.h.s. vector) | $m\\times 1$ ($m=4$) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data points\n",
    "data = np.array([[-1, 1], [0, 0], [1,0], [2, -2]], dtype=np.float64)\n",
    "\n",
    "# number of data points/observations\n",
    "m = data.shape[0]\n",
    "\n",
    "# vector of unknowns (model 2)\n",
    "n = 3\n",
    "c = np.zeros(n)\n",
    "\n",
    "# matrix of least squares (model 2)\n",
    "A = np.zeros((m, n))\n",
    "ones = np.ones(m)\n",
    "A = np.column_stack((ones, data[:, 0], data[:, 0]**2))\n",
    "\n",
    "# vector of observations\n",
    "b = data[:, 1]\n",
    "\n",
    "# least squares solution (model 2)\n",
    "c = np.linalg.solve(A.T @ A, A.T @ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI usage: initial plotting block is doen with copilot\n",
    "\n",
    "# Calculate the predicted values\n",
    "y_pred = c[0] + c[1] * data[:, 0] + c[2] * data[:, 0]**2\n",
    "\n",
    "# Calculate the root mean squared error\n",
    "rmse = np.sqrt(np.mean((data[:, 1] - y_pred)**2))\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(data[:, 0], data[:, 1], label='Data')\n",
    "\n",
    "# Plot the parabola model\n",
    "x = np.linspace(-2, 3, 100)\n",
    "y = c[0] + c[1] * x + c[2] * x**2\n",
    "plt.plot(x, y, color='green', label='Parabola Model')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Parabola Model with Least Square Solution\\nRoot Mean Squared Error: {rmse:.3g}')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing nonlinear model functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Sauer (2017) p. 209)\n",
    "\n",
    "Fit the recorded temperatures in Washington, D.C., on January 1, 2001, as listed in\n",
    "the following table, to a periodic model:\n",
    "\n",
    "| time of day | $t$ | temp $(\\mathrm{C})$ |\n",
    "| :---: | :---: | :---: |\n",
    "| 12 mid. | 0 | -2.2 |\n",
    "| 3 am | $\\frac{1}{8}$ | -2.8 |\n",
    "| $6 \\mathrm{am}$ | $\\frac{1}{4}$ | -6.1 |\n",
    "| $9 \\mathrm{am}$ | $\\frac{3}{8}$ | -3.9 |\n",
    "| 12 noon | $\\frac{1}{2}$ | 0.0 |\n",
    "| $3 \\mathrm{pm}$ | $\\frac{5}{8}$ | 1.1 |\n",
    "| $6 \\mathrm{pm}$ | $\\frac{3}{4}$ | -0.6 |\n",
    "| $9 \\mathrm{pm}$ | $\\frac{7}{8}$ | -1.1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choose the model\n",
    "   - $y=c_1+c_2 \\cos 2 \\pi t+c_3 \\sin 2 \\pi t$\n",
    "2. Force the model to fit the data: Write out $Ax=b$\n",
    "   - Be clear about settings. \n",
    "3. Solve normal equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings/Notations** (A subtask in Step 2)\n",
    "\n",
    "- Settings\n",
    "  - $m$: (usually) No. observations $\\longrightarrow$ $b$\n",
    "  - $n$: d.o.f. (dimension of solution) $\\longrightarrow$ $x$\n",
    "  - $m\\times n$: coefficent matrix $\\longrightarrow$ $A$\n",
    "\n",
    "| Generic LS | Our problem | Shape (math-oriented, not Python) |\n",
    "|---|---|---|\n",
    "| $x$ | $[c_1, c_2, c_3]$ <br> (Unknown/Solution vector) | $n\\times 1$ ($n=3$) |\n",
    "| $A$ | $1$, $\\cos(2\\pi t_i)$, $\\sin(2\\pi t_i)$ <br> (coefficents matrix) | $m\\times n$ ($m=4$, $n=3$) | \n",
    "| $b$ | $y_i$ <br> (r.h.s. vector) | $m\\times 1$ ($m=4$) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# number of data points/observations\n",
    "m = 8\n",
    "\n",
    "# data \n",
    "temp = np.array([-2.2, -2.8, -6.1, -3.9, 0., 1.1, -0.6, -1.1])\n",
    "t = np.arange(m)*(1./m)\n",
    "\n",
    "# vector of unknowns (model 2)\n",
    "n = 3\n",
    "c = np.zeros(n)\n",
    "\n",
    "# matrix of least squares (model 2)\n",
    "A = np.zeros((m, n))\n",
    "ones = np.ones(m)\n",
    "A = np.column_stack((ones, np.cos(2*np.pi*t), np.sin(2*np.pi*t)))\n",
    "\n",
    "# vector of observations\n",
    "b = temp\n",
    "\n",
    "# least squares solution (model 2)\n",
    "c = np.linalg.solve(A.T @ A, A.T @ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI usage: initial plotting block is doen with copilot\n",
    "\n",
    "# Calculate the predicted values\n",
    "y_pred = A @ c\n",
    "\n",
    "# Calculate the root mean squared error\n",
    "rmse = np.sqrt(np.mean((b - y_pred)**2))\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(t, b, label='Data')\n",
    "\n",
    "# Plot the model\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = c[0] + c[1] * np.cos(2*np.pi*x) + c[2] *  np.sin(2*np.pi*x)\n",
    "plt.plot(x, y, color='green', label='Periodic Model')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Periodic Model with Least Square Solution\\nRoot Mean Squared Error: {rmse:.3g}')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data linearization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Sauer (2017) p. 212)\n",
    "\n",
    "Use model linearization to find the best least squares exponential fit $y = c_1 e^{c_2 t}$ to the\n",
    "following world automobile supply data:\n",
    "\n",
    "| year | cars $\\left(\\times 10^6\\right)$ |\n",
    "| :---: | :---: |\n",
    "| 1950 | 53.05 |\n",
    "| 1955 | 73.04 |\n",
    "| 1960 | 98.31 |\n",
    "| 1965 | 139.78 |\n",
    "| 1970 | 193.48 |\n",
    "| 1975 | 260.20 |\n",
    "| 1980 | 320.39 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Data linearization)\n",
    "\n",
    "- Exponential growth is a reasonable model for population.\n",
    "  - The more individuals are, the more new born individuals will be (at least for a short term).\n",
    "  - That is, the rate of change of the population is proportional to the population itself. And exponential function has this property.\n",
    "- \"Force the model to fit the data\" does not work: It results in a nonlinear equations.\n",
    "  - Direction 1: Take nonlinear least square problem.\n",
    "  - Direction 2: (Data linearization) Change the problem to obtain linear equations.\n",
    "    - In the current example, take logarithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Take a transformation and rename quantities approprately to obtain a linear problem.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\ln y &=\\ln \\left(c_1 e^{c_2 t}\\right)=\\ln c_1+c_2 t = k +c_2 t\n",
    "\\\\\n",
    "k &= \\ln c_1\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- new unknown vector: $[k, c_2]$\n",
    "- new data: $\\ln(\\text{cars})$\n",
    "\n",
    "Step 2: Solve normal equation\n",
    "\n",
    "Step 3: Check the result with $c_1= e^k$ and $c_2$ in the setting before transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# number of data points/observations\n",
    "m = 7\n",
    "y = np.array([53.05, 73.04, 98.31, 139.78, 193.48, 260.20, 320.39])\n",
    "# 1950 is treated as 0\n",
    "t = np.arange(m)*5.\n",
    "\n",
    "# vector of unknowns\n",
    "c = np.zeros(2)\n",
    "\n",
    "# matrix of least squares\n",
    "A = np.column_stack((np.ones(m), t))\n",
    "\n",
    "# vector of observations\n",
    "b = np.log(y)\n",
    "\n",
    "# least squares solution\n",
    "c = np.linalg.solve(A.T @ A, A.T @ b)\n",
    "\n",
    "# invert the transoformation for c_1\n",
    "c[0] = np.exp(c[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI usage: initial plotting block is doen with copilot\n",
    "\n",
    "# Calculate the predicted values\n",
    "y_pred = c[0]*np.exp(c[1]*t)\n",
    "\n",
    "# Calculate the root mean squared error\n",
    "rmse = np.sqrt(np.mean((b - y_pred)**2))\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(t, y, label='Data')\n",
    "\n",
    "# Plot the model\n",
    "x = np.linspace(t[0], t[-1], 100)\n",
    "y_ = c[0]*np.exp(c[1]*x)\n",
    "plt.plot(x, y_, color='green', label='Exponential Model')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Car Population growth\\n(Data linearized with log)')\n",
    "plt.ylim(0, np.max(y)*1.1)\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Consequence of data linearization)\n",
    "\n",
    "- Model linearization changes the least squares problem. \n",
    "  - The solution minimize the RMSE with respect to the linearized problem, not the original problem.\n",
    "    - Original problem: minimize $\\left(c_1 e^{c_2 t_1}-y_1\\right)^2+\\cdots+\\left(c_1 e^{c_2 t_m}-y_m\\right)^2$ over $[c_1, c_2]\\in\\mathbb{R}^2$.\n",
    "    - Linearized problem: minimize $\\left(\\ln c_1+c_2 t_1-\\ln y_1\\right)^2+\\cdots+\\left(\\ln c_1+c_2 t_m-\\ln y_m\\right)^2$ over $[c_1, c_2]\\in\\mathbb{R}^2$.\n",
    "  - The solutions of these two are different in general.\n",
    "  - This means, clear meaning of the solution is no longer available: It mimimizes something (i.e., a distorted distance), but we are not sure whether that quantity is the best quantity to minimize. \n",
    "- The answer to which method is \"correct\" depends on the context of the data. \n",
    "  - To answer the question, the user needs to decide which errors are most important to minimize. (Sauer (2017) p. 212)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Important preliminary\n",
    "\n",
    "- Projections (Linear algebra)\n",
    "- Gradient of vector field (Vector calculus 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- (1st index matching requirement on $A$ and $b$) The number of rows of $A$ and the lenth of $b$ must match. \n",
    "- The number of columns of $A$ must match the length of the solution $x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "This work is licensed under [Creative Commons Attribution-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
