{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem of interest**\n",
    "\n",
    "Given $m$-by-$m$ matrix $A$, find eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings/Notation**\n",
    "\n",
    "| symbol | meaning |\n",
    "|---|---|\n",
    "| $A$ | matrix whose eigenvalues are sought ($m$-by-$m$ or $n$-by-$n$)  |\n",
    "| $\\lambda_i$ | eigenvalues in decreasing order in modulus $\\vert \\lambda_1 \\vert \\ge \\vert \\lambda_2 \\vert \\ge \\cdots$ |\n",
    "| $v_i$ | eigenvectors associated to $\\lambda_i$ |\n",
    "| $\\sigma(A)$ | spectrum of $A$, i.e., the set of eigenvalues |\n",
    "| $M_n(R)$, $M_n(\\mathbb{C})$| set of all $n$-by-$n$ real and complex matrices respectively | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- Charateristic polynomial is not a good approach due to ill-conditioned nature of root-finding of polynomials.\n",
    "  - For $A=\\mathrm{diag}(1,2,\\cdots,20)$, we have a characteristic polynomial: $\\mathrm{det}(xI - A)=(x-1)(x-2)\\cdots(x-20)$ (Wilkinson polynomial).\n",
    "  - This is so ill-conditioned, no root-finding method can find the roots in a satisfactory manner. (Try it!) \n",
    "- There is no direct method for computing eigenvalues ([Sauer (2017) p. 556]). --> iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Gershgorin disks; Salgado and Wise (2022) p. 200)\n",
    "\n",
    "Let $n \\geq 2$ and $\\mathrm{A} \\in \\mathbb{C}^{n \\times n}$. The Gershgorin disks $D_i$ of $\\mathrm{A}$ are\n",
    "$$\n",
    "D_i=\\left\\{z \\in \\mathbb{C}|| z-a_{i, i} \\mid \\leq R_i\\right\\}, \\quad R_i=\\sum_{\\substack{j=1 \\\\ j \\neq i}}^n\\left|a_{i, j}\\right|, \\quad i=1, \\ldots, n,\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Gershgorin Circle Theorem; Salgado and Wise (2022) p. 200) \n",
    "\n",
    "Let $n \\geq 2$ and $\\mathrm{A}=\\left[a_{i, j}\\right] \\in \\mathbb{C}^{n \\times n}$. Then\n",
    "$$\n",
    "\\sigma(\\mathrm{A}) \\subset \\bigcup_{i=1}^n D_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gershgorin disks](https://www.wolfram.com/language/12/complex-visualization/assets.en/gerschgorin-disks/O_47.png)\n",
    "\n",
    "Figure: Wolfram (Gershgorin disks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that $(\\lambda, \\boldsymbol{w})$ is an eigenpair of $A$. Then\n",
    "$$\n",
    "\\sum_{j=1}^n a_{i, j} w_j=\\lambda w_i, \\quad i=1,2, \\ldots, n .\n",
    "$$\n",
    "\n",
    "Suppose that\n",
    "$$\n",
    "\\left|w_k\\right|=\\|\\boldsymbol{w}\\|_{\\infty}=\\max _{i=1}^n\\left|w_i\\right| .\n",
    "$$\n",
    "\n",
    "Observe that $w_k \\neq 0$, since $\\boldsymbol{w} \\neq \\mathbf{0}$. Then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left|\\lambda-a_{k, k}\\right| \\cdot\\left|w_k\\right| & =\\left|\\lambda w_k-a_{k, k} w_k\\right| \\\\\n",
    "& =\\left|\\sum_{j=1}^n a_{k, j} w_j-a_{k, k} w_k\\right| \\\\\n",
    "& =\\left|\\sum_{\\substack{j=1 \\\\\n",
    "j \\neq k}}^n a_{k, j} w_j\\right| \\\\\n",
    "& \\leq \\sum_{\\substack{j=1 \\\\\n",
    "j \\neq k}}^n\\left|a_{k, j} w_j\\right| \\\\\n",
    "& \\leq \\sum_{\\substack{j=1 \\\\\n",
    "j \\neq k}}^n\\left|a_{k, j}\\right| \\cdot\\left|w_k\\right| \\\\\n",
    "& =R_k\\left|w_k\\right| .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, $\\lambda \\in D_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- The theorem does not say one-to-one correspondence between a disk and an eigenvalue.\n",
    "  - $k$ is simply the index of largest entry in modulus of $\\boldsymbol{w}$, and two eigenvectors may have the largest entry at the index.\n",
    "- But the next theorem gives such an impression, showing the subtlety of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Gershgorin Second Theorem; Salgado and Wise (2022) p. 201). \n",
    "\n",
    "Let $n \\geq 2$ and $\\mathrm{A} \\in \\mathbb{C}^{n \\times n}$. Suppose that $1 \\leq p \\leq n-1$ and that the Gershgorin disks of the matrix A can be divided into disjoint subsets $D^{(p)}$ and $D^{(q)}$ containing $p$ and $q=n-p$ disks, respectively. Then the union of the disks in $D^{(p)}$ contains $p$ eigenvalues, and the union of the disks in $D^{(q)}$ contains $q$ eigenvalues, counting multiplicities. In particular, if one disk is disjoint from all the others, it contains exactly one eigenvalue. And, if all of the disks are disjoint, then each contains exactly one eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminology**\n",
    "\n",
    "- Dominant eigenvalue: $\\lambda_1$ (eigenvalue of the largest modulus)\n",
    "- Dominant eigenvector: $v_1$ (eigenvector associated to the dominant eigenvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea**\n",
    "\n",
    "Repeat application of $A$ turns a vector toward its dominant eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Power iteration](https://www.gastonsanchez.com/matrix4sl/matrix4sl_files/figure-html/power-method-example-1.png)\n",
    "\n",
    "Figure: Gaston Sanchez (Illustration of Power iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Let $A=\\left[\\begin{array}{ll}1 & 3 \\\\ 2 & 2\\end{array}\\right]$, and $x_0=\\left[\\begin{array}{r}-5 \\\\ 5\\end{array}\\right]$. Then,\n",
    "\n",
    "$$\n",
    "\\begin{aligned} & x_1=A x_0=\\left[\\begin{array}{ll}1 & 3 \\\\ 2 & 2\\end{array}\\right]\\left[\\begin{array}{r}-5 \\\\ 5\\end{array}\\right]=\\left[\\begin{array}{r}10 \\\\ 0\\end{array}\\right] \\\\ & x_2=A^2 x_0=\\left[\\begin{array}{ll}1 & 3 \\\\ 2 & 2\\end{array}\\right]\\left[\\begin{array}{r}10 \\\\ 0\\end{array}\\right]=\\left[\\begin{array}{l}10 \\\\ 20\\end{array}\\right] \\\\ & x_3=A^3 x_0=\\left[\\begin{array}{ll}1 & 3 \\\\ 2 & 2\\end{array}\\right]\\left[\\begin{array}{l}10 \\\\ 20\\end{array}\\right]=\\left[\\begin{array}{l}70 \\\\ 60\\end{array}\\right] \\\\ & x_4=A^4 x_0=\\left[\\begin{array}{ll}1 & 3 \\\\ 2 & 2\\end{array}\\right]\\left[\\begin{array}{l}70 \\\\ 60\\end{array}\\right]=\\left[\\begin{array}{l}250 \\\\ 260\\end{array}\\right]=260\\left[\\begin{array}{r}\\frac{25}{26} \\\\ 1\\end{array}\\right] .\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "More vividly, use the knowledge of eigenpairs\n",
    "\n",
    "$$\n",
    "(\\lambda_1, v_1) = (4, [1,1]^T), \\qquad (\\lambda_2, v_2) = (−1, [−3,2]^T),\n",
    "$$\n",
    "\n",
    "and expand the initial vector\n",
    "\n",
    "$$\n",
    "x_0=1\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array}\\right]+2\\left[\\begin{array}{r}\n",
    "-3 \\\\\n",
    "2\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "Then, the above calculations read:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_1=A x_0 & =4\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array}\\right]-2\\left[\\begin{array}{r}\n",
    "-3 \\\\\n",
    "2\n",
    "\\end{array}\\right] \\\\\n",
    "x_2=A^2 x_0 & =4^2\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array}\\right]+2\\left[\\begin{array}{r}\n",
    "-3 \\\\\n",
    "2\n",
    "\\end{array}\\right] \\\\\n",
    "x_3=A^3 x_0 & =4^3\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array}\\right]-2\\left[\\begin{array}{r}\n",
    "-3 \\\\\n",
    "2\n",
    "\\end{array}\\right] \\\\\n",
    "x_4=A^4 x_0 & =4^4\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array}\\right]+2\\left[\\begin{array}{r}\n",
    "-3 \\\\\n",
    "2\n",
    "\\end{array}\\right] \\\\\n",
    "& =256\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{array}\\right]+2\\left[\\begin{array}{r}\n",
    "-3 \\\\\n",
    "2\n",
    "\\end{array}\\right] .\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we introduce normalization, we can focus on the eigen-direction itself. (See the algorithm below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Power iteration](https://www.gastonsanchez.com/matrix4sl/matrix4sl_files/figure-html/power-method-rescale-1.png)\n",
    "\n",
    "Figure: Gaston Sanchez (Illustration of Power iteration with normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm** (Power iteration; Sauer (2017) p. 559)\n",
    "\n",
    "- Given\n",
    "  - $A$: matrix\n",
    "  - $x_0$: initial guess vector\n",
    "- **For** $j=1,2,\\cdots$\n",
    "  - $u_{j-1}=x_{j-1} /\\left\\|x_{j-1}\\right\\|_2$\n",
    "  - $x_j=A u_{j-1}$\n",
    "  - $\\lambda_j=u_{j-1}^T x_j$\n",
    "- Return\n",
    "  - $u_{j}=x_{j} /\\left\\|x_{j}\\right\\|_2$\n",
    "  - $\\lambda_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** \n",
    "\n",
    "Implement Power Iteration with a 4-by-4 symmetric matrix, $A$, and an asymmetric one, $B$.\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}  \n",
    "                1 & 2 & 3 & 4 \\\\\n",
    "                4 & 5 & 6 & 7 \\\\\n",
    "                2 & 1 & 5 & 0 \\\\\n",
    "                4 & 2 & 1 & 0 \n",
    "    \\end{bmatrix}\n",
    "    \\quad\n",
    "B = \\begin{bmatrix} \n",
    "                1 & 2 & 2 & 4 \\\\\n",
    "                2 & 5 & 6 & 2 \\\\\n",
    "                2 & 6 & 5 & 0 \\\\\n",
    "                4 & 2 & 0 & 0 \n",
    "    \\end{bmatrix}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# DATA\n",
    "#   1. asymmetric matrix\n",
    "A = np.array([  [1 , 2 , 3 , 4],\n",
    "                [4 , 5 , 6 , 7],\n",
    "                [2 , 1 , 5 , 0],\n",
    "                [4 , 2 , 1 , 0]], dtype=np.float64)\n",
    "\n",
    "#   2. symmetric matrix\n",
    "B = np.array([  [1 , 2 , 2 , 4],\n",
    "                [2 , 5 , 6 , 2],\n",
    "                [2 , 6 , 5 , 0],\n",
    "                [4 , 2 , 0 , 0]], dtype=np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_iter(A, x0=None, max_iter=20):\n",
    "    \"\"\"\n",
    "    Return an approximate eigenvector associated with \n",
    "    the largest eigenvalue in modulus.\n",
    "\n",
    "    Input:\n",
    "        A (2D array): matrix of interest\n",
    "        x0 (1D array): initial guess\n",
    "        max_iter (int): maximum number of iteration of power iteration\n",
    "    Output:\n",
    "\n",
    "    \"\"\"\n",
    "    # initial guess\n",
    "    if x0 is None:\n",
    "        x = np.zeros(A.shape[1])\n",
    "        x[0] = 1.               # x = [1., 0, 0, ..., 0]\n",
    "    else:\n",
    "        x = x0\n",
    "    \n",
    "    # main loop of Power iteration\n",
    "    for k in range(max_iter):\n",
    "        u = x / np.linalg.norm(x)\n",
    "        x = A @ u\n",
    "        lamb = np.dot(u, x)    # same as u^T A u: Rayleigh quotient\n",
    "    \n",
    "    u = x / np.linalg.norm(x)\n",
    "\n",
    "    return lamb, u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicker question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.97490979e-09 -1.42785606e-08 -6.35508535e-10  5.48972601e-09]\n"
     ]
    }
   ],
   "source": [
    "# Power iteration with A and B\n",
    "lamb, u = power_iter(A, max_iter=20)\n",
    "\n",
    "print(A@u - lamb*u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Rayleigh quotient)\n",
    "\n",
    "Given an $m$-by-$m$ matrix $A$ and a vector $x$ of length $m$,  \n",
    "\n",
    "$$\n",
    "R(A, x)=\\frac{x^T A x}{x^T x}\n",
    "$$\n",
    "\n",
    "is called *Rayleigh quotient*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- $R(A, x)$ is the best approximation of an eigenvalue provided $x$ points to a similar direction to an eigenvector.\n",
    "  - If $x$ is an eigenvector associated to an eigenvalue $\\lambda$, \n",
    "\n",
    "$$\n",
    "Ax=\\lambda x \\implies x^T Ax=\\lambda x^T x \\implies \\lambda =\\frac{x^T A x}{x^T x}.\n",
    "$$\n",
    "\n",
    "- Least square interpretation (Sauer (2017) p. 559)\n",
    "  - Consider the eigenvalue equation $x\\lambda = Ax$, where x is an approximate eigenvector and $\\lambda$ is unknown. Looked at this way, the coefficient matrix is the n × 1 matrix $x$. The normal equations say that the least squares answer is the solution of $x^T x\\lambda = x^T Ax$, or $\\lambda=(x^T Ax)/(x^Tx)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Convergence of Power iteration; Sauer (2017 p. 560))\n",
    "\n",
    "Let $A$ be an $m \\times m$ matrix with real eigenvalues $\\lambda_1, \\ldots, \\lambda_m$ satisfying $\\left|\\lambda_1\\right|>\\left|\\lambda_2\\right| \\geq$ $\\left|\\lambda_3\\right| \\geq \\cdots \\geq\\left|\\lambda_m\\right|$. Assume that the eigenvectors of $A$ span $R^m$. For almost every initial vector, Power Iteration converges linearly to an eigenvector associated to $\\lambda_1$ with convergence rate constant $S=\\left|\\lambda_2 / \\lambda_1\\right|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the first inequality is strict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spectral theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several versions of spectral theorems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Hermitian matrix)\n",
    "\n",
    "A complex matrix is called *Hermitian* if $A^H=A$, where $A^H = \\overline{A}^T$ (conjugate transpose). In particular, if $A$ is a real matrix, Hermitian and symmetric are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Horn and Johnson (2013) Matrix analysis 2ed. Theorem 4.1.5. p. 229) \n",
    "\n",
    "A matrix $A \\in M_n$ is Hermitian if and only if there is a unitary $U \\in M_n$ and a real diagonal $\\Lambda \\in M_n$ such that $A=U \\Lambda U^*$, where $M_n$ is the set of $n$-by- $n$ complex matrices. Moreover, $A$ is real and Hermitian (that is, real symmetric) if and only if there is a real orthogonal $P \\in M_n$ and a real diagonal $\\Lambda \\in M_n$ such that $A=P \\Lambda P^T$.\n",
    "\n",
    "**Remark**\n",
    "\n",
    "- Observe the subtlety of the statement: If $A$ is symmetric as a complex matrix, then the conclusion is different. (See e.g., [Wikipedia - Complex symmetric matrices](https://en.wikipedia.org/wiki/Symmetric_matrix#Complex_symmetric_matrices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Spectral theorem for symmetric matrix; Lay, Lay, McDonald (2014) Linear Algebra and its applications. p. 399)\n",
    "\n",
    "An $m \\times m$ symmetric matrix $A$ has the following properties:\n",
    "- $A$ has $m$ real eigenvalues, counting multiplicities.\n",
    "- The dimension of the eigenspace for each eigenvalue $\\lambda$ equals the multiplicity of $\\lambda$ as a root of the characteristic equation.\n",
    "- The eigenspaces are mutually orthogonal, in the sense that eigenvectors corresponding to different eigenvalues are orthogonal.\n",
    "- $A$ is orthogonally diagonalizable.\n",
    "  - Hence, there is a orthonormal basis for $R^m$ consisting of eigenvectors of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverse power iteration and shifted Inverse power iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma** (HW00 #3 of Math 104B 2024 Winter)\n",
    "\n",
    "Let $A\\in R^{m\\times m}$ be nonsingular. Then, $\\lambda$ is an eigenvalue of $A$ if and only if $\\lambda^{-1}$ is an eigenvalue of $A^{-1}$. Also, the corresponding eigenvectors are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma** (HW07 #2 of Math 104B 2024 Winter)\n",
    "\n",
    "$\\lambda$ is an eigenvalue of $A\\in R^{m\\times m}$ if and only if $\\kappa\\lambda+\\mu$ is an eigenvalue of $\\kappa A+\\mu I$, where $\\kappa(\\neq0), \\mu\\in R$. Also, the eigenvectors of $A$ and $\\kappa A+\\mu I$ associated with $\\lambda$ and $\\kappa\\lambda + \\mu$, respectively, are the same.\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation 1**\n",
    "\n",
    "- Replace $A$ with $A^{-1}$, \n",
    "  - $x_j=A u_{j-1}$ $\\xrightarrow{\\text{replace}}$ $x_j=A^{-1} u_{j-1}$ $\\Longleftrightarrow$ solve $Ax_j=u_{j-1}$ for $x_j$\n",
    "  - This will result in the largest eigenvalue of $A^{-1}$ in modulus, i.e., the smallest eigenvalue of $A$ in modulus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation 2** \n",
    "\n",
    "- Further replace $A^{-1}$ with $(A-sI)^{-1}$,\n",
    "  - solve $Ax_j=u_{j-1}$ for $x_j$ $\\xrightarrow{\\text{replace}}$ solve $(A-sI)x_j=u_{j-1}$ for $x_j$\n",
    "  - This will result in the smallest eigenvalue of $A-sI$ in modulus among $|\\lambda_1 - s|, |\\lambda_2 - s|, \\cdots, |\\lambda_k - s|$. In other words, the *closest* eigenvalue to $s$.\n",
    "  - If we have good candidates of eigenvalues, we can take them as $s$, and find the eigenvalues nearby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm** ((shifted) Inverse Power iteration; Sauer (2017) p. 561)\n",
    "\n",
    "- Given\n",
    "  - $A$: matrix\n",
    "  - $x_0$: initial guess vector\n",
    "  - $s$: shift\n",
    "- **For** $j=1,2,\\cdots$\n",
    "  - $u_{j-1}=x_{j-1} /\\left\\|x_{j-1}\\right\\|_2$\n",
    "  - Solve $(A-sI)x_j=u_{j-1}$\n",
    "  - $\\lambda_j=u_{j-1}^T x_j$\n",
    "- Return\n",
    "  - $u_{j}=x_{j} /\\left\\|x_{j}\\right\\|_2$\n",
    "  - $s+\\lambda_j^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicker Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- Once an eigenvalue $\\mu$ of $(A-sI)^{-1}$ is found, the eigenvalue of $A$ found is $s+\\mu^{-1}$.\n",
    "- This technique depends on the fact that the eigenvectors are all the same for shift ($A-sI$) and inversion ($A^{-1}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rayleigh quotient iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea**\n",
    "\n",
    "- Inverse Power Iteration will perform better if good shifts are provided.\n",
    "- Use Rayleigh quotient for the shifts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm** (Rayleigh quotient iteration; Sauer (2017) p. 561)\n",
    "\n",
    "- Given\n",
    "  - $A$: matrix\n",
    "  - $x_0$: initial guess vector\n",
    "- **For** $j=1,2,\\cdots$\n",
    "  - $u_{j-1}=x_{j-1} /\\left\\|x_{j-1}\\right\\|_2$\n",
    "  - $\\lambda_{j-1}=u_{j-1}^T Au_{j-1}$\n",
    "  - Solve $(A-\\lambda_{j-1}I)x_j=u_{j-1}$\n",
    "- Return\n",
    "  - $u_{j}=x_{j} /\\left\\|x_{j}\\right\\|_2$\n",
    "  - $\\lambda_{j}=u_{j}^T Au_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Convergence of Rayleigh Quotient Iteration)\n",
    "\n",
    "- (Convergence)\n",
    "  - While Inverse Power Iteration converges linearly, Rayleigh Quotient Iteration is quadratically convergent for simple (nonrepeated) eigenvalues and will converge cubically if the matrix is symmetric. [Sauer (2017) p. 562]\n",
    "  - After convergence, the matrix $A − \\lambda_{j−1} I$ is singular and no more steps can be performed. As a result, trial and error should be used to stop the iteration just before this occurs. [Sauer (2017) p. 562]\n",
    "- (Complexity) \n",
    "  - Inverse Power Iteration requires only one LU factorization for $A-sI$; but for Rayleigh Quotient Iteration, each step requires a new factorization for $A − \\lambda_{j−1} I$ since the shift has changed. [Sauer (2017) p. 563]\n",
    "  - Even so, Rayleigh Quotient Iteration is the fastest converging method among what we have presented in this section on finding one eigenvalue at a time. [Sauer (2017) p. 563]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![QR algorithm outline](../images/fig_QRalgorithmOutline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- QR algorithm finds all eigenvalues at once. \n",
    "- This algorithm relies on ideas that are far from natural as it stands. So, we approach QR algorithm trying to understand what's already known rather than trying to derive it.\n",
    "<!-- - The performance of QR algorithm is sensitive to how eigenvalues are located. We discuss favorable settings to more general settings. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Schur triangularization)\n",
    "\n",
    "Every square matrix is unitarily similar to an upper triangular matrix. That is, given $m$-by-$m$ matrix $A$, there exists a square matrix $Q$ with $Q^H Q = QQ^H=I$ such that $A = Q T Q^H$, where $T$ is an $m$-by-$m$ upper triangular matrix. This factorization is called *Schur decomposition, factorization, or triangularization*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Real Schur form)\n",
    "\n",
    "A matrix $T$ has real Schur form if it is upper triangular, except possibly for $2 \\times 2$ blocks on the main diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left[\\begin{array}{lllll}\n",
    "\\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "& \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "& & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "& & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "& & & & \\mathrm{*}\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Eigenvalues of real Schur form)\n",
    "\n",
    "- The determinant of a matrix in real Schur form is the product of the determinants of the $1 \\times 1$ and $2 \\times 2$ blocks on the main diagonal. \n",
    "- The eigenvalues of a matrix in real Schur form are the eigenvalues of the $1 \\times 1$ and $2 \\times 2$ blocks on the main diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: HW problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Existence of Real Schur form; Horn and Johnson (2013) Matrix Analysis p. 103)\n",
    "\n",
    "Let $A$ be a real $n$-by-$n$ matrix. Then, there is an $n$-by-$n$ real orthogonal matrix $Q$ such that $Q^T A Q$ is a real upper block-triangular matrix (real Schur form) with the following properties: (i) its 1-by-1 diagonal blocks display the real eigenvalues of $A$; (ii) each of its 2-by-2 diagonal blocks has a conjugate pair of non-real eigenvalues (but no special form); (iii) the ordering of its diagonal blocks may be prescribed in the following sense: If the real eigenvalues and conjugate pairs of non-real eigenvalues of A are listed in a prescribed order, then the real eigenvalues and conjugate pairs of non-real eigenvalues of the respective diagonal blocks $A_{(1)}, \\ldots, A_{(p)}$ of $Q^T A Q$ are in the same order.\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{llll}\n",
    "A_{(1)} & & & \\star \\\\\n",
    "& A_{(2)}& & \\\\\n",
    "& & \\ddots & \\\\\n",
    "0 & & & A_{(p)}\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm** (Unshifted QR algorithm)\n",
    "\n",
    "- Given\n",
    "  - $A$: a symmetric matrix\n",
    "- $A_0 = A$\n",
    "- **For** $j=0,1,2,\\cdots$\n",
    "  - $Q_{j}R_{j}=A_{j}$ (QR factorization)\n",
    "  - $A_{j+1}=R_{j}Q_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- Loosely speaking, the full QR algorithm iteratively moves an arbitrary matrix A toward its real Schur factorization by a series of similarity transformations. [Sauer (2017) p. 568]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Convergence of QR algorithm)\n",
    "\n",
    "Assume that $A$ is a symmetric $m \\times m$ matrix with eigenvalues $\\lambda_i$ satisfying $\\left|\\lambda_1\\right|>$ $\\left|\\lambda_2\\right|>\\cdots>\\left|\\lambda_m\\right|$. The unshifted QR algorithm converges linearly to the eigenvectors and eigenvalues of $A$. As $j \\rightarrow \\infty$, $A_j$ converges to a diagonal matrix containing the eigenvalues on the main diagonal and $\\bar{Q}_j=Q_1 \\cdots Q_j$ converges to an orthogonal matrix whose columns are the eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't prove the whole claim, but check some pieces of facts that help us understand the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fact**\n",
    "\n",
    "- $A_j$'s in the QR algorithm are all similar. \n",
    "  - Therefore, they have the same eigenvalues.\n",
    "$$\n",
    "A_{j+1} = R_j Q_j = (Q_j^T Q_j) R_j Q_j =Q_j^T (Q_j R_j) Q_j = Q_j^T A_j Q_j,\n",
    "$$\n",
    "where we used the orthogonality of $Q_j$.\n",
    "- If we repeat this over $j$, we have, for $j=0,1,2,\\cdots$,\n",
    "$$\n",
    "A_{j+1} = \\bar Q_j^T A \\bar Q_j, \\text{ where } \\bar Q_j = Q_0 Q_1 Q_{2}\\cdots Q_j \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- The above theorem does **not** say that we can apply unshifted QR algorithm only to symmetric matrices. It only says that we can say something for certain when the situations are favorable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Modification of Kincaid and Cheney (2002) p. 301)\n",
    "\n",
    "Carry out QR algorithm for 10 iterations to the following symmetric matrix.\n",
    "$$\n",
    "A=\\left[\\begin{array}{cccc}\n",
    "1 & 2 & 2 & 4\\\\\n",
    "2 & 5 & 6 & 2\\\\\n",
    "2 & 6 & 5 & 0\\\\\n",
    "4 & 2 & 0 & 0\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "<!-- $$\n",
    "A=\\left[\\begin{array}{cccc}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "4 & 5 & 6 & 7 \\\\\n",
    "2 & 1 & 5 & 0 \\\\\n",
    "4 & 2 & 1 & 0\n",
    "\\end{array}\\right]\n",
    "$$ -->\n",
    "\n",
    "<!-- $$\n",
    "A=\\left[\\begin{array}{cc}\n",
    "{[1]} & {\\left[\\begin{array}{lll}\n",
    "2 & 3 & 4\n",
    "\\end{array}\\right]} \\\\\n",
    "{\\left[\\begin{array}{l}\n",
    "4 \\\\\n",
    "2 \\\\\n",
    "4\n",
    "\\end{array}\\right]} & {\\left[\\begin{array}{lll}\n",
    "5 & 6 & 7 \\\\\n",
    "1 & 5 & 0 \\\\\n",
    "2 & 1 & 0\n",
    "\\end{array}\\right]}\n",
    "\\end{array}\\right]\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from internallib import qr\n",
    "\n",
    "def qr_alg_unshift(A, max_iter=10):\n",
    "    \"\"\"\n",
    "    Return approximate Schur factorization using QR algorithm.\n",
    "\n",
    "    Input:\n",
    "        A (array): A square matrix\n",
    "    Output:\n",
    "        T (array): approximate Schur form of A\n",
    "        U (array): The unitary matrix involved in the similarity T=U^H*A*U\n",
    "    Note: \n",
    "        - When A is a real matrix, T is an approxiate real Schur form.\n",
    "        In this case, U is an orthogonal matrix.\n",
    "        - \n",
    "    \"\"\"\n",
    "    m = A.shape[0]\n",
    "    T = A.copy()\n",
    "    U = np.eye(m)\n",
    "    for _ in range(max_iter):\n",
    "        Q, R = qr(T)\n",
    "        T = R @ Q\n",
    "        U = U @ Q\n",
    "    \n",
    "    return T, U\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- The decision of right-multiplication by `Q` for `U` is based on the property of the QR algorithm.\n",
    "  - For $j=0,1,2,\\cdots$,\n",
    "$$\n",
    "A_{j+1} = \\bar Q_j^T A \\bar Q_j, \\text{ where } \\bar Q_j = Q_0 Q_1 Q_{2}\\cdots Q_j \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 2. 4.]\n",
      " [2. 5. 6. 2.]\n",
      " [2. 6. 5. 0.]\n",
      " [4. 2. 0. 0.]]\n",
      "[[12.25824  0.       0.       0.     ]\n",
      " [-0.      -3.95885 -0.00008  0.     ]\n",
      " [-0.      -0.00008  3.52016  0.     ]\n",
      " [ 0.       0.      -0.      -0.81954]]\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Suggested implementation\n",
    "- max_iter = 10, 100, 1000\n",
    "\"\"\"\n",
    "\n",
    "A = np.array([  [1 , 2 , 2 , 4],\n",
    "                [2 , 5 , 6 , 2],\n",
    "                [2 , 6 , 5 , 0],\n",
    "                [4 , 2 , 0 , 0]], dtype=np.float64)\n",
    "\n",
    "T, U = qr_alg_unshift(A, max_iter=100)\n",
    "\n",
    "with np.printoptions(precision=5, suppress=True):\n",
    "    print(A)\n",
    "    print(T)\n",
    "    print(np.allclose(A, U @ T @ U.T))\n",
    "    print(np.allclose(A @ U, np.diag(T)*U)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plan** \n",
    "\n",
    "We will improve (unshifted) QR algorithm as follows.\n",
    "\n",
    "1. Convert a matrix into upper Hessenberg form.\n",
    "    -  Introduce as many zeros as possible.\n",
    "2. Apply shifting to QR algorithm.\n",
    "    -  Accelerate finding an eigenvalue.\n",
    "3. Deflate into smaller matrices.\n",
    "   - \"Harvest\" an eigenvalue and narrow down to smaller problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hessenberg form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Efficiency of the QR algorithm increases considerably if we first put $A$ into upper Hessenberg form. [Sauer (2017) p. 570]\n",
    "  - Hessenberg form introduces as many zeros into $A$ as possible while preserving all eigenvalues. \n",
    "- Upper Hessenberg form eliminates the final difficulty—convergence to multiple complex eigenvalues— and the QR iteration will always proceed to 1 × 1 or 2 × 2 blocks. [Sauer (2017) p. 570]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Hessenberg form)\n",
    "\n",
    "The $m \\times n$ matrix $A$ is in upper Hessenberg form if $a_{i j}=0$ for $i>j+1$.\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{lllll}\n",
    "\\times & \\times & \\times & \\times & \\times \\\\\n",
    "\\times & \\times & \\times & \\times & \\times \\\\\n",
    "& \\times & \\times & \\times & \\times \\\\\n",
    "& & \\times & \\times & \\times \\\\\n",
    "& & & \\times & \\times\n",
    "\\end{array}\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Hessenberg form)\n",
    "\n",
    "Let $A$ be a real square matrix. There exists an orthogonal matrix $Q$ such that $A=Q B Q^T$ and $B$ is in upper Hessenberg form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: See construction below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construction of Hessenberg form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea**: A similar to QR factorization via Householder reflector yields Hessenberg form. But if we do that less agressive, meaning, zeroing only the below subdiagonal, as opposed to directly below diagonal, we obtain similiarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.a. Zero out 3rd entry and below of the first column.\n",
    "\n",
    "$$\n",
    "H_1 A=\\left[\\begin{array}{c:cccc}\n",
    "1 & 0 & 0 & 0 & 0 \\\\\n",
    "\\hdashline 0 & & & \\\\\n",
    "0 & & \\hat{H}_1 & & \\\\\n",
    "0 & & & & \\\\\n",
    "0 & & & &\n",
    "\\end{array}\\right]\\left[\\begin{array}{c:cccc}\n",
    "\\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "\\hdashline \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "\\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "\\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "\\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c:cccc}\n",
    "\\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "\\hdashline \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "0 & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "0 & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "0 & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*}\n",
    "\\end{array}\\right] .\n",
    "$$\n",
    "\n",
    "Here, $\\hat H_1$ is $m$-by-$m$ matrix that sends $x=A[1:, 0]$ (in NumPy slicing notation; lower $(m-1)$-vector of the 1st column of A) to $(\\mathrm{sign}(x_0) \\Vert x \\Vert, 0, 0, \\cdots, 0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b. Also right-multiply by $H_1$.\n",
    "\n",
    "$$\n",
    "H_1 A H_1=\\left[\\begin{array}{c:cccc}\n",
    "\\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "\\hdashline \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "0 & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "0 & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "0 & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*}\n",
    "\\end{array}\\right]\\left[\\begin{array}{c:cccc}\n",
    "1 & 0 & 0 & 0 & 0 \\\\\n",
    "\\hdashline 0 & & & \\\\\n",
    "0 & & \\hat{H}_1 & & \\\\\n",
    "0 & & & & \\\\\n",
    "0 & & & &\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c:cccc}\n",
    "\\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "\\hdashline \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "0 & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "0 & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "0 & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "A block multiplication shows that the zeros introduced by left-multiplication by $H_1$ is kept after left-multiplication by $H_1$.\n",
    "\n",
    "The resulting product is similar to $A$ since $H_1=H_1^{-1}(=H_1^T)$ (symmetric, orthoginal). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Repeat the same to bottom right $(m-1)$-by-$(m-1)$ submatrix of the result.\n",
    "\n",
    "$$\n",
    "H_2\\left(H_1 A H_1\\right)=\\left[\\begin{array}{cc:ccc}\n",
    "1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 \\\\\n",
    "\\hdashline 0 & 0 & & & \\\\\n",
    "0 & 0 & & \\hat{H}_2 & \\\\\n",
    "0 & 0 & & &\n",
    "\\end{array}\\right]\\left[\\begin{array}{cc:ccc}\n",
    "\\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "\\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "\\hdashline 0 & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "0 & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "0 & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{cc:ccc}\n",
    "\\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "\\hdashline \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "0 & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "0 & 0 & \\mathrm{*} & \\mathrm{*} & \\mathrm{*} \\\\\n",
    "0 & 0 & \\mathrm{*} & \\mathrm{*} & \\mathrm{*}\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We end up getting an upper Hessenberg form.\n",
    "\n",
    "$$\n",
    "H_3 H_2 H_1 A H_1^T H_2^T H_3^T=H_3 H_2 H_1 A\\left(H_3 H_2 H_1\\right)^T=Q A Q^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- If the column to reflect is pathological, i.e., 0 vector, then we can take the Householder reflector $H=I$ and move on to the next submatrix. Therefore, the theorem holds for all general case even though the above construction cannot proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- If we tried to zero out from the 2nd entry of the 1st column, right-multiplication by $H_1$ would mess up the zeros back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "\n",
    "- There is a finite algorithm for putting matrices in upper Hessenberg form by similarity transformations. See the construction below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Practicality of Hessenberge form)\n",
    "\n",
    "- Since Hessenberg form is similar to $A$, it has the same eigenvalues as $A$. \n",
    "  - We can find eigenvalues of the Hessenberg form instead, which has way more zeros.\n",
    "- Hessenberg structure is preserved by QR algorithm.\n",
    "  - Computational efficiency is gained throughout the iterations of QR algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma** (Hessenberg and triangular)\n",
    "\n",
    "Let $B$ an upper Hessenberge matrix and $R$ be an upper triangular matrix. Then, $BR$ and $RB$ are upper Hessenberg form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discuss only an intuitive argument.\n",
    "\n",
    "1. For $BR$, \n",
    "   - view the product as a horizontal stack of columns, each of which is $BR_i$, where $R_i$ is $i$-th column of $R$.\n",
    "   - Also, view each $BR_i$ as linear combination of columns of $B$ with entries of $R_i$ being the coefficients. Then, the following structure is evident.\n",
    "\n",
    "$$\n",
    "BR=\\left[\\begin{array}{lllll}\n",
    "\\mathbf{*} & \\mathbf{*} & \\mathbf{*} & \\mathbf{*} & \\mathbf{*} \\\\\n",
    "\\mathbf{*} & \\mathbf{*} & \\mathbf{*} & \\mathbf{*} & \\mathbf{*} \\\\\n",
    "& \\mathbf{*} & \\mathbf{*} & \\mathbf{*} & \\mathbf{*} \\\\\n",
    "& & \\mathbf{*} & \\mathbf{*} & \\mathbf{*} \\\\\n",
    "& & & \\mathbf{*} & \\mathbf{*}\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{lllll}\n",
    "\\mathbf{\\times} & \\mathbf{\\times} & \\mathbf{\\times} & \\mathbf{\\times} & \\mathbf{\\times} \\\\\n",
    " & \\mathbf{\\times} & \\mathbf{\\times} & \\mathbf{\\times} & \\mathbf{\\times} \\\\\n",
    "&  & \\mathbf{\\times} & \\mathbf{\\times} & \\mathbf{\\times} \\\\\n",
    "& &  & \\mathbf{\\times} & \\mathbf{\\times} \\\\\n",
    "& & &  & \\mathbf{\\times}\n",
    "\\end{array}\\right]\n",
    "=\n",
    "\\left[\\begin{array}{lllll}\n",
    "\\mathbf{+} & \\mathbf{+} & \\mathbf{+} & \\mathbf{+} & \\mathbf{+} \\\\\n",
    "\\mathbf{+} & \\mathbf{+} & \\mathbf{+} & \\mathbf{+} & \\mathbf{+} \\\\\n",
    "& \\mathbf{+} & \\mathbf{+} & \\mathbf{+} & \\mathbf{+} \\\\\n",
    "& & \\mathbf{+} & \\mathbf{+} & \\mathbf{+} \\\\\n",
    "& & & \\mathbf{+} & \\mathbf{+}\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The same argument applies to $RB$.\n",
    "\n",
    "\n",
    "$$\n",
    "BR=\n",
    "\\left[\\begin{array}{lllll}\n",
    "\\mathbf{\\times} & \\mathbf{\\times} & \\mathbf{\\times} & \\mathbf{\\times} & \\mathbf{\\times} \\\\\n",
    " & \\mathbf{\\times} & \\mathbf{\\times} & \\mathbf{\\times} & \\mathbf{\\times} \\\\\n",
    "&  & \\mathbf{\\times} & \\mathbf{\\times} & \\mathbf{\\times} \\\\\n",
    "& &  & \\mathbf{\\times} & \\mathbf{\\times} \\\\\n",
    "& & &  & \\mathbf{\\times}\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{lllll}\n",
    "\\mathbf{*} & \\mathbf{*} & \\mathbf{*} & \\mathbf{*} & \\mathbf{*} \\\\\n",
    "\\mathbf{*} & \\mathbf{*} & \\mathbf{*} & \\mathbf{*} & \\mathbf{*} \\\\\n",
    "& \\mathbf{*} & \\mathbf{*} & \\mathbf{*} & \\mathbf{*} \\\\\n",
    "& & \\mathbf{*} & \\mathbf{*} & \\mathbf{*} \\\\\n",
    "& & & \\mathbf{*} & \\mathbf{*}\n",
    "\\end{array}\\right]\n",
    "=\n",
    "\\left[\\begin{array}{lllll}\n",
    "\\mathbf{+} & \\mathbf{+} & \\mathbf{+} & \\mathbf{+} & \\mathbf{+} \\\\\n",
    "\\mathbf{+} & \\mathbf{+} & \\mathbf{+} & \\mathbf{+} & \\mathbf{+} \\\\\n",
    "& \\mathbf{+} & \\mathbf{+} & \\mathbf{+} & \\mathbf{+} \\\\\n",
    "& & \\mathbf{+} & \\mathbf{+} & \\mathbf{+} \\\\\n",
    "& & & \\mathbf{+} & \\mathbf{+}\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma** (QR algorithm preserve Hessenberg form)\n",
    "\n",
    "QR algorithm preserve Hessenberg form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof:\n",
    "\n",
    "Let $B$ is an upper Hessenberg matrix. Then, by the construction of the QR algorithm, we have\n",
    "\n",
    "$$\n",
    "B_1 = B, \\qquad B_1 = Q_1 R_1, \\qquad B_2 = R_1 Q_1.\n",
    "$$\n",
    "\n",
    "We want to show: $B_2$ is upper Hessenberg form. \n",
    "\n",
    "Then, we can repeat the argument and $B_i$ ($i=3,4,\\cdots$) are all upper Hessenberg form.\n",
    "\n",
    "Now, from the second equality above, we have $Q_1 = B_1 R_1^{-1}$. But the inverse of the upper triangular matrix is again an upper triangular. (See `na05linsystem1LU.ipynb` Theorem (Triangular matrices and their algebraic structure)). Therefore, by the previous lemma $Q_1$ is upper Hessenberg form. \n",
    "\n",
    "Plug this into the third equality above, we have $B_2 = Q_1 R_1$. Then, by the previous lemma, we again have $B_2$ is also upper Hessenberg.\n",
    "\n",
    "QED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Remark** \n",
    "\n",
    "- In the limit, $B_i$ will converges to real Schur form. (**To be double-checked**.) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computation of Hessenberg form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing upper Hessenberg form\n",
    "\n",
    "- Basically similar to QR factorization via Householder reflector.\n",
    "  - Use the same reflection operator once $x$ is determined. (But $x$ is different.)\n",
    "  - Slicing for matrix multiplications are apparent when if writing out the procedure in a block form.\n",
    "- Differences of Hessenberg form compared to QR factorization\n",
    "  - Only square matrix is allowed due to similarity of matrices\n",
    "  - loop over the first $m-2$ columns: \n",
    "  - $x$ (reflected vector) is sliced from one entry lower in row index than QR factorization.\n",
    "    - $k$ (length of reflected vector) is smaller than QR by 1\n",
    "  - $\\hat H$ is also multiplied on the right after on the left.\n",
    "  - If $B=Q A Q^T$ is desired, multiply Q by H on the left. \n",
    "    - If $A=Q B Q^T$ is wanted, multiply Q by H on the right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hessen_v1(A):\n",
    "    \"\"\"\n",
    "    Return a upper Hessenberg form that is similar to given matrix.\n",
    "\n",
    "    Input:\n",
    "        A (array): a square matrix.\n",
    "    Output:\n",
    "        B, Q (array): Hessenberg matrix and an orthogonal matrix \n",
    "            such that B = Q A Q^T\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    assert m == n\n",
    "    B = A.copy()\n",
    "    Q = np.eye(m)\n",
    "    \n",
    "    for i in range(m-2):\n",
    "        k = m - i - 1\n",
    "\n",
    "        x = B[(i+1):, i]\n",
    "        w = np.zeros_like(x)\n",
    "        w[0] = - np.sign(x[0])*np.linalg.norm(x)\n",
    "\n",
    "        v = w - x\n",
    "        H = np.eye(k) - 2.*(np.outer(v, v))/(np.dot(v, v))\n",
    "        \n",
    "        B[(i+1):, :] = H @ B[(i+1):, :]\n",
    "        B[:, (i+1):] = B[:, (i+1):] @ H\n",
    "\n",
    "        # Multiply by H on the left: e.g. Q = H_3 H_2 H_1 \n",
    "        Q[(i+1):, :] = H @ Q[(i+1):, :]\n",
    "\n",
    "    return B, Q\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04 -0.67 -1.17 -0.17 -0.28]\n",
      " [-0.65  1.65  0.78 -0.47 -0.18]\n",
      " [-0.    1.11  0.49  0.24  0.16]\n",
      " [-0.    0.   -1.03 -0.08 -0.08]\n",
      " [-0.   -0.    0.    0.53 -0.54]]\n",
      "upper Hessenberg form? --> True\n",
      "[[ 1.    0.    0.    0.    0.  ]\n",
      " [ 0.   -0.37 -0.37 -0.85 -0.1 ]\n",
      " [ 0.   -0.14 -0.4   0.34 -0.84]\n",
      " [ 0.    0.86  0.1  -0.37 -0.34]\n",
      " [ 0.    0.33 -0.83  0.17  0.41]]\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0. -0. -0.]\n",
      " [ 0.  0.  1. -0.  0.]\n",
      " [ 0. -0. -0.  1. -0.]\n",
      " [ 0. -0.  0. -0.  1.]]\n",
      "Q orthogonal? --> True\n",
      "B = QAQ^T? --> True\n"
     ]
    }
   ],
   "source": [
    "m = 5\n",
    "A = np.random.rand(m,m)\n",
    "B, Q = hessen_v1(A)\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(B)\n",
    "    print(f\"upper Hessenberg form? --> {np.allclose(np.tril(B, -2), np.zeros((m,m)))}\")\n",
    "    print(Q)\n",
    "    print(Q.T @ Q)\n",
    "    print(f\"Q orthogonal? --> {np.allclose(Q.T @ Q, np.eye(m))}\")\n",
    "    print(f\"B = QAQ^T? --> {np.allclose(B, Q @ A @ Q.T)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- We can further make the code more efficient as we did in QR factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hessen(A):\n",
    "    \"\"\"\n",
    "    Return a upper Hessenberg form that is similar to given matrix.\n",
    "\n",
    "    Input:\n",
    "        A (array): a square matrix.\n",
    "    Output:\n",
    "        B, Q (array): Hessenberg matrix and an orthogonal matrix \n",
    "            such that A = Q^T B Q\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    assert m == n\n",
    "    B = A.copy()\n",
    "    Q = np.eye(m)\n",
    "    \n",
    "    for i in range(m-2):\n",
    "        k = m - i - 1\n",
    "\n",
    "        x = B[(i+1):, i].reshape(-1, 1)\n",
    "        w = np.zeros_like(x).reshape(-1, 1)\n",
    "        w[0] = - np.sign(x[0])*np.linalg.norm(x)\n",
    "\n",
    "        v = w - x\n",
    "        v_ = ((2./(v.T @ v))*v)\n",
    "        \n",
    "        B[(i+1):, :] = B[(i+1):, :] - v_ @ (v.T @ B[(i+1):, :])\n",
    "        B[:, (i+1):] = B[:, (i+1):] - (B[:, (i+1):] @ v_) @ v.T\n",
    "\n",
    "        # Multiply by H on the left: e.g. Q = H_3 H_2 H_1 \n",
    "        Q[(i+1):, :] = Q[(i+1):, :] - v_ @ (v.T @ Q[(i+1):, :])\n",
    "        \n",
    "    return B, Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7  -0.19 -0.14 -0.03 -0.12]\n",
      " [-0.93  0.79  1.32 -0.   -0.18]\n",
      " [ 0.    1.05  0.31 -0.09 -0.21]\n",
      " [ 0.   -0.    0.33  0.09  0.09]\n",
      " [ 0.   -0.    0.    0.09  0.44]]\n",
      "upper Hessenberg form? --> True\n",
      "[[ 1.    0.    0.    0.    0.  ]\n",
      " [ 0.   -0.38 -0.22 -0.9  -0.02]\n",
      " [ 0.   -0.24 -0.72  0.29 -0.59]\n",
      " [ 0.    0.8   0.01 -0.33 -0.49]\n",
      " [ 0.    0.39 -0.66 -0.02  0.64]]\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0. -0.]\n",
      " [ 0.  0.  0.  1. -0.]\n",
      " [ 0.  0. -0. -0.  1.]]\n",
      "Q orthogonal? --> True\n",
      " B= QAQ^T? --> True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m = 5\n",
    "A = np.random.rand(m,m)\n",
    "B, Q = hessen(A)\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(B)\n",
    "    print(f\"upper Hessenberg form? --> {np.allclose(np.tril(B, -2), np.zeros((m,m)))}\")\n",
    "    print(Q)\n",
    "    print(Q.T @ Q)\n",
    "    print(f\"Q orthogonal? --> {np.allclose(Q.T @ Q, np.eye(m))}\")\n",
    "    print(f\" B= QAQ^T? --> {np.allclose(B, Q @ A @ Q.T)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Kincaid and Cheney (2002) p. 301)\n",
    "\n",
    "Reduce the following (asymmetric) matrix to upper Hessenberg form by means of unitary similar­ity transforms. And then carry out QR algorithm for 10 iterations. \n",
    "\n",
    "$$\n",
    "A=\\left[\\begin{array}{cccc}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "4 & 5 & 6 & 7 \\\\\n",
    "2 & 1 & 5 & 0 \\\\\n",
    "4 & 2 & 1 & 0\n",
    "\\end{array}\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.      -4.89898 -0.      -0.     ]\n",
      " [-4.89898  5.       5.7735   0.     ]\n",
      " [-0.       5.7735   5.4     -2.12289]\n",
      " [-0.       0.      -2.12289 -0.4    ]]\n",
      "[[12.25824  0.      -0.      -0.     ]\n",
      " [-0.      -3.95885  0.00003  0.     ]\n",
      " [ 0.       0.00003  3.52016 -0.     ]\n",
      " [-0.       0.      -0.      -0.81954]]\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Symmetric case:\n",
    "    max_iter = 105\n",
    "Asymmetric case:\n",
    "    max_iter = 6\n",
    "\"\"\"\n",
    "\n",
    "# A = np.array([  [1 , 2 , 3 , 4],\n",
    "#                 [4 , 5 , 6 , 7],\n",
    "#                 [2 , 1 , 5 , 0],\n",
    "#                 [4 , 2 , 1 , 0]], dtype=np.float64)\n",
    "\n",
    "A = np.array([  [1 , 2 , 2 , 4],\n",
    "                [2 , 5 , 6 , 2],\n",
    "                [2 , 6 , 5 , 0],\n",
    "                [4 , 2 , 0 , 0]], dtype=np.float64)\n",
    "\n",
    "B, Q = hessen(A)\n",
    "T, U = qr_alg_unshift(B, max_iter=100)\n",
    "\n",
    "with np.printoptions(precision=5, suppress=True):\n",
    "    print(B)\n",
    "    print(T)\n",
    "    print(np.allclose(T, U.T @ B @ U))\n",
    "    print(np.allclose(B @ U, np.diag(T)*U)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shifting and deflating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivating Example** (continued)\n",
    "\n",
    "In the previous example, the process is still sluggish (in terms of the number of iterations) even after converting to Hessenberg form. Note that the matrix $A$ has distinct (approximate) eigenvalues $\\{11.106, -3.8556, 3.5736, 0.17645\\}$. Even 6 iterations have revealed the last eigenvalue quite accurately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea** \n",
    "\n",
    "- Deflation\n",
    "  - \"Harvest\" the eigenvalue when it is certain.\n",
    "- Shifting\n",
    "  - Use shifting, adding $-sI$ for some $s$, to find that eigenvalue even faster.\n",
    "  - This is the same idea as in Inverse Power Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deflation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma** (Spectrum of block triangular matrix; Kincaid and Cheney (2002) p. 303)\n",
    "\n",
    "Let $A$ be a matrix in partitioned form\n",
    "$$\n",
    "A=\\left[\\begin{array}{ll}\n",
    "B & C \\\\\n",
    "0 & E\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "in which $B$ and $E$ are square matrices. Then the spectrum of $A$ (i.e., the set of its eigenvalues) is the union of the spectra of $B$ and $E$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: (Kincaid and Cheney (2002) p. 303)\n",
    "\n",
    "The equation $A x=\\lambda x$, in partitioned form, is\n",
    "$$\\tag{Eq1}\n",
    "\\left[\\begin{array}{ll}\n",
    "B & C \\\\\n",
    "0 & E\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "u \\\\\n",
    "v\n",
    "\\end{array}\\right]=\\lambda\\left[\\begin{array}{l}\n",
    "u \\\\\n",
    "v\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "or, equivalently,\n",
    "\n",
    "$$\n",
    "\\left\\{\\begin{aligned}\n",
    "B u+C v & =\\lambda u \\\\\n",
    "E v & =\\lambda v\n",
    "\\end{aligned}\\right.\n",
    "$$\n",
    "\n",
    "If $\\lambda$ is an eigenvalue of $A$, then (Eq1) has a nontrivial solution $(u, v)^T$. If $v \\neq 0$, then $\\lambda$ is an eigenvalue of $E$. If $v=0$, then $u \\neq 0$, and $\\lambda$ is an eigenvalue of $B$. This proves that $\\mathrm{sp}(A) \\subseteq \\operatorname{sp}(B) \\cup \\mathrm{sp}(E)$.\n",
    "\n",
    "Conversely, if $\\lambda$ is an eigenvalue of $B$, and if $u$ is a corresponding (nonzero) eigenvector, then $(u, 0)^T$ will solve (Eq1). If $\\lambda$ is an eigenvalue of $E$ but not an eigenvalue of $B$, then let $v$ be a nonzero vector satisfying $E v=\\lambda v$. Next, solve the equation $(B-\\lambda I) u=-C v$. This can be done since $\\lambda$ is not an eigenvalue of $B$. Then the vector $(u, v)^T$ solves (Eq1). This proves that $\\operatorname{sp}(B) \\cup \\operatorname{sp}(E) \\subseteq \\operatorname{sp}(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation** (Deflation)\n",
    "\n",
    "If the last row has all near 0 but the last entry, then we declare that the last entry corresponds to $E$ in the previous lemma. Find the rest of the eigenvalues from $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shifting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Shifting preserves similarity.\n",
    "\n",
    "- Each step, apply the shift, compute a QR factorization, and then take the shift back. \n",
    "$$\n",
    "\\begin{aligned}\n",
    "A_0-s I & =Q_1 R_1 \\\\\n",
    "A_1 & =R_1 Q_1+s I .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A_1-s I & =R_1 Q_1 \\\\\n",
    "& =Q_1^T\\left(A_0-s I\\right) Q_1 \\\\\n",
    "& =Q_1^T A_0 Q_1-s I\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, we have\n",
    "\n",
    "$$\n",
    "A_1=Q_1^T A_0 Q_1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- Q's appearing in the shifted version will be different from those in unshifted version. But that will lead to a faster path to compute the nearest eigenvalue to $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from internallib import qr\n",
    "\n",
    "def qr_alg(A, max_iter=10, shift=False):\n",
    "    \"\"\"\n",
    "    Return approximate Schur factorization using QR algorithm.\n",
    "\n",
    "    Input:\n",
    "        A (array): A square matrix\n",
    "    Output:\n",
    "        T (array): approximate Schur form of A\n",
    "        U (array): The unitary matrix involved in the similarity T=U^H*A*U\n",
    "    Note: \n",
    "        - When A is a real matrix, T is an approxiate real Schur form.\n",
    "        In this case, U is an orthogonal matrix.\n",
    "        - \n",
    "    \"\"\"\n",
    "    m = A.shape[0]\n",
    "    T = A.copy()\n",
    "    U = np.eye(m)\n",
    "    for _ in range(max_iter):\n",
    "        \n",
    "        sI = T[-1,-1]*np.eye(T.shape[0]) if shift == True else None\n",
    "        \n",
    "        Q, R = qr(T) if shift==False else qr(T - sI)\n",
    "\n",
    "        T = R @ Q if shift==False else R @ Q + sI\n",
    "        \n",
    "        U = U @ Q\n",
    "    \n",
    "    return T, U\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "[[ 1.97781  5.33406  3.25384  2.41068]\n",
      " [ 9.9802   5.21467  3.16599  2.92323]\n",
      " [-0.       0.09477  0.2339   0.19199]\n",
      " [-0.       0.       0.       3.57362]]\n",
      "(3, 3)\n",
      "[[11.06754 -4.84049  4.19047]\n",
      " [-0.1171  -3.8176  -1.31762]\n",
      " [ 0.       0.       0.17645]]\n",
      "(2, 2)\n",
      "[[11.10552 -4.72338]\n",
      " [ 0.      -3.85559]]\n",
      "(1, 1)\n",
      "eigenvlaues = [[ 3.57361662  0.17645187 -3.85558822 11.10551973]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Suggested implementation\n",
    "1. Effect of only deflation\n",
    "    - shift=False\n",
    "    - max_iter = 10, 100, 1000\n",
    "2. Effect of shifting + deflation\n",
    "    - shift=True\n",
    "    - max_iter = 5\n",
    "\"\"\"\n",
    "\n",
    "A = np.array([  [1 , 2 , 3 , 4],\n",
    "                [4 , 5 , 6 , 7],\n",
    "                [2 , 1 , 5 , 0],\n",
    "                [4 , 2 , 1 , 0]], dtype=np.float64)\n",
    "\n",
    "m = A.shape[0]\n",
    "B, Q = hessen(A)\n",
    "eig = np.full(m, np.nan)\n",
    "\n",
    "shift = True\n",
    "max_iter = 5\n",
    "\n",
    "for i in range(m):\n",
    "    print(B.shape)\n",
    "    if B.shape[0] == 1:\n",
    "        eig[i] = B[-1, -1]\n",
    "        break\n",
    "\n",
    "    T, U = qr_alg(B, max_iter=max_iter, shift=shift)\n",
    "    with np.printoptions(precision=5, suppress=True):\n",
    "        print(T)\n",
    "    \n",
    "    if np.allclose(T[-1, :-1], 0):\n",
    "        eig[i] = T[-1, -1]\n",
    "        B = T[:-1, :-1]\n",
    "    else:\n",
    "        print(\"No deflation carried out: the last row not of the form: [0, 0, ..., r]\")\n",
    "    \n",
    "print(f\"eigenvlaues = [{eig}]\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-by-2 blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Sauer (2017) p. 569)\n",
    "\n",
    "- For complex eigenvalues, we must allow for 2 × 2 blocks on the diagonal of the real Schur form.\n",
    "- If deflating a 1 × 1 diagonal block in the bottom right corner fails (after a user-specified number of tries), we declare a 2 × 2 block. \n",
    "  - Find the pair of eigenvalues, and then deflates by 2. \n",
    "- This will make the algorithm converge to real Schur form for most, but not all, input matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spectral theorem on normal matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theomrem** (Trefethen and Bau, Numerical Linear Algebra, p. 187.; Horn and Johnson (2013) Matrix Analysis p. 133)\n",
    "\n",
    "A matrix is unitarily diagonalizable iff it is normal: $A^H A= A A^H$.\n",
    "\n",
    "**Remark**\n",
    "\n",
    "- The eigenvalues of a normal matrix may be complex while Hermitian matrices have real eigenvalues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrch",
   "language": "python",
   "name": "ptrch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
