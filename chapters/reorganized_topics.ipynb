{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropped from na02approximation.ipynb\n",
    "\n",
    "Move to M104C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Chebychev polynomial 3](https://jhparkyb.github.io/resources/notes/na/104ASlides_Approximation021.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divided differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Divided differences 1](https://jhparkyb.github.io/resources/notes/na/104ASlides_Approximation026.png)\n",
    "\n",
    "![Divided differences 2](https://jhparkyb.github.io/resources/notes/na/104ASlides_Approximation027.png)\n",
    "\n",
    "![Divided differences 3](https://jhparkyb.github.io/resources/notes/na/104ASlides_Approximation028.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Horner's algorithm: efficient evaluation of interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficient way of evaluating polynomails.\n",
    "\n",
    "![Example of Horner's algorithm](https://jhparkyb.github.io/resources/notes/na/104ASlides_Approximation010.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Finding interpolating polynomial in the Newton form 2](https://jhparkyb.github.io/resources/notes/na/104ABoardWork_Approximation002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "swapping\n",
    "<!-- **Motivating example**\n",
    "\n",
    "See the example below. There are no errors or bugs in the function `gauss_elimination2D`. But it produces something weird: `x_lib` is the correct solution, which is obtained by `numpy.linalg` (which uses LAPACK library). What has happened? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "swapping\n",
    "<!-- import numpy as np\n",
    "\n",
    "def gauss_elimination2D(AA, bb):\n",
    "    \"\"\"Solves Ax = b using Gauss elimination.\n",
    "\n",
    "    Args:\n",
    "        A: A square matrix.\n",
    "        b: A vector.\n",
    "\n",
    "    Returns:\n",
    "        x: A vector.\n",
    "    \"\"\"\n",
    "    n = 2\n",
    "    AA = A.copy()\n",
    "    bb = b.copy()\n",
    "    x = np.zeros(n).reshape(-1, 1)\n",
    "\n",
    "    m = AA[1, 0] / AA[0, 0]\n",
    "    AA[1, 1] -= m * AA[1, 0]\n",
    "    bb[1] -= m * bb[0]\n",
    "    \n",
    "    x[1] = bb[1] / AA[1, 1]\n",
    "    x[0] = (bb[0] - AA[0, 1] * x[1]) / AA[0, 0]\n",
    "    \n",
    "    return x\n",
    "\n",
    "A = np.array([[10e-20,  1.], \n",
    "              [     1.,  2.]])\n",
    "b = np.array([1., 4.]).reshape(-1, 1)\n",
    "\n",
    "x_naive = gauss_elimination2D(A, b)\n",
    "x_lib = np.linalg.solve(A, b)\n",
    "\n",
    "print(\"x_naive: \\n\", x_naive)\n",
    "print(\"x: \\n\", x) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### Inner product\n",
    "\n",
    "Inner product is a far-reaching concept that helps us tackle differential geometry, Fourier analysis, solutions of partial differential equations just to name a few. Here, we give just a bit more details to see if the $$(f,g)_w:=\\int_a^b f(x)g(x)w(x)dx$$ is indeed an inner product. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Motivating example**\n",
    "\n",
    "Recall the homework problem, where we solved the following linear system by programming 2D version of Gaussian elimination.\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "10^{-20} x_1+x_2 & =1 \\\\\n",
    "x_1+2 x_2 & =4\n",
    "\\end{aligned}\n",
    "\\right. \n",
    "$$\n",
    "\n",
    "If we programmed \"right,\" it should produce $(x_1, x_2)\\approx(0,1)$, while the package `numpy.linalg.solve` gave $(x_1, x_2)\\approx(2,1)$. \n",
    "\n",
    "What is going on? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Remark**\n",
    "\n",
    "- The source of error is that the first coefficient `A[0,0]` is so small that the computer (using IEEE double precision) \"thinks\" it is 0. \n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "10^{-20} x_1+x_2 & =1 \\\\\n",
    "x_1+2 x_2 & =4\n",
    "\\end{aligned}\n",
    "\\right. \n",
    "\\Rightarrow\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "0 x_1+x_2 & =1 \\\\\n",
    "x_1+2 x_2 & =4\n",
    "\\end{aligned}\n",
    "\\right. \n",
    "$$\n",
    "\n",
    "Then, the first equation leads to $x_2=1$, which in turn, leads to $x_1 = 0$. Hence, we have $(x_1, x_2)\\approx(0,1)$. \n",
    "\n",
    "- However, if we carry the small coefficient $10^{-20}$ and meticulously solve the equation, we obtain $(x_1, x_2)\\approx(2,1)$. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Idea** (Swapping or partial pivoting)\n",
    "\n",
    "- Push down a row whose pivot is small, and pull up the one with the largest coefficient in that column.\n",
    "- This procedure is called *partial pivoting*. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "According to Sauer (2017) Numerical Analysis 3rd ed., the practical difference among the different types of PDE is:\n",
    "- parabolic and hyperbolic\n",
    "  - PDE is defined on an open region.\n",
    "  - Boundary conditions for one variable are specified at one end of the region (initial state) and the system is solved by moving away from the boundary. (Think of time evolution.)\n",
    "- elliptic\n",
    "  - PDE is defined on the entire boundary of a closed region.\n",
    "  - Boundary conditions are imposed literally on the boundary of the region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: INCLUDE MORE COMPLETE EXPOSITION\n",
    "\n",
    "**Remark** (Why try to avoid `for` loop in Python)\n",
    "\n",
    "- Python `int` is not purely an integer, it is an object. (See code below.)\n",
    "- When `for` loop is implemented, it takes out only pure integer part and work. (More process)\n",
    "- Python lacks JIT (just-in-time compilation). \n",
    "  - Python is an interpreter. Hence, it executes each line.\n",
    "  - But for `for` loop, interpreting each line is waste of time. We want just repeat the same task, and carry out the whole `for` loop as a single command.\n",
    "  - If those lines are skipped while interpreting, and executed directly during the run time, a lot of computing time can be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "9822216\n",
      "b'\"'\n"
     ]
    }
   ],
   "source": [
    "# Python int is not purely an integer, it is an object.\n",
    "\n",
    "i = 34\n",
    "\n",
    "print(type(i))\n",
    "print(id(i))\n",
    "print(i.to_bytes(length=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Rewriting to avoid loss of significand)\n",
    "\n",
    "Suppose that your project involves the following computation near $x = 0$ for a fixed value $a > 0$: \n",
    "\n",
    "$$\n",
    "\\sqrt{x^2 + a^2} - a .\n",
    "$$\n",
    "\n",
    "Rewrite this expression to avoid loss of significant digits along the computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "The quantity can be rewritten as \n",
    "\n",
    "$$\n",
    "\\left(\\sqrt{x^2+a^2}-a\\right)\\left(\\frac{\\sqrt{x^2+a^2}+a}{\\sqrt{x^2+a^2}+a}\\right)=\\frac{x^2}{\\sqrt{x^2+a^2}+a}\n",
    "$$\n",
    "\n",
    "We can use the last term in place of what's given in the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume the following, which is totally made up for illustrative purposes. \n",
    "\n",
    "- Our computer uses decimals, instead of binary.\n",
    "- Our computer uses six-decimal-digit mantissa, including the integer part, floating point number system along with power of tens as scalar.\n",
    "- $x=0.01$ and $a = 3$\n",
    "\n",
    "Then, the input will be stored as\n",
    "\n",
    "- $x^2 + a^2 = 9.00010 \\times 10^0$\n",
    "- $a = 3.00000 \\times 10^0$\n",
    "\n",
    "1. Original formula\n",
    "\n",
    "Even assuming infinite precision for evaluating square root, the square root is stored as\n",
    "\n",
    "- $\\sqrt{x^2 + a^2} = 3.00002 \\times 10^0$ (more precise value is $\\sqrt{9.00010}=3.0000166666203705$)\n",
    "- $\\sqrt{x^2 + a^2} - a = 0.00002 = 2.0\\times 10^{-5}$. $\\longrightarrow$ (Only one significant digit) \n",
    "\n",
    "2. Rewritten formula\n",
    "\n",
    "$$\n",
    "\\frac{x^2}{\\sqrt{x^2+a^2}+a} = \\frac{0.0001}{3.00002 + 3.00000} = 4.10000 \\times 10^{-5}\n",
    "$$\n",
    "\n",
    "(More precise value is $\\frac{0.0001}{3.00002 + 3.00000} = 0.00004082476100517466$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0000166666203705\n",
      "4.082476100517466e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = 9.0001\n",
    "a = 3.\n",
    "print(np.sqrt(x))\n",
    "print(0.0001/(np.sqrt(3.00002 + 3.00000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python package for least square method (To be desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.  25. 100.]\n",
      " [  1.  36. 121.]\n",
      " [  4.  49. 144.]\n",
      " [  9.  64. 169.]\n",
      " [ 16.  81. 196.]]\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 4.]\n",
      " [ 9.]\n",
      " [16.]]\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]]\n",
      "[[0.00000000e+00]\n",
      " [1.33226763e-15]\n",
      " [5.32907052e-15]\n",
      " [1.24344979e-14]\n",
      " [2.13162821e-14]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Create A and b for a least square problem\n",
    "A = np.arange(3*5, dtype=np.float64).reshape((3,5))**2\n",
    "b = np.arange(5, dtype=np.float64).reshape((5,1))**2\n",
    "\n",
    "# 2. Solve the least square problem using pseudo inverse\n",
    "x = np.linalg.pinv(A.T)@b\n",
    "\n",
    "# 3. Check the result\n",
    "print(A.T)\n",
    "print(b)\n",
    "print(x)\n",
    "\n",
    "# 4. Check the residual\n",
    "res = A.T@x - b\n",
    "print(res)\n",
    "\n",
    "# 5. Check the residual for the true solution (cheating)\n",
    "x_ = np.array([1,0,0]).reshape((3,1))\n",
    "print(A.T@x_ - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Projection matrix)\n",
    "\n",
    "For a matrix $A\\in\\mathbb{R}^{m\\times n}$, \n",
    "\n",
    "$$\n",
    "P=A\\left(A^T A\\right)^{-1} A^T\n",
    "$$ \n",
    "\n",
    "defines the orthogonal projection from $\\mathbb{R}^{m}$ onto column space of $A$. In particular, if $A=v$ (a single column), then \n",
    "\n",
    "$$\n",
    "P=\\frac{v v^T}{v^T v}\n",
    "$$\n",
    "\n",
    "defines the orthogonal projection from ${R}^{m}$ onto the line $\\{\\lambda v \\ : \\  \\lambda \\in \\mathbb{R} \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Proof**\n",
    "\n",
    "From the least square discussion, we already know that the vector $\\bar{x} =\\left(A^T A\\right)^{-1} A^T b$ yields the projection of $b$ onto the column space of $A$. Thus, taking the linear combination of columns of $A$ with $\\bar x$ being coefficients, we obtain the projected vector of $b$. In other words, the map given by \n",
    "\n",
    "$$\n",
    "b \\mapsto A\\left(A^T A\\right)^{-1} A^T b (=A\\bar{x})\n",
    "$$\n",
    "\n",
    "defines a projection onto column space of $A$.\n",
    "\n",
    "In particular, if $A$ consists of a single column $v$, then this reads \n",
    "\n",
    "$$\n",
    "b \\mapsto \\frac{v v^T}{v^T v} b\n",
    "$$\n",
    "\n",
    " -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Numerical issue with classical Gram-Schmidt\n",
    "\n",
    "```\n",
    "d = 1e-10\n",
    "A = np.array([[1, d, 0, 0], [1, 0, d, 0], [1, 0, 0, d]], dtype=np.float64)\n",
    "\n",
    "A = A.T\n",
    "\n",
    "Q, R = qr_red_GS(A)\n",
    "\n",
    "print(\"Q^TQ\\n\", Q.T @ Q) # gives non-orthogonal for classical Gram-Schmidt\n",
    "``` -->"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
