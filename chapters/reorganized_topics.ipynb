{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropped from na02approximation.ipynb\n",
    "\n",
    "Move to M104C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Chebychev polynomial 3](https://jhparkyb.github.io/resources/notes/na/104ASlides_Approximation021.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divided differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Divided differences 1](https://jhparkyb.github.io/resources/notes/na/104ASlides_Approximation026.png)\n",
    "\n",
    "![Divided differences 2](https://jhparkyb.github.io/resources/notes/na/104ASlides_Approximation027.png)\n",
    "\n",
    "![Divided differences 3](https://jhparkyb.github.io/resources/notes/na/104ASlides_Approximation028.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Horner's algorithm: efficient evaluation of interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficient way of evaluating polynomails.\n",
    "\n",
    "![Example of Horner's algorithm](https://jhparkyb.github.io/resources/notes/na/104ASlides_Approximation010.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Finding interpolating polynomial in the Newton form 2](https://jhparkyb.github.io/resources/notes/na/104ABoardWork_Approximation002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "swapping\n",
    "<!-- **Motivating example**\n",
    "\n",
    "See the example below. There are no errors or bugs in the function `gauss_elimination2D`. But it produces something weird: `x_lib` is the correct solution, which is obtained by `numpy.linalg` (which uses LAPACK library). What has happened? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "swapping\n",
    "<!-- import numpy as np\n",
    "\n",
    "def gauss_elimination2D(AA, bb):\n",
    "    \"\"\"Solves Ax = b using Gauss elimination.\n",
    "\n",
    "    Args:\n",
    "        A: A square matrix.\n",
    "        b: A vector.\n",
    "\n",
    "    Returns:\n",
    "        x: A vector.\n",
    "    \"\"\"\n",
    "    n = 2\n",
    "    AA = A.copy()\n",
    "    bb = b.copy()\n",
    "    x = np.zeros(n).reshape(-1, 1)\n",
    "\n",
    "    m = AA[1, 0] / AA[0, 0]\n",
    "    AA[1, 1] -= m * AA[1, 0]\n",
    "    bb[1] -= m * bb[0]\n",
    "    \n",
    "    x[1] = bb[1] / AA[1, 1]\n",
    "    x[0] = (bb[0] - AA[0, 1] * x[1]) / AA[0, 0]\n",
    "    \n",
    "    return x\n",
    "\n",
    "A = np.array([[10e-20,  1.], \n",
    "              [     1.,  2.]])\n",
    "b = np.array([1., 4.]).reshape(-1, 1)\n",
    "\n",
    "x_naive = gauss_elimination2D(A, b)\n",
    "x_lib = np.linalg.solve(A, b)\n",
    "\n",
    "print(\"x_naive: \\n\", x_naive)\n",
    "print(\"x: \\n\", x) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### Inner product\n",
    "\n",
    "Inner product is a far-reaching concept that helps us tackle differential geometry, Fourier analysis, solutions of partial differential equations just to name a few. Here, we give just a bit more details to see if the $$(f,g)_w:=\\int_a^b f(x)g(x)w(x)dx$$ is indeed an inner product. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Motivating example**\n",
    "\n",
    "Recall the homework problem, where we solved the following linear system by programming 2D version of Gaussian elimination.\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "10^{-20} x_1+x_2 & =1 \\\\\n",
    "x_1+2 x_2 & =4\n",
    "\\end{aligned}\n",
    "\\right. \n",
    "$$\n",
    "\n",
    "If we programmed \"right,\" it should produce $(x_1, x_2)\\approx(0,1)$, while the package `numpy.linalg.solve` gave $(x_1, x_2)\\approx(2,1)$. \n",
    "\n",
    "What is going on? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Remark**\n",
    "\n",
    "- The source of error is that the first coefficient `A[0,0]` is so small that the computer (using IEEE double precision) \"thinks\" it is 0. \n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "10^{-20} x_1+x_2 & =1 \\\\\n",
    "x_1+2 x_2 & =4\n",
    "\\end{aligned}\n",
    "\\right. \n",
    "\\Rightarrow\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "0 x_1+x_2 & =1 \\\\\n",
    "x_1+2 x_2 & =4\n",
    "\\end{aligned}\n",
    "\\right. \n",
    "$$\n",
    "\n",
    "Then, the first equation leads to $x_2=1$, which in turn, leads to $x_1 = 0$. Hence, we have $(x_1, x_2)\\approx(0,1)$. \n",
    "\n",
    "- However, if we carry the small coefficient $10^{-20}$ and meticulously solve the equation, we obtain $(x_1, x_2)\\approx(2,1)$. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- **Idea** (Swapping or partial pivoting)\n",
    "\n",
    "- Push down a row whose pivot is small, and pull up the one with the largest coefficient in that column.\n",
    "- This procedure is called *partial pivoting*. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
