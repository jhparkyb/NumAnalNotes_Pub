{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation/Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Symbol        | meaning                                                                             |\n",
    "| ------------- | ----------------------------------------------------------------------------------- |\n",
    "| $\\delta_{ij}$ | Kronecker delta, that is $\\delta_{ij}=1$ if $i=j$, and $\\delta_{ij}=0$ if $i\\neq j$ |\n",
    "\n",
    "- We restrict all discussion to real matrices.\n",
    "  - All results stay true for complex matrices upon replacing tranpose (e.g., $A^T$, $V^T$) with conjugate transpose (e.g., $A^H$, $V^H$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary** (Case I: $m\\ge n$) \n",
    "\n",
    "- For any $m$-by-$n$ matrix $A$ ($m\\ge n$), we can choose \n",
    "  - $\\left\\{u_1, \\ldots, u_m\\right\\}$ orthonormal vectors of length $m$, (left singular vector)\n",
    "  - $\\left\\{v_1, \\ldots, v_n\\right\\}$ orthonormal vectors of length $n$, (right singular vector) and\n",
    "  - $s_1 \\geq \\cdots \\geq$ $s_{n} \\geq 0$, (singular values) satisfying\n",
    "$$\n",
    "\\begin{gathered}\n",
    "A v_1=s_1 u_1 \\\\\n",
    "A v_2=s_2 u_2 \\\\\n",
    "\\vdots \\\\\n",
    "A v_n=s_n u_n .\n",
    "\\end{gathered}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geometric Intuition** (Sauer (2017) p. 579)\n",
    "\n",
    "$v_i$'s form the basis of a rectangular coordinate system on which $A$ acts in a simple way: It produces the basis vectors of a new coordinate system, the $u_i$’s, with some stretching quantified by the scalars $s_i$'s. The stretched basis vectors $s_i u_i$ are the semimajor axes of the ellipse.\n",
    "\n",
    "![SVD geometry](https://blogs.sas.com/content/iml/files/2017/08/svd1.png)\n",
    "\n",
    "Figure: Rick Wicklin, SAS blog (Geometry of 2-by-2 SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Building SVD easy case; Sauer (2017) p. 580)\n",
    "\n",
    "(Step 1)\n",
    "\n",
    "Find the singular values and singular vectors for $A=\\begin{bmatrix}3 & 0 \\\\ 0 & 1/2 \\end{bmatrix}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### hide/show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{aligned} & A\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right]=3\\left[\\begin{array}{l}1 \\\\ 0\\end{array}\\right] \\\\ & A\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{l}0 \\\\ 1\\end{array}\\right]\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Step 2)\n",
    "\n",
    "Find the singular values and singular vectors for $A=\\begin{bmatrix}0 & -1/2 \\\\ 3 & 0 \\\\ 0 & 0 \\end{bmatrix}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### hide/show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& A\\left[\\begin{array}{l}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{array}\\right]=3\\left[\\begin{array}{l}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{array}\\right] \\\\\n",
    "& A\\left[\\begin{array}{l}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array}\\right]=\\frac{1}{2}\\left[\\begin{array}{r}\n",
    "-1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{array}\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Even for this simple matrix, guessing singular vectors is not that easy, especially when requiring the left singular vectors be orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Step 3) \n",
    "\n",
    "Can you do the same for $A=\\begin{bmatrix}2 & -1/2 \\\\ 3 & 1 \\\\ -2 & 5 \\end{bmatrix}$?\n",
    "\n",
    "We need more systematic approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamentals of SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Spectral theorem for real symmetric matrix; Rephrase of Horn and Johnson (2013) Matrix analysis 2ed. Theorem 4.1.5. p. 229)\n",
    "\n",
    "If $A$ is a real symmetric $n$-by-$n$ matrix, then there exists an orthonormal basis of $R^n$ consisting of eigenvectors of $A$. Each eigenvalue of $A$ is real.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma** \n",
    "\n",
    "Let $A$ be an $m \\times n$ matrix. The eigenvalues of $A^T A$ are nonnegative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof\n",
    "\n",
    "Let $v$ be a unit eigenvector of $A^T A$, and $A^T A v=\\lambda v$. Then\n",
    "$$\n",
    "0 \\leq\\|A v\\|^2=v^T A^T A v=\\lambda v^T v=\\lambda .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Sauer (2017) p. 581)\n",
    "\n",
    "Let $A$ be an $m \\times n$ matrix where $m \\geq n$. Then there exist two orthonormal bases $\\left\\{v_1, \\ldots, v_n\\right\\}$ of $R^n$, and $\\left\\{u_1, \\ldots, u_m\\right\\}$ of $R^m$, and real numbers $s_1 \\geq \\cdots \\geq s_n \\geq 0$ such that $A v_i=s_i u_i$ for $1 \\leq i \\leq n$. The columns of $V=\\left[v_1|\\ldots| v_n\\right]$, the right singular vectors, are the set of orthonormal eigenvectors of $A^T A$; and the columns of $U=\\left[u_1|\\ldots| u_m\\right]$, the left singular vectors, are the set of orthonormal eigenvectors of $A A^T$. That is, we have $A=USV^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructive version (Human-friendly; Sauer (2017) p. 581)\n",
    "\n",
    "1. $s_i$'s (singular values): Find eigenvalues (nonnegative) of $A^T A$ ($n$-by-$n$) in the decreasing order $s_1^2 \\ge s_2^2 \\ge \\cdots \\ge s_n^2 \\ge 0$ along with\n",
    "1. $v_i$'s (right singular vectors): corresponding eigenvectors $v_i$ ($i=1,2,\\cdots, n$).\n",
    "1. $u_i$'s (left singular vectors): If $s_i \\neq 0$, define $u_i$ by the equation $s_i u_i=A v_i$. Choose each remaining $u_i$ as an arbitrary unit vector subject to being orthogonal to $u_1, \\ldots, u_{i-1}$ ($i=1,2,\\cdots, m$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- The SVD is not unique. \n",
    "  - Replacing $v_1$ by $-v_1$ and $u_1$ by $-u_1$ does not change the equality, but changes the matrices $U$ and $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Sauer (2017) p. 581)\n",
    "\n",
    "Find the singular value decomposition of the $4 \\times 2$ matrix\n",
    "$$\n",
    "A=\\left[\\begin{array}{rr}\n",
    "3 & 3 \\\\\n",
    "-3 & -3 \\\\\n",
    "-1 & 1 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right] .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Preliminary\n",
    "\n",
    "$$\n",
    "A^T A=\\left[\\begin{array}{ll}\n",
    "20 & 16 \\\\\n",
    "16 & 20\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "1. Eigenvectors and eigenvalues \n",
    "\n",
    "$$\n",
    "v_1=\\begin{bmatrix}1 / \\sqrt{2} \\\\ 1 / \\sqrt{2}\\end{bmatrix}, \n",
    "\\quad \n",
    "v_2=\\begin{bmatrix}1 / \\sqrt{2} \\\\ -1 / \\sqrt{2}\\end{bmatrix},\n",
    "\\quad\n",
    "\\begin{array}{l}\n",
    "s_1^2=36 \\\\ \n",
    "s_2^2=4\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Singular values\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "s_1=6 \\\\ \n",
    "s_2=2\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "3. Right singular vectors\n",
    "\n",
    "$v_1, v_2$ (same as eigenvectors of $A^T A$)\n",
    "\n",
    "4. Left singular vectors\n",
    "\n",
    "From \n",
    "\n",
    "$$\n",
    "6 u_1=A v_1=\\left[\\begin{array}{r}\n",
    "3 \\sqrt{2} \\\\\n",
    "-3 \\sqrt{2} \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{array}\\right] \\quad 2 u_2=A v_2=\\left[\\begin{array}{r}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "-\\sqrt{2} \\\\\n",
    "\\sqrt{2}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "we have\n",
    "\n",
    "$$\n",
    "u_1=\\left[\\begin{array}{r}\n",
    "\\frac{1}{\\sqrt{2}} \\\\\n",
    "-\\frac{1}{\\sqrt{2}} \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{array}\\right] \\quad u_2=\\left[\\begin{array}{r}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "-\\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}}\n",
    "\\end{array}\\right] .\n",
    "$$\n",
    "\n",
    "For $i = 3, 4$, choose\n",
    "\n",
    "$$\n",
    "u_3=\\left[\\begin{array}{c}\n",
    "\\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{array}\\right] \\quad u_4=\\left[\\begin{array}{c}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "If such vectors are not easy to guess, we can use Gram-Schmidt starting with $\\{u_1, u_2, e_3, e_4\\}$, where $e_i = [\\delta_{ij}]_{1\\le j \\le 4}^T$ and $\\delta_{ij}$ is Kronecker delta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "6. SVD\n",
    "\n",
    "$$\n",
    "A=\\left[\\begin{array}{rr}\n",
    "3 & 3 \\\\\n",
    "-3 & -3 \\\\\n",
    "-1 & 1 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right]=U S V^T=\\left[\\begin{array}{rrrr}\n",
    "\\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\\n",
    "-\\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\\n",
    "0 & -\\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\\\\n",
    "0 & \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}}\n",
    "\\end{array}\\right]\\left[\\begin{array}{ll}\n",
    "6 & 0 \\\\\n",
    "0 & 2 \\\\\n",
    "0 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\n",
    "\\end{array}\\right] .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduced/Economic SVD**\n",
    "\n",
    "- The lower block of zeros in $S$ and the corresponding number of left singular vectors do not contribute to $A$.\n",
    "- Remove them, making it *reduced SVD* or *economic SVD*.\n",
    "\n",
    "\n",
    "SVD\n",
    "\n",
    "$$\n",
    "A=\\left[\\begin{array}{rr}\n",
    "3 & 3 \\\\\n",
    "-3 & -3 \\\\\n",
    "-1 & 1 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right]=\\hat U \\hat S V^T=\\left[\\begin{array}{rrrr}\n",
    "\\frac{1}{\\sqrt{2}} & 0 \\\\\n",
    "-\\frac{1}{\\sqrt{2}} & 0 \\\\\n",
    "0 & -\\frac{1}{\\sqrt{2}}\\\\\n",
    "0 & \\frac{1}{\\sqrt{2}} \n",
    "\\end{array}\\right]\\left[\\begin{array}{ll}\n",
    "6 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\n",
    "\\end{array}\\right] .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2: $m\\le n$\n",
    "\n",
    "- Find SVD of $A^T$ to get $A^T=U S V^T$. \n",
    "- Then, $A=\\left(U S V^T\\right)^T=V S^T U^T$ is the SVD of $A$. [Sauer (2017) p. 582]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Sauer (2017) p. 583)\n",
    "\n",
    "Find the singular value decomposition of the $2 \\times 3$ matrix\n",
    "$$\n",
    "A=\\left[\\begin{array}{rrr}\n",
    "-1 & 3 & 7 \\\\\n",
    "7 & 4 & 1\n",
    "\\end{array}\\right] .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Plan: Find the SVD of $A^T$ and then transpose the result. \n",
    "\n",
    "0. Preliminary\n",
    "\n",
    "$$\n",
    "A A^T=\\left[\\begin{array}{ll}\n",
    "59 & 12 \\\\\n",
    "12 & 66\n",
    "\\end{array}\\right],\n",
    "$$\n",
    "\n",
    "1. Eigenvectors and eigenvalues \n",
    "\n",
    "$$\n",
    "v_1=\\begin{bmatrix}3 / 5 \\\\ 4 / 5\\end{bmatrix}, \n",
    "\\quad \n",
    "v_2=\\begin{bmatrix}-4 / 5 \\\\ 3 / 5\\end{bmatrix},\n",
    "\\quad\n",
    "\\begin{array}{l}\n",
    "s_1^2=75 \\\\ \n",
    "s_2^2=50\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Singular values\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "s_1=5 \\sqrt{3} \\\\ \n",
    "s_2=5 \\sqrt{2}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "3. Right singular vectors\n",
    "\n",
    "$v_1, v_2$ (same as eigenvectors of $AA^T$)\n",
    "\n",
    "4. Left singular vectors\n",
    "\n",
    "From \n",
    "\n",
    "$$\n",
    "5 \\sqrt{3} u_1=A^T v_1=\\left[\\begin{array}{l}\n",
    "5 \\\\\n",
    "5 \\\\\n",
    "5\n",
    "\\end{array}\\right] \\quad 5 \\sqrt{2} u_2=A^T v_2=\\left[\\begin{array}{r}\n",
    "5 \\\\\n",
    "0 \\\\\n",
    "-5\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "we have\n",
    "\n",
    "$$\n",
    "u_1=\\left[\\begin{array}{c}\n",
    "\\frac{1}{\\sqrt{3}} \\\\\n",
    "\\frac{1}{\\sqrt{3}} \\\\\n",
    "\\frac{1}{\\sqrt{3}}\n",
    "\\end{array}\\right] \\quad u_2=\\left[\\begin{array}{c}\n",
    "\\frac{1}{\\sqrt{2}} \\\\\n",
    "0 \\\\\n",
    "-\\frac{1}{\\sqrt{2}}\n",
    "\\end{array}\\right] .\n",
    "$$\n",
    "\n",
    "For $i = 3$, choose\n",
    "\n",
    "$$\n",
    "u_3=\\left[\\begin{array}{c} \\frac{1}{\\sqrt{6}} \\\\ -\\frac{2}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{6}}\n",
    "\\end{array}\\right]\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. SVD of $A^T$\n",
    "\n",
    "$$\n",
    "A^T=\\left[\\begin{array}{rr}\n",
    "-1 & 7 \\\\\n",
    "3 & 4 \\\\\n",
    "7 & 1\n",
    "\\end{array}\\right]=\\tilde U \\tilde S \\tilde V^T=\\left[\\begin{array}{rrr}\n",
    "\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}} \\\\\n",
    "\\frac{1}{\\sqrt{3}} & 0 & -\\frac{2}{\\sqrt{6}} \\\\\n",
    "\\frac{1}{\\sqrt{3}} & -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{6}}\n",
    "\\end{array}\\right]\\left[\\begin{array}{rr}\n",
    "5 \\sqrt{3} & 0 \\\\\n",
    "0 & 5 \\sqrt{2} \\\\\n",
    "0 & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{rr}\n",
    "3 / 5 & 4 / 5 \\\\\n",
    "-4 / 5 & 3 / 5\n",
    "\\end{array}\\right],\n",
    "$$\n",
    "\n",
    "6. SVD of $A$ is\n",
    "\n",
    "$$\n",
    "A= \\left[\\begin{array}{rrr}\n",
    "-1 & 3 & 7 \\\\\n",
    "7 & 4 & 1\n",
    "\\end{array}\\right]\n",
    "=USV^T\n",
    "=\\left[\\begin{array}{rr}\n",
    "3 / 5 & 4 / 5 \\\\\n",
    "-4 / 5 & 3 / 5\n",
    "\\end{array}\\right]\\left[\\begin{array}{rrr}\n",
    "5 \\sqrt{3} & 0 & 0 \\\\\n",
    "0 & 5 \\sqrt{2} & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{rrr}\n",
    "\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} & 0 & -\\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{6}} & -\\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{6}}\n",
    "\\end{array}\\right] .\n",
    "$$\n",
    "\n",
    "Here, we note $USV^T=\\tilde V \\tilde S^T \\tilde U^T$ with equality holding factor-wisely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "7 Reduced SVD of $A$\n",
    "\n",
    "$$\n",
    "A=\\left[\\begin{array}{rrr}\n",
    "-1 & 3 & 7 \\\\\n",
    "7 & 4 & 1\n",
    "\\end{array}\\right]\n",
    "=U\\hat S \\hat V^T \n",
    "=\\left[\\begin{array}{rr}\n",
    "3 / 5 & 4 / 5 \\\\\n",
    "-4 / 5 & 3 / 5\n",
    "\\end{array}\\right]\\left[\\begin{array}{rr}\n",
    "5 \\sqrt{3} & 0 \\\\\n",
    "0 & 5 \\sqrt{2}\n",
    "\\end{array}\\right]\\left[\\begin{array}{rrr}\n",
    "\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} & 0 & -\\frac{1}{\\sqrt{2}}\n",
    "\\end{array}\\right] .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Sizes of matrices; Sauer (2017) p. 582)\n",
    "\n",
    "- SVD of $m$-by-$n$ matrix $A=USV^T$\n",
    "  - $S$ has the same size as $A$.\n",
    "- Reduced SVD of $m$-by-$n$ matrix $A=\\hat U \\hat S V^T$ with $m\\ge n$\n",
    "  - $\\hat U$ has the same size as $A$.\n",
    "- Reduced SVD of $m$-by-$n$ matrix $A=U \\hat S \\hat V^T$ with $m\\le n$\n",
    "  - $\\hat V^T$ has the same size as $A$.\n",
    "  - Warning: Here, $A=U \\hat S \\hat V^T$ is an reduced SVD of $A$, not of $A^T$ in the intermediate steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property 1** (Sauer (2017) p. 585)\n",
    "\n",
    "The rank of the matrix $A=U S V^T$ is the number of nonzero entries in $S$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Proof. Since $U$ and $V^T$ are invertible matrices, $\\operatorname{rank}(A)=\\operatorname{rank}(S)$, and the latter is the number of nonzero diagonal entries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property 2** (Sauer (2017) p. 585)\n",
    "\n",
    "If $A$ is an $n \\times n$ matrix, $|\\operatorname{det}(A)|=s_1 \\cdots s_n$.\n",
    "\n",
    "Proof. Since $U^T U=I$ and $V^T V=I$, the determinants of $U$ and $V^T$ are 1 or -1 , due to the fact that the determinant of a product equals the product of the determinants. Property 2 follows from the factorization $A=U S V^T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property 3** (Sauer (2017) p. 585)\n",
    "\n",
    "If $A$ is an invertible $m \\times m$ matrix, then $A^{-1}=V S^{-1} U^T$.\n",
    "\n",
    "Proof. By Property $1, S$ is invertible, meaning all $s_i>0$. Now Property 3 follows from the fact that if $A_1, A_2$, and $A_3$ are invertible matrices, then $\\left(A_1 A_2 A_3\\right)^{-1}=$ $A_3^{-1} A_2^{-1} A_1^{-1}$ and that $U,V$ are orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- Property 3 says that obtaining $A$ is simple once its SVD is known\n",
    "  - $V$ and $U$ are just transposed and $S^{-1}=\\mathrm{diag}(s_i^{-1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property 4** (Sauer (2017) p. 586)\n",
    "\n",
    "The $m \\times n$ matrix $A$ can be written as the sum of rank-one matrices\n",
    "\n",
    "$$\n",
    "A=\\sum_{i=1}^r s_i u_i v_i^T,\n",
    "$$\n",
    "\n",
    "where $r$ is the rank of $A$, and $u_i$ and $v_i$ are the $i$ th columns of $U$ and $V$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof. (sketch)\n",
    "\n",
    "Given $A=USV^T$, \n",
    "\n",
    "1. Split $S$ into sum of $r$ matrices of a single nonzero entry.\n",
    "2. Expand the result and carry out block multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- Each summand in Property 4 is called *rank-one* matrix.\n",
    "  - Each column is a scalar multiple of of the first column.\n",
    "  - If you haven't, write it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low-rank approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penrose Pseudoinverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Pseudoinverse; Kincaid and Cheney (2002) p. 290)\n",
    "\n",
    "For an $m$-by-$n$ matrix\n",
    "$$\n",
    "S=\\left[\\begin{array}{ccccccc}\n",
    "\\sigma_1 & \\\\\n",
    " & \\sigma_2 & \\\\\n",
    " & & \\ddots \\\\\n",
    " & & & \\sigma_r & \\\\\n",
    " & & & & 0 & \\\\\n",
    " & & & & & \\ddots & \\\\\n",
    " & & & & & & 0 \n",
    "\\end{array}\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "where $\\sigma_i > 0$ for $i=1,2,\\cdots, r$, its *pseudoinverse* is defined to be $n$-by-$m$ matrix\n",
    "\n",
    "$$\n",
    "S^+=\\left[\\begin{array}{ccccccc}\n",
    "\\sigma_1^{-1} & \\\\\n",
    " & \\sigma_2^{-1} & \\\\\n",
    " & & \\ddots \\\\\n",
    " & & & \\sigma_r^{-1} & \\\\\n",
    " & & & & 0 & \\\\\n",
    " & & & & & \\ddots & \\\\\n",
    " & & & & & & 0 \n",
    "\\end{array}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "For a general $m$-by-$n$ matrix with an SVD \n",
    "\n",
    "$$A=USV^T,$$ \n",
    "\n",
    "its pseudoinverse is defined by\n",
    "\n",
    "$$A^+=VS^+ U^T,$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Kincaid and Cheney (2002) p. 290)\n",
    "\n",
    "- Given a matrix, pseudoinverse is unique while SVD is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Kincaid and Cheney (2002) p. 291)\n",
    "\n",
    "Find the pseudoinverse of the following matrix $A$ with its SVD:\n",
    "$$\n",
    "A=\\left[\\begin{array}{rrr}\n",
    "0 & -1.6 & 0.6 \\\\\n",
    "0 & 1.2 & 0.8 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{array}\\right]=\\left[\\begin{array}{rrrr}\n",
    "0.6 & 0.8 & 0 & 0 \\\\\n",
    "0.8 & -0.6 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array}\\right]\\left[\\begin{array}{lll}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 2 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{rrr}\n",
    "0 & 0 & 1 \\\\\n",
    "0 & -1 & 0 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "A^{+} & =\\left[\\begin{array}{rrr}\n",
    "0 & 0 & 1 \\\\\n",
    "0 & -1 & 0 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{rrrr}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 0.5 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{rrrr}\n",
    "0.6 & 0.8 & 0 & 0 \\\\\n",
    "0.8 & -0.6 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array}\\right] \\\\\n",
    "& =\\left[\\begin{array}{rrrr}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "-0.4 & 0.3 & 0 & 0 \\\\\n",
    "0.6 & 0.8 & 0 & 0\n",
    "\\end{array}\\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inconsistent and Underdetermined Systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudoinverse gives a way to answer least square-type problem: over-determined AND underdetermined system of linear equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Minimal solution 1; Kincaid and Cheney (2002) p. 291)\n",
    "\n",
    "Consider a linear system\n",
    "\n",
    "$$\n",
    "A x=b\n",
    "$$\n",
    "\n",
    "where $A$ is $m \\times n$, $x$ is $n \\times 1$, and $b$ is $m \\times 1$.\n",
    "\n",
    "The *minimal solution* of this problem is defined as follows:\n",
    "1. If the system is consistent and has a unique solution, $x$, then the minimal solution is defined to be $x$.\n",
    "2. If the system is consistent and has a set of solutions, then the minimal solution is the element of this set having the least Euclidean norm.\n",
    "3. If the system is inconsistent and has a unique least-squares solution, $x$, then the minimal solution is defined to be $x$.\n",
    "4. If the system is inconsistent and has a set of least-squares solutions, then the minimal solution is the element of this set having the least Euclidean norm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Minimal solution 2; Kincaid and Cheney (2002) p. 291)\n",
    "\n",
    "The following definition is equivalent to the previous one.\n",
    "\n",
    "Let\n",
    "$$\n",
    "\\rho=\\min \\left\\{\\|A x-b\\|_2: x \\in \\mathbb{C}^n\\right\\}\n",
    "$$\n",
    "\n",
    "Then the minimal solution of equation $A x=b$ is the element of least norm in the set $K=\\left\\{x:\\|A x-b\\|_2=\\rho\\right\\}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Kincaid and Cheney (2002) p. 291)\n",
    "\n",
    "- The second definition encompasses all four cases described earlier. \n",
    "  - For example, if $\\rho=0$, we have Cases 1 and 2, whereas Cases 3 and 4 correspond to $\\rho>0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Pseudoinverse Minimal Solution; Kincaid and Cheney (2002) p. 291)\n",
    "\n",
    "The minimal solution of the equation $A x=b$ is given by the pseudoinverse\n",
    "\n",
    "$$\n",
    "x=A^{+} b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof \n",
    "\n",
    "Let a singular-value decomposition of $A$ be $A=U S V^T$. Let\n",
    "$$\n",
    "c=U^T b \\quad \\text { and } \\quad y=V^T x\n",
    "$$\n",
    "\n",
    "As $x$ runs over $R^n$, so does $y$ because $V^T$ is surjective; that is, it maps $R^n$ onto $R^n$. Therefore,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\rho & =\\inf _x\\|A x-b\\|_2=\\inf _x\\|U S V^T x-b\\|_2=\\inf _x\\left\\|U^T(U S V^T x-b)\\right\\|_2 \\\\\n",
    "& =\\inf _x\\left\\|S V^T x-U^T b\\right\\|_2=\\inf _y\\|S y-c\\|_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From the special nature of the matrix $S$, we now have\n",
    "$$\n",
    "\\|S y-c\\|_2^2=\\sum_{i=1}^r\\left(\\sigma_i y_i-c_i\\right)^2+\\sum_{i=r+1}^m c_i^2,\n",
    "$$\n",
    "\n",
    "where $r$ is the index of the smallest nonzero singular value. This quantity is minimized by letting $y_i=c_i / \\sigma_i$ for $1 \\leq i \\leq r$ and by permitting $y_{r+1}, y_{r+2}, \\ldots, y_n$ to be arbitrary. Thus, we have\n",
    "$$\n",
    "\\rho=\\left(\\sum_{i=r+1}^m c_i^2\\right)^{1 / 2}\n",
    "$$\n",
    "\n",
    "Among all the $y$-vectors that yield this minimum value $\\rho$, the vector of least norm has $y_{r+1}=y_{r+2}=\\cdots=y_n=0$. This vector is given by\n",
    "$$\n",
    "y=S^{+} c\n",
    "$$\n",
    "\n",
    "The minimal solution of our problem is therefore\n",
    "$$\n",
    "x=V y=V S^{+} c=V S^{+} U^T b=A^{+} b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Kincaid and Cheney (2002) p. 292)\n",
    "\n",
    "- The pseudoinverse plays the same role for inconsistent or underdetermined sys­tems as the inverse does for invertible systems. \n",
    "- The minimal solution of any equation $Ах = b$ is unique.\n",
    "  - The set $K=\\left\\{x:\\|A x-b\\|_2=\\rho\\right\\}$ is convex and has а unique element of least norm. (HW problem)\n",
    "  - This is not trivial because, while the pseudoinverse is determined by SVD, SVD itself is not unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Uniqueness of pseudoinverse; Kincaid and Cheney (2002) p. 293)\n",
    "\n",
    "The pseudoinverse of a matrix has the four Penrose properties. Hence, each matrix has a unique pseudoinverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: See the Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Kincaid and Cheney (2002) p. 292)\n",
    "\n",
    "Find the minimal solution of the system\n",
    "$$\n",
    "\\left\\{\\begin{array}{l}\n",
    "0 x-1.6 y+0.6 z=5 \\\\\n",
    "0 x+1.2 y+0.8 z=7 \\\\\n",
    "0 x+0 y+0 z=3 \\\\\n",
    "0 x+0 y+0 z=-2\n",
    "\\end{array}\\right.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the coefficient matrix is the same as previous example.\n",
    "\n",
    "$$\n",
    "A^{+} b=\\left[\\begin{array}{rrrr}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "-0.4 & 0.3 & 0 & 0 \\\\\n",
    "0.6 & 0.8 & 0 & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{r}\n",
    "5 \\\\\n",
    "7 \\\\\n",
    "3 \\\\\n",
    "-2\n",
    "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
    "0.0 \\\\\n",
    "0.1 \\\\\n",
    "8.6\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More comprehensive citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spectral theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Horn and Johnson (2013) Matrix analysis 2ed. Theorem 4.1.5. p. 229) \n",
    "\n",
    "A matrix $A \\in M_n$ is Hermitian if and only if there is a unitary $U \\in M_n$ and a real diagonal $\\Lambda \\in M_n$ such that $A=U \\Lambda U^*$, where $M_n$ is the set of $n$-by- $n$ complex matrices. Moreover, $A$ is real and Hermitian (that is, real symmetric) if and only if there is a real orthogonal $P \\in M_n$ and a real diagonal $\\Lambda \\in M_n$ such that $A=P \\Lambda P^T$.\n",
    "\n",
    "**Remark**\n",
    "\n",
    "- Observe the subtlety of the statement: If $A$ is symmetric as a complex matrix, then the conclusion is different. (See e.g., [Wikipedia - Complex symmetric matrices](https://en.wikipedia.org/wiki/Symmetric_matrix#Complex_symmetric_matrices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uniqueness of pseudoinverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discussions on the uniqueness of pseudoinverse are borrowed from Kincaid and Cheney (2002) pp. 293--296, including remarks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Kincaid and Cheney (2002) p. 293)\n",
    "\n",
    "- The pseudoinverse has some (but not all) of the properties of an inverse. \n",
    "  - For example, we cannot expect $A^{+} A=I$ to be true if $n>m$, because the ranks of $A^{+}, A$, and $A^{+} A$ are at most $m$, whereas $I$ is $n \\times n$. \n",
    "  - However, equations such as $A A^{+} A=A$ are true for arbitrary $A$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Penrose Properties R. Penrose [1955] according to Kincaid and Cheney (2002) p. 293)\n",
    "\n",
    "Corresponding to any matrix $A$, there exists at most one matrix $X$ having these four properties:\n",
    "1. $A X A=A$\n",
    "2. $X A X=X$\n",
    "3. $(A X)^*=A X$\n",
    "4. $(X A)^*=X A$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: (Kincaid and Cheney (2002) p. 293)\n",
    "\n",
    "Let $X$ and $Y$ be two matrices having Properties $1-4$. Then by systematic use of these properties as indicated, we have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X & =X A X       & \\text{(property 2)}\\\\\n",
    "& =X A Y A X       & \\text{(property 1)} \\\\\n",
    "& =X A Y A Y A Y A X       & \\text{(property 1)} \\\\\n",
    "& =(X A)^*(Y A)^* Y(A Y)^*(A X)^*       & \\text{(property 4, 3)} \\\\\n",
    "& =A^* X^* A^* Y^* Y Y^* A^* X^* A^* & \\\\\n",
    "& =(A X A)^* Y^* Y Y^*(A X A)^* & \\\\\n",
    "& =A^* Y^* Y Y^* A^*       & \\text{(property 1)}\\\\\n",
    "& =(Y A)^* Y(A Y)^*  & \\\\\n",
    "& =Y A Y A Y         & \\text{(property 4, 3)}\\\\\n",
    "& =Y A Y         & \\text{(property 2)}\\\\\n",
    "& =Y          & \\text{(property 2)}\\\\\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Uniqueness of pseudoinverse; Kincaid and Cheney (2002) p. 293)\n",
    "\n",
    "The pseudoinverse of a matrix has the four Penrose properties. Hence, each matrix has a unique pseudoinverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: (Kincaid and Cheney (2002) p. 293)\n",
    "\n",
    "We address only property 1.\n",
    "\n",
    "Let $A$ be any matrix, and let its singular-value decomposition be\n",
    "$$\n",
    "A=P D Q\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "A^{+}=Q^* D^{+} P^*\n",
    "$$\n",
    "\n",
    "If $A$ is $m \\times n$, then so is $D$, and $D$ has the form\n",
    "$$\n",
    "D_{i j}= \\begin{cases}\\sigma_i & \\text { if } i=j \\leq r \\\\ 0 & \\text { otherwise }\\end{cases}\n",
    "$$\n",
    "\n",
    "From this we can prove that\n",
    "$$\n",
    "D D^{+} D=D\n",
    "$$\n",
    "\n",
    "To do so, we write\n",
    "$$\n",
    "\\left(D D^{+} D\\right)_{i j}=\\sum_{\\nu=1}^n D_{i v} \\sum_{\\mu=1}^m D_{v \\mu}^{+} D_{\\mu j}\n",
    "$$\n",
    "\n",
    "The right-hand side will be 0 unless $i \\leq r$ and $j \\leq r$ because of the presence of the terms $D_{i \\nu}$ and $D_{\\mu j}$. Thus, we assume $i \\leq r$ and $j \\leq r$ and continue, simplifying the right-hand side to\n",
    "$$\n",
    "\\sum_{\\nu=1}^r D_{i \\nu} \\sum_{\\mu=1}^r D_{\\nu \\mu}^{+} D_{\\mu j}=\\sigma_i \\sum_{\\mu=1}^r D_{i \\mu}^{+} D_{\\mu j}=\\sigma_i \\sigma_i^{-1} D_{i j}=D_{i j}\n",
    "$$\n",
    "\n",
    "By similar reasoning, we prove that $D^{+}$has the remaining three Penrose properties relative to $D$. Then it is a simple matter to prove these four properties for $A^{+}$. For example, the first property is proved as follows:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A A^{+} A & =P D Q Q^* D^{+} P^* P D Q \\\\\n",
    "& =P D D^{+} D Q \\\\\n",
    "& =P D Q=A\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Visualization of SVD)\n",
    "\n",
    "- $x=\\left[\\begin{array}{cccc}-10 & -10 & 20 & 20 \\\\ -10 & 20 & 20 & -10\\end{array}\\right]$ \n",
    "- $A=\\left[\\begin{array}{cc}1 & 0.3 \\\\ 0.45 & 1.2\\end{array}\\right]$.\n",
    "- $A=USV^T = \\begin{bmatrix} -0.5819 & -0.8133 \\\\ -0.8133 & 0.5819 \\end{bmatrix} \\begin{bmatrix} 1.4907 & 0 \\\\ 0 & 0.7144 \\end{bmatrix} \\begin{bmatrix} -0.6359 & -0.7718 \\\\ -0.7718 & 0.6359 \\end{bmatrix}$.\n",
    "\n",
    "Example and figures: Alyssa Quek ([SVD visualization](https://alyssaq.github.io/2015/singular-value-decomposition-visualisation/))\n",
    "\n",
    "- This example is replaced by the picture of transformation of a circle to an ellipse.\n",
    "  - It is in favor of simplitiy and to focus only on the intuition.\n",
    "  - However, via this example, you can keep track of numerical values while transformation, hence kept in appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | |\n",
    "|---|---|\n",
    "| $$Ax$$ <br> ![Figure 1](https://alyssaq.github.io/blog/images/eigens-transformation_matrix.png) | | \n",
    "| $$V^Tx$$ <br> ![Figure 2](https://alyssaq.github.io/blog/images/svd_Vx.png) | $$SV^Tx$$ <br> ![Figure 3](https://alyssaq.github.io/blog/images/svd_SVx.png) | \n",
    "| $$USV^Tx$$ <br> ![Figure 4](https://alyssaq.github.io/blog/images/svd_USVx.png) | |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrch",
   "language": "python",
   "name": "ptrch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
