{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation/Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Symbol        | meaning                                                                             |\n",
    "| ------------- | ----------------------------------------------------------------------------------- |\n",
    "| $\\delta_{ij}$ | Kronecker delta, that is $\\delta_{ij}=1$ if $i=j$, and $\\delta_{ij}=0$ if $i\\neq j$ |\n",
    "\n",
    "- We restrict most discussions to real matrices.\n",
    "  - All results stay true for complex matrices upon replacing tranpose (e.g., $A^T$, $V^T$) with conjugate transpose (e.g., $A^H$, $V^H$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea**\n",
    "\n",
    "- Any matrix, viewed as a linear map, is essentially stretching along orthogonal directions.\n",
    "  - A linear map is completely determined by its actions on basis vectors.\n",
    "    - Recall how to construct matrix representation of a linear map $T:V\\to W$.\n",
    "  - It is remarkable that there exist (a) orthonormal bases, one on the domain and the other on the codomain, that (b) make any linear map nothing but scalar multiplications with respect to those bases.\n",
    "  - The name *singular* comes from the surprise that the pineering mathematicians felt: too good to be normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geometric Intuition** (Sauer (2017) p. 579)\n",
    "\n",
    "$v_i$'s form the basis of a rectangular coordinate system on which $A$ acts in a simple way: It produces the basis vectors of a new coordinate system, the $u_i$’s, with some stretching quantified by the scalars $s_i$'s. The stretched basis vectors $s_i u_i$ are the semimajor axes of the ellipse.\n",
    "\n",
    "![SVD geometry](https://www.researchgate.net/profile/Gowtham-Sivaraman/publication/312040021/figure/fig1/AS:654753159716872@1533116726789/Geometric-interpretation-of-the-SVD-s-1-and-s-2-denotes-the-principal-radii-of-the.png)\n",
    "\n",
    "Figure: Gowtham Sivaraman (Geometry of 2-by-2 SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamentals of SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Spectral theorem for real symmetric matrix; Rephrase of Horn and Johnson (2013) Matrix analysis 2ed. Theorem 4.1.5. p. 229)\n",
    "\n",
    "If $A$ is a real symmetric $n$-by-$n$ matrix, then there exists an orthonormal basis of $R^n$ consisting of eigenvectors of $A$. Each eigenvalue of $A$ is real.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma** \n",
    "\n",
    "Let $A$ be an $m \\times n$ matrix. The eigenvalues of $A^T A$ are nonnegative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof\n",
    "\n",
    "Let $v$ be a unit eigenvector of $A^T A$, and $A^T A v=\\lambda v$. Then\n",
    "$$\n",
    "0 \\leq\\|A v\\|^2=v^T A^T A v=\\lambda v^T v=\\lambda .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Sauer (2017) p. 581)\n",
    "\n",
    "Let $A$ be an $m \\times n$ matrix where $m \\geq n$. Then there exist two orthonormal bases $\\left\\{v_1, \\ldots, v_n\\right\\}$ of $R^n$, and $\\left\\{u_1, \\ldots, u_m\\right\\}$ of $R^m$, and real numbers $s_1 \\geq \\cdots \\geq s_n \\geq 0$ such that $A v_i=s_i u_i$ for $1 \\leq i \\leq n$. The columns of $V=\\left[v_1|\\ldots| v_n\\right]$, the right singular vectors, are the set of orthonormal eigenvectors of $A^T A$; and the columns of $U=\\left[u_1|\\ldots| u_m\\right]$, the left singular vectors, are the set of orthonormal eigenvectors of $A A^T$. That is, we have $A=USV^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructive version (Human-friendly; Sauer (2017) p. 581)\n",
    "\n",
    "1. $s_i$'s (singular values): Find eigenvalues (nonnegative) of $A^T A$ ($n$-by-$n$) in the decreasing order $s_1^2 \\ge s_2^2 \\ge \\cdots \\ge s_n^2 \\ge 0$ along with\n",
    "1. $v_i$'s (right singular vectors): corresponding eigenvectors $v_i$ ($i=1,2,\\cdots, n$).\n",
    "1. $u_i$'s (left singular vectors): If $s_i \\neq 0$, define $u_i$ by the equation $s_i u_i=A v_i$. Choose each remaining $u_i$ as an arbitrary unit vector subject to being orthogonal to $u_1, \\ldots, u_{i-1}$ ($i=1,2,\\cdots, m$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- $u_i$'s are automatically mutually orthogonal. (Why?)\n",
    "- The SVD is not unique. \n",
    "  - Replacing $v_1$ by $-v_1$ and $u_1$ by $-u_1$ does not change the equality, but changes the matrices $U$ and $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Sauer (2017) p. 581)\n",
    "\n",
    "Find the singular value decomposition of the $4 \\times 2$ matrix\n",
    "$$\n",
    "A=\\left[\\begin{array}{rr}\n",
    "3 & 3 \\\\\n",
    "-3 & -3 \\\\\n",
    "-1 & 1 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right] .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Preliminary\n",
    "\n",
    "$$\n",
    "A^T A=\\left[\\begin{array}{ll}\n",
    "20 & 16 \\\\\n",
    "16 & 20\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "1. Eigenvectors and eigenvalues \n",
    "\n",
    "$$\n",
    "v_1=\\begin{bmatrix}1 / \\sqrt{2} \\\\ 1 / \\sqrt{2}\\end{bmatrix}, \n",
    "\\quad \n",
    "v_2=\\begin{bmatrix}1 / \\sqrt{2} \\\\ -1 / \\sqrt{2}\\end{bmatrix},\n",
    "\\quad\n",
    "\\begin{array}{l}\n",
    "s_1^2=36 \\\\ \n",
    "s_2^2=4\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Singular values\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "s_1=6 \\\\ \n",
    "s_2=2\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "3. Right singular vectors\n",
    "\n",
    "$v_1, v_2$ (same as eigenvectors of $A^T A$)\n",
    "\n",
    "4. Left singular vectors\n",
    "\n",
    "From \n",
    "\n",
    "$$\n",
    "6 u_1=A v_1=\\left[\\begin{array}{r}\n",
    "3 \\sqrt{2} \\\\\n",
    "-3 \\sqrt{2} \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{array}\\right] \\quad 2 u_2=A v_2=\\left[\\begin{array}{r}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "-\\sqrt{2} \\\\\n",
    "\\sqrt{2}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "we have\n",
    "\n",
    "$$\n",
    "u_1=\\left[\\begin{array}{r}\n",
    "\\frac{1}{\\sqrt{2}} \\\\\n",
    "-\\frac{1}{\\sqrt{2}} \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{array}\\right] \\quad u_2=\\left[\\begin{array}{r}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "-\\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}}\n",
    "\\end{array}\\right] .\n",
    "$$\n",
    "\n",
    "For $i = 3, 4$, choose\n",
    "\n",
    "$$\n",
    "u_3=\\left[\\begin{array}{c}\n",
    "\\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{array}\\right] \\quad u_4=\\left[\\begin{array}{c}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "If such vectors are not easy to guess, we can use Gram-Schmidt starting with $\\{u_1, u_2, e_3, e_4\\}$, where $e_i = [\\delta_{ij}]_{1\\le j \\le 4}^T$ and $\\delta_{ij}$ is Kronecker delta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "6. SVD\n",
    "\n",
    "$$\n",
    "A=\\left[\\begin{array}{rr}\n",
    "3 & 3 \\\\\n",
    "-3 & -3 \\\\\n",
    "-1 & 1 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right]=U S V^T=\\left[\\begin{array}{rrrr}\n",
    "\\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\\n",
    "-\\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\\n",
    "0 & -\\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\\\\n",
    "0 & \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}}\n",
    "\\end{array}\\right]\\left[\\begin{array}{ll}\n",
    "6 & 0 \\\\\n",
    "0 & 2 \\\\\n",
    "0 & 0 \\\\\n",
    "0 & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\n",
    "\\end{array}\\right] .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduced/Economic SVD**\n",
    "\n",
    "- The lower block of zeros in $S$ and the corresponding number of left singular vectors do not contribute to $A$.\n",
    "- Remove them, making it *reduced SVD* or *economic SVD*.\n",
    "\n",
    "\n",
    "SVD\n",
    "\n",
    "$$\n",
    "A=\\left[\\begin{array}{rr}\n",
    "3 & 3 \\\\\n",
    "-3 & -3 \\\\\n",
    "-1 & 1 \\\\\n",
    "1 & -1\n",
    "\\end{array}\\right]=\\hat U \\hat S V^T=\\left[\\begin{array}{rrrr}\n",
    "\\frac{1}{\\sqrt{2}} & 0 \\\\\n",
    "-\\frac{1}{\\sqrt{2}} & 0 \\\\\n",
    "0 & -\\frac{1}{\\sqrt{2}}\\\\\n",
    "0 & \\frac{1}{\\sqrt{2}} \n",
    "\\end{array}\\right]\\left[\\begin{array}{ll}\n",
    "6 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
    "\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n",
    "\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}}\n",
    "\\end{array}\\right] .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicker question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case 2: $m\\le n$\n",
    "\n",
    "- Find SVD of $A^T$ to get $A^T=U S V^T$. \n",
    "- Then, $A=\\left(U S V^T\\right)^T=V S^T U^T$ is the SVD of $A$. [Sauer (2017) p. 582]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Sizes of matrices; Sauer (2017) p. 582)\n",
    "\n",
    "- SVD of $m$-by-$n$ matrix $A=USV^T$\n",
    "  - $S$ has the same size as $A$.\n",
    "- Reduced SVD of $m$-by-$n$ matrix $A=\\hat U \\hat S V^T$ with $m\\ge n$\n",
    "  - $\\hat U$ has the same size as $A$.\n",
    "- Reduced SVD of $m$-by-$n$ matrix $A=U \\hat S \\hat V^T$ with $m\\le n$\n",
    "  - $\\hat V^T$ has the same size as $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- The geometric intuition of SVD is, in indeed, carried out in a literal sense.\n",
    "  - $V^T=V^{-1}$ is the change of basis matrix from standard basis $E=\\{e_i \\in R^n \\ : \\ [e_i]_j = \\delta_{ij} \\}$ to $V=[v_1 | v_2 | \\cdots | v_n]$ as a column stack of $\\{v_1, v_2, \\cdots, v_n \\}$.\n",
    "    - Input vector in standard basis is changed to a coordinate in $V$.\n",
    "  - Aligned in directions in $v_i$, the input vector is scaled by $s_i$.\n",
    "  - With stretch applied in each direction $v_i$, which results in coordinate vector in $u_i$'s, recover the ouput in standard basis by taking linear combination of $u_i$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, suppose $A=USV^T$ is an SVD of $m$-by-$n$ matrix $A$ with $s_r$ is the smallest nonzero singular value: $s_1 \\ge \\cdots \\ge s_r > s_{r+1} = \\cdots 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property 1** (Sauer (2017) p. 585)\n",
    "\n",
    "The rank of the matrix $A=U S V^T$ is the number of nonzero entries in $S$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Proof. Since $U$ and $V^T$ are invertible matrices, $\\operatorname{rank}(A)=\\operatorname{rank}(S)$, and the latter is the number of nonzero diagonal entries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property 2** (Sauer (2017) p. 585)\n",
    "\n",
    "If $A$ is an $n \\times n$ matrix, $|\\operatorname{det}(A)|=s_1 \\cdots s_n$.\n",
    "\n",
    "Proof. Since $U^T U=I$ and $V^T V=I$, the determinants of $U$ and $V^T$ are 1 or -1 , due to the fact that the determinant of a product equals the product of the determinants. Property 2 follows from the factorization $A=U S V^T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property 3** (Sauer (2017) p. 585)\n",
    "\n",
    "If $A$ is an invertible $m \\times m$ matrix, then $A^{-1}=V S^{-1} U^T$.\n",
    "\n",
    "Proof. By Property $1, S$ is invertible, meaning all $s_i>0$. Now Property 3 follows from the fact that if $A_1, A_2$, and $A_3$ are invertible matrices, then $\\left(A_1 A_2 A_3\\right)^{-1}=$ $A_3^{-1} A_2^{-1} A_1^{-1}$ and that $U,V$ are orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- Property 3 says that obtaining $A^{-1}$ is simple once its SVD is known\n",
    "  - $V$ and $U$ are just transposed and $S^{-1}=\\mathrm{diag}(s_i^{-1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property 4** (Sauer (2017) p. 586)\n",
    "\n",
    "The $m \\times n$ matrix $A$ can be written as the sum of rank-one matrices\n",
    "\n",
    "$$\n",
    "A=\\sum_{i=1}^r s_i u_i v_i^T,\n",
    "$$\n",
    "\n",
    "where $r$ is the rank of $A$, and $u_i$ and $v_i$ are the $i$ th columns of $U$ and $V$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof. (sketch)\n",
    "\n",
    "Given $A=USV^T$, \n",
    "\n",
    "1. Split $S$ into sum of $r$ matrices of a single nonzero entry.\n",
    "2. Expand the result and carry out block multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- Each summand in Property 4 is called *rank-one* matrix.\n",
    "  - Each column is a scalar multiple of of the first column.\n",
    "  - If you haven't, write it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property 5**\n",
    "\n",
    "range $(A)=\\left\\langle u_1, \\ldots, u_r\\right\\rangle$ and $\\operatorname{null}(A)=\\left\\langle v_{r+1}, \\ldots, v_n\\right\\rangle$, where $\\left\\langle u_1, \\ldots, u_r\\right\\rangle = \\mathrm{span}\\{u_1, \\ldots, u_r\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof. This is a consequence of the fact that $\\operatorname{range}(S)=\\left\\langle e_1, \\ldots, e_r\\right\\rangle \\subseteq R^m$ and $\\operatorname{null}(S)=\\left\\langle e_{r+1}, \\ldots, e_n\\right\\rangle \\subseteq R^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Computational application of SVD; Trefethen and Bau Numerical Linear Algebra p. 36)\n",
    "\n",
    "- Once one can compute it, the SVD can be used as a tool for all kinds of problems. \n",
    "  - Rank: The best method for determining the rank of a matrix is to count the number of singular values greater than a judiciously chosen tolerance (Property 1). \n",
    "  - Range and Null space: The most accurate method for finding an orthonormal basis of a range or a nullspace is via Property 5. \n",
    "    - QR factorization provides alternative algorithms that are faster but not always as accurate.\n",
    "  - Low rank approximation: Property 4 is a basis of low-rank approximations (this is the next topic). \n",
    "  - Besides these examples, the SVD is also an ingredient in robust algorithms for least squares fitting, intersection of subspaces, regularization, and numerous other problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low-rank approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Recall rank one sum expansion of $A$, using unit vectors, \n",
    "\n",
    "$$\n",
    "A=\\sum_{i=1}^r s_i u_i v_i^T,\n",
    "$$\n",
    "\n",
    "The larger $s_j$ is, the more contribution of the term to make up $A$. \n",
    "$\\longrightarrow$ Truncate at some point where $s_j$ drops to small numbers, say index $p$. \n",
    "\n",
    "\n",
    "$$\n",
    "A\\approx \\sum_{i=1}^p s_i u_i v_i^T = A_p,\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Low rank approximation; Trefethen and Bau Numerical Linear Algebra p. 35)\n",
    "\n",
    "For any $\\nu$ with $0 \\leq \\nu \\leq r$, define\n",
    "$$\n",
    "A_\\nu=\\sum_{j=1}^\\nu \\sigma_j u_j v_j^T\n",
    "$$\n",
    "if $\\nu=p=\\min \\{m, n\\}$, define $\\sigma_{\\nu+1}=0$. Then\n",
    "$$\n",
    "\\left\\|A-A_\\nu\\right\\|_2=\\inf _{\\substack{B \\in R^{m \\times n} \\\\ \\operatorname{rank}(B) \\leq \\nu}}\\|A-B\\|_2=\\sigma_{\\nu+1} .\n",
    "$$\n",
    "\n",
    "In other words, $A_\\nu$ is the best approximation  of $A$ in matrix 2-norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- We do not want to discuss details on matrix 2-norm here. But we want to say that the low rank approximation gives the best approximation in some norm that measures the distance between two different vectors.\n",
    "- Matrix 2-norm, also called spectral norm, is defined to be the largest singular value of the matrix.\n",
    "  - This measures the maximum stretch that a matrix multiplication results in.\n",
    "  - $\\Vert A \\Vert_2:=\\max_{\\Vert x \\Vert_2=1} \\Vert Ax \\Vert_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can use low rank approximation to compress data such as images.\n",
    "\n",
    "1. Load an image as matrix of color scales.\n",
    "2. Take SVD.\n",
    "3. Take a low rank approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | |\n",
    "|---|---|\n",
    "| ![original rank (480)](https://www.mathworks.com/help/examples/matlab/win64/ImageCompressionWithLowRankSVDExample_01.png) |  ![original rank (288)](https://www.mathworks.com/help/examples/matlab/win64/ImageCompressionWithLowRankSVDExample_02.png) | \n",
    "| ![rank (48)](https://www.mathworks.com/help/examples/matlab/win64/ImageCompressionWithLowRankSVDExample_03.png) | ![rank (15)](https://www.mathworks.com/help/examples/matlab/win64/ImageCompressionWithLowRankSVDExample_04.png) |\n",
    "\n",
    "Figure: MathWorks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Settings\n",
    "\n",
    "- Abudance of data: $a_j\\in R^m$ ($j=1,2,\\cdots,n$) with $m\\ll n$.\n",
    "- Data are centered: $\\frac 1 n \\sum_{j=1}^n [a_j]_i = 0$ for $i=1,2,\\cdots,m$.\n",
    "  - Each component is mean zero across the data.\n",
    "  - If not, subtract the average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Want\n",
    "\n",
    "- Find $p$-dimensional subspace of $R^m$ spanned by $p$ orthonormal vectors onto which data $a_j$'s are projected, yielding least square error caused by projection among all such subspaces.\n",
    "- Also, find the projected data onto that space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PCA](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/1280px-GaussianScatterPCA.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dimension reduction](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/GUID-D7896DEA-1569-4F15-B83D-21BF41CB3511-web.png)\n",
    "\n",
    "Figure: Wikipedia (top), ArcGIS Pro (bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea**: Low rank approximation suggests choosing the first $p$ terms of rank one expansion since \n",
    "$$\n",
    "A \\approx A_p=\\sum_{i=1}^p s_i u_i v_i^T\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The projection of a collection of vectors $a_1, \\ldots, a_n$ to their best least squares $p$-dimensional subspace is precisely the best rank- $p$ approximation matrix $A_p$.\n",
    "  - The space $\\left\\langle u_1, \\ldots, u_p\\right\\rangle$ spanned by the left singular vectors $u_1, \\ldots, u_p$ is the best approximating dimension-$p$ subspace to $a_1, \\ldots, a_n$ in the sense of least squares\n",
    "  - The orthogonal projections of the columns $a_j$ of $A$ into this space are the columns of $A_p$. (Exercise: Prove this.)\n",
    "  - The vectors $u_i$ are often called the *principal components* of the data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Find the best one-dimensional subspace fitting the data vectors $[-4,-4.5],[0.8,1.9]$, $[2.6,-0.7],[0.6,3.3]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Put\n",
    "$$\n",
    "A=\\left[\\begin{array}{rrrr}\n",
    "-4 & 0.8 & 2.6 & 0.6 \\\\\n",
    "-4.5 & 1.9 & -0.7 & 3.3\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "2. Find its reduced SVD\n",
    "$$\n",
    "U S V^T=\\left[\\begin{array}{rr}\n",
    "0.6 & -0.8 \\\\\n",
    "0.8 & 0.6\n",
    "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
    "5 \\sqrt{2} & 0 \\\\\n",
    "0 & 3\n",
    "\\end{array}\\right]\\left[\\begin{array}{cccc}\n",
    "-0.6 \\sqrt{2} & 0.2 \\sqrt{2} & 0.1 \\sqrt{2} & 0.3 \\sqrt{2} \\\\\n",
    "1 / 6 & 1 / 6 & -5 / 6 & 1 / 2\n",
    "\\end{array}\\right] .\n",
    "$$\n",
    "\n",
    "3. The best one-dimensional subspace: $\\mathrm{span}\\{u_1=[0.6,0.8]^T\\}$. \n",
    "4. Projected data onto this subspace: $s_1 u_1 v_1^T$, which is also equal to the following by zeroing $s_2$ from the reduced SVD:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& A_1=\\left[\\begin{array}{rr}\n",
    "0.6 & -0.8 \\\\\n",
    "0.8 & 0.6\n",
    "\\end{array}\\right]\\left[\\begin{array}{cc}\n",
    "5 \\sqrt{2} & 0 \\\\\n",
    "0 & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{cccc}\n",
    "-0.6 \\sqrt{2} & 0.2 \\sqrt{2} & 0.1 \\sqrt{2} & 0.3 \\sqrt{2} \\\\\n",
    "1 / 6 & 1 / 6 & -5 / 6 & 1 / 2\n",
    "\\end{array}\\right] \\\\\n",
    "& =\\left[\\begin{array}{llll}\n",
    "-3.6 & 1.2 & 0.6 & 1.8 \\\\\n",
    "-4.8 & 1.6 & 0.8 & 2.4\n",
    "\\end{array}\\right] \\\\\n",
    "&\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penrose Pseudoinverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Pseudoinverse; Kincaid and Cheney (2002) p. 290)\n",
    "\n",
    "For an $m$-by-$n$ matrix\n",
    "$$\n",
    "S=\\left[\\begin{array}{ccccccc}\n",
    "\\sigma_1 & \\\\\n",
    " & \\sigma_2 & \\\\\n",
    " & & \\ddots \\\\\n",
    " & & & \\sigma_r & \\\\\n",
    " & & & & 0 & \\\\\n",
    " & & & & & \\ddots & \\\\\n",
    " & & & & & & 0 \n",
    "\\end{array}\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "where $\\sigma_i > 0$ for $i=1,2,\\cdots, r$, its *pseudoinverse* is defined to be $n$-by-$m$ matrix\n",
    "\n",
    "$$\n",
    "S^+=\\left[\\begin{array}{ccccccc}\n",
    "\\sigma_1^{-1} & \\\\\n",
    " & \\sigma_2^{-1} & \\\\\n",
    " & & \\ddots \\\\\n",
    " & & & \\sigma_r^{-1} & \\\\\n",
    " & & & & 0 & \\\\\n",
    " & & & & & \\ddots & \\\\\n",
    " & & & & & & 0 \n",
    "\\end{array}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "For a general $m$-by-$n$ matrix with an SVD \n",
    "\n",
    "$$A=USV^T,$$ \n",
    "\n",
    "its pseudoinverse is defined by\n",
    "\n",
    "$$A^+=VS^+ U^T,$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Kincaid and Cheney (2002) p. 290)\n",
    "\n",
    "- Given a matrix, pseudoinverse is unique while SVD is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Kincaid and Cheney (2002) p. 291)\n",
    "\n",
    "Find the pseudoinverse of the following matrix $A$ with its SVD:\n",
    "$$\n",
    "A=\\left[\\begin{array}{rrr}\n",
    "0 & -1.6 & 0.6 \\\\\n",
    "0 & 1.2 & 0.8 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{array}\\right]=\\left[\\begin{array}{rrrr}\n",
    "0.6 & 0.8 & 0 & 0 \\\\\n",
    "0.8 & -0.6 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array}\\right]\\left[\\begin{array}{lll}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 2 & 0 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{rrr}\n",
    "0 & 0 & 1 \\\\\n",
    "0 & -1 & 0 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "A^{+} & =\\left[\\begin{array}{rrr}\n",
    "0 & 0 & 1 \\\\\n",
    "0 & -1 & 0 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{rrrr}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 0.5 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{rrrr}\n",
    "0.6 & 0.8 & 0 & 0 \\\\\n",
    "0.8 & -0.6 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{array}\\right] \\\\\n",
    "& =\\left[\\begin{array}{rrrr}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "-0.4 & 0.3 & 0 & 0 \\\\\n",
    "0.6 & 0.8 & 0 & 0\n",
    "\\end{array}\\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inconsistent and Underdetermined Systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudoinverse gives a way to answer least square-type problem: over-determined AND underdetermined system of linear equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Minimal solution 1; Kincaid and Cheney (2002) p. 291)\n",
    "\n",
    "Consider a linear system\n",
    "\n",
    "$$\n",
    "A x=b\n",
    "$$\n",
    "\n",
    "where $A$ is $m \\times n$, $x$ is $n \\times 1$, and $b$ is $m \\times 1$.\n",
    "\n",
    "The *minimal solution* of this problem is defined as follows:\n",
    "1. If the system is consistent and has a unique solution, $x$, then the minimal solution is defined to be $x$.\n",
    "2. If the system is consistent and has a set of solutions, then the minimal solution is the element of this set having the least Euclidean norm.\n",
    "3. If the system is inconsistent and has a unique least-squares solution, $x$, then the minimal solution is defined to be $x$.\n",
    "4. If the system is inconsistent and has a set of least-squares solutions, then the minimal solution is the element of this set having the least Euclidean norm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Minimal solution 2; Kincaid and Cheney (2002) p. 291)\n",
    "\n",
    "The following definition is equivalent to the previous one.\n",
    "\n",
    "Let\n",
    "$$\n",
    "\\rho=\\min \\left\\{\\|A x-b\\|_2: x \\in \\mathbb{C}^n\\right\\}\n",
    "$$\n",
    "\n",
    "Then the minimal solution of equation $A x=b$ is the element of least norm in the set $K=\\left\\{x:\\|A x-b\\|_2=\\rho\\right\\}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Kincaid and Cheney (2002) p. 291)\n",
    "\n",
    "- The second definition encompasses all four cases described earlier. \n",
    "  - For example, if $\\rho=0$, we have Cases 1 and 2, whereas Cases 3 and 4 correspond to $\\rho>0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Pseudoinverse Minimal Solution; Kincaid and Cheney (2002) p. 291)\n",
    "\n",
    "The minimal solution of the equation $A x=b$ is given by the pseudoinverse\n",
    "\n",
    "$$\n",
    "x=A^{+} b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof \n",
    "\n",
    "Let a singular-value decomposition of $A$ be $A=U S V^T$. Let\n",
    "$$\n",
    "c=U^T b \\quad \\text { and } \\quad y=V^T x\n",
    "$$\n",
    "\n",
    "As $x$ runs over $R^n$, so does $y$ because $V^T$ is surjective; that is, it maps $R^n$ onto $R^n$. Therefore,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\rho & =\\inf _x\\|A x-b\\|_2=\\inf _x\\|U S V^T x-b\\|_2=\\inf _x\\left\\|U^T(U S V^T x-b)\\right\\|_2 \\\\\n",
    "& =\\inf _x\\left\\|S V^T x-U^T b\\right\\|_2=\\inf _y\\|S y-c\\|_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From the special nature of the matrix $S$, we now have\n",
    "$$\n",
    "\\|S y-c\\|_2^2=\\sum_{i=1}^r\\left(\\sigma_i y_i-c_i\\right)^2+\\sum_{i=r+1}^m c_i^2,\n",
    "$$\n",
    "\n",
    "where $r$ is the index of the smallest nonzero singular value. This quantity is minimized by letting $y_i=c_i / \\sigma_i$ for $1 \\leq i \\leq r$ and by permitting $y_{r+1}, y_{r+2}, \\ldots, y_n$ to be arbitrary. Thus, we have\n",
    "$$\n",
    "\\rho=\\left(\\sum_{i=r+1}^m c_i^2\\right)^{1 / 2}\n",
    "$$\n",
    "\n",
    "Among all the $y$-vectors that yield this minimum value $\\rho$, the vector of least norm has $y_{r+1}=y_{r+2}=\\cdots=y_n=0$. This vector is given by\n",
    "$$\n",
    "y=S^{+} c\n",
    "$$\n",
    "\n",
    "Since $V$ preserves the 2-norm, the minimality of 2-norm of $y$ carries over to $x$. Therefore, the minimal solution of our problem is, \n",
    "$$\n",
    "x=V y=V S^{+} c=V S^{+} U^T b=A^{+} b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Kincaid and Cheney (2002) p. 292)\n",
    "\n",
    "- The pseudoinverse plays the same role for inconsistent or underdetermined sys­tems as the inverse does for invertible systems. \n",
    "- The minimal solution of any equation $Ах = b$ is unique.\n",
    "  - The set $K=\\left\\{x:\\|A x-b\\|_2=\\rho\\right\\}$ is convex and has а unique element of least norm. (HW problem)\n",
    "  - This is not trivial because, while the pseudoinverse is determined by SVD, SVD itself is not unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Uniqueness of pseudoinverse; Kincaid and Cheney (2002) p. 293)\n",
    "\n",
    "The pseudoinverse of a matrix has the four Penrose properties. Hence, each matrix has a unique pseudoinverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: See the Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Kincaid and Cheney (2002) p. 292)\n",
    "\n",
    "Find the minimal solution of the system\n",
    "$$\n",
    "\\left\\{\\begin{array}{l}\n",
    "0 x-1.6 y+0.6 z=5 \\\\\n",
    "0 x+1.2 y+0.8 z=7 \\\\\n",
    "0 x+0 y+0 z=3 \\\\\n",
    "0 x+0 y+0 z=-2\n",
    "\\end{array}\\right.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the coefficient matrix is the same as previous example.\n",
    "\n",
    "$$\n",
    "A^{+} b=\\left[\\begin{array}{rrrr}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "-0.4 & 0.3 & 0 & 0 \\\\\n",
    "0.6 & 0.8 & 0 & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{r}\n",
    "5 \\\\\n",
    "7 \\\\\n",
    "3 \\\\\n",
    "-2\n",
    "\\end{array}\\right]=\\left[\\begin{array}{l}\n",
    "0.0 \\\\\n",
    "0.1 \\\\\n",
    "8.6\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More comprehensive citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spectral theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Horn and Johnson (2013) Matrix analysis 2ed. Theorem 4.1.5. p. 229) \n",
    "\n",
    "A matrix $A \\in M_n$ is Hermitian if and only if there is a unitary $U \\in M_n$ and a real diagonal $\\Lambda \\in M_n$ such that $A=U \\Lambda U^*$, where $M_n$ is the set of $n$-by- $n$ complex matrices. Moreover, $A$ is real and Hermitian (that is, real symmetric) if and only if there is a real orthogonal $P \\in M_n$ and a real diagonal $\\Lambda \\in M_n$ such that $A=P \\Lambda P^T$.\n",
    "\n",
    "**Remark**\n",
    "\n",
    "- Observe the subtlety of the statement: If $A$ is symmetric as a complex matrix, then the conclusion is different. (See e.g., [Wikipedia - Complex symmetric matrices](https://en.wikipedia.org/wiki/Symmetric_matrix#Complex_symmetric_matrices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Uniqueness of pseudoinverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discussions on the uniqueness of pseudoinverse are borrowed from Kincaid and Cheney (2002) pp. 293--296, including remarks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Kincaid and Cheney (2002) p. 293)\n",
    "\n",
    "- The pseudoinverse has some (but not all) of the properties of an inverse. \n",
    "  - For example, we cannot expect $A^{+} A=I$ to be true if $n>m$, because the ranks of $A^{+}, A$, and $A^{+} A$ are at most $m$, whereas $I$ is $n \\times n$. \n",
    "  - However, equations such as $A A^{+} A=A$ are true for arbitrary $A$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Penrose Properties R. Penrose [1955] according to Kincaid and Cheney (2002) p. 293)\n",
    "\n",
    "Corresponding to any matrix $A$, there exists at most one matrix $X$ having these four properties:\n",
    "1. $A X A=A$\n",
    "2. $X A X=X$\n",
    "3. $(A X)^*=A X$\n",
    "4. $(X A)^*=X A$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: (Kincaid and Cheney (2002) p. 293)\n",
    "\n",
    "Let $X$ and $Y$ be two matrices having Properties $1-4$. Then by systematic use of these properties as indicated, we have\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X & =X A X       & \\text{(property 2)}\\\\\n",
    "& =X A Y A X       & \\text{(property 1)} \\\\\n",
    "& =X A Y A Y A Y A X       & \\text{(property 1)} \\\\\n",
    "& =(X A)^*(Y A)^* Y(A Y)^*(A X)^*       & \\text{(property 4, 3)} \\\\\n",
    "& =A^* X^* A^* Y^* Y Y^* A^* X^* A^* & \\\\\n",
    "& =(A X A)^* Y^* Y Y^*(A X A)^* & \\\\\n",
    "& =A^* Y^* Y Y^* A^*       & \\text{(property 1)}\\\\\n",
    "& =(Y A)^* Y(A Y)^*  & \\\\\n",
    "& =Y A Y A Y         & \\text{(property 4, 3)}\\\\\n",
    "& =Y A Y         & \\text{(property 2)}\\\\\n",
    "& =Y          & \\text{(property 2)}\\\\\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Uniqueness of pseudoinverse; Kincaid and Cheney (2002) p. 293)\n",
    "\n",
    "The pseudoinverse of a matrix has the four Penrose properties. Hence, each matrix has a unique pseudoinverse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: (Kincaid and Cheney (2002) p. 293)\n",
    "\n",
    "We address only property 1.\n",
    "\n",
    "Let $A$ be any matrix, and let its singular-value decomposition be\n",
    "$$\n",
    "A=P D Q\n",
    "$$\n",
    "\n",
    "Then\n",
    "$$\n",
    "A^{+}=Q^* D^{+} P^*\n",
    "$$\n",
    "\n",
    "If $A$ is $m \\times n$, then so is $D$, and $D$ has the form\n",
    "$$\n",
    "D_{i j}= \\begin{cases}\\sigma_i & \\text { if } i=j \\leq r \\\\ 0 & \\text { otherwise }\\end{cases}\n",
    "$$\n",
    "\n",
    "From this we can prove that\n",
    "$$\n",
    "D D^{+} D=D\n",
    "$$\n",
    "\n",
    "To do so, we write\n",
    "$$\n",
    "\\left(D D^{+} D\\right)_{i j}=\\sum_{\\nu=1}^n D_{i v} \\sum_{\\mu=1}^m D_{v \\mu}^{+} D_{\\mu j}\n",
    "$$\n",
    "\n",
    "The right-hand side will be 0 unless $i \\leq r$ and $j \\leq r$ because of the presence of the terms $D_{i \\nu}$ and $D_{\\mu j}$. Thus, we assume $i \\leq r$ and $j \\leq r$ and continue, simplifying the right-hand side to\n",
    "$$\n",
    "\\sum_{\\nu=1}^r D_{i \\nu} \\sum_{\\mu=1}^r D_{\\nu \\mu}^{+} D_{\\mu j}=\\sigma_i \\sum_{\\mu=1}^r D_{i \\mu}^{+} D_{\\mu j}=\\sigma_i \\sigma_i^{-1} D_{i j}=D_{i j}\n",
    "$$\n",
    "\n",
    "By similar reasoning, we prove that $D^{+}$has the remaining three Penrose properties relative to $D$. Then it is a simple matter to prove these four properties for $A^{+}$. For example, the first property is proved as follows:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A A^{+} A & =P D Q Q^* D^{+} P^* P D Q \\\\\n",
    "& =P D D^{+} D Q \\\\\n",
    "& =P D Q=A\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Visualization of SVD)\n",
    "\n",
    "- $x=\\left[\\begin{array}{cccc}-10 & -10 & 20 & 20 \\\\ -10 & 20 & 20 & -10\\end{array}\\right]$ \n",
    "- $A=\\left[\\begin{array}{cc}1 & 0.3 \\\\ 0.45 & 1.2\\end{array}\\right]$.\n",
    "- $A=USV^T = \\begin{bmatrix} -0.5819 & -0.8133 \\\\ -0.8133 & 0.5819 \\end{bmatrix} \\begin{bmatrix} 1.4907 & 0 \\\\ 0 & 0.7144 \\end{bmatrix} \\begin{bmatrix} -0.6359 & -0.7718 \\\\ -0.7718 & 0.6359 \\end{bmatrix}$.\n",
    "\n",
    "Example and figures: Alyssa Quek ([SVD visualization](https://alyssaq.github.io/2015/singular-value-decomposition-visualisation/))\n",
    "\n",
    "- This example is replaced by the picture of transformation of a circle to an ellipse.\n",
    "  - It is in favor of simplitiy and to focus only on the intuition.\n",
    "  - However, via this example, you can keep track of numerical values while transformation, hence kept in appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | |\n",
    "|---|---|\n",
    "| $$Ax$$ <br> ![Figure 1](https://alyssaq.github.io/blog/images/eigens-transformation_matrix.png) | | \n",
    "| $$V^Tx$$ <br> ![Figure 2](https://alyssaq.github.io/blog/images/svd_Vx.png) | $$SV^Tx$$ <br> ![Figure 3](https://alyssaq.github.io/blog/images/svd_SVx.png) | \n",
    "| $$USV^Tx$$ <br> ![Figure 4](https://alyssaq.github.io/blog/images/svd_USVx.png) | |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrch",
   "language": "python",
   "name": "ptrch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
