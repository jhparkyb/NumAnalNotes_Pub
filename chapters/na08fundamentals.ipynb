{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take-aways\n",
    "\n",
    "- Efficient ways to compute polynomials\n",
    "  - Horner's algorithm\n",
    "- Computational ways of thinking\n",
    "  - They are often different from mathematics.\n",
    "  - They are often non-obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Horner's algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Devising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 1** (Simplest way)\n",
    "\n",
    "To make the discussion concrete, fix a polynomial\n",
    "\n",
    "$$\n",
    "p(x) = 2 x^4+3 x^3-3 x^2+5 x-1\n",
    "$$\n",
    "\n",
    "Plug in $1/2$\n",
    "\n",
    "$$\n",
    "P\\left(\\frac{1}{2}\\right)=2 * \\frac{1}{2} * \\frac{1}{2} * \\frac{1}{2} * \\frac{1}{2}+3 * \\frac{1}{2} * \\frac{1}{2} * \\frac{1}{2}-3 * \\frac{1}{2} * \\frac{1}{2}+5 * \\frac{1}{2}-1=\\frac{5}{4}\n",
    "$$\n",
    "\n",
    "Complexity: count `*` and `+` (or `-`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 2** (Recycle previous powers)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{2} * \\frac{1}{2} & =\\left(\\frac{1}{2}\\right)^2 \\\\\n",
    "\\left(\\frac{1}{2}\\right)^2 * \\frac{1}{2} & =\\left(\\frac{1}{2}\\right)^3 \\\\\n",
    "\\left(\\frac{1}{2}\\right)^3 * \\frac{1}{2} & =\\left(\\frac{1}{2}\\right)^4\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Take the linear combination:\n",
    "\n",
    "$$\n",
    "P\\left(\\frac{1}{2}\\right)=2 *\\left(\\frac{1}{2}\\right)^4+3 *\\left(\\frac{1}{2}\\right)^3-3 *\\left(\\frac{1}{2}\\right)^2+5 * \\frac{1}{2}-1=\\frac{5}{4} .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 3** (Horner's algorithm - Nested multiplication)\n",
    "\n",
    "Given\n",
    "\n",
    "$$\n",
    "p(x) = 2 x^4+3 x^3-3 x^2+5 x-1\n",
    "$$\n",
    "\n",
    "rewrite \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(x) & =-1+x\\left(5-3 x+3 x^2+2 x^3\\right) \\\\\n",
    "& =-1+x\\left(5+x\\left(-3+3 x+2 x^2\\right)\\right) \\\\\n",
    "& =-1+x(5+x(-3+x(3+2 x))) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "P(\\frac{1}{2}) =-1+\\frac{1}{2} *(5+\\frac{1}{2} *(-3+\\frac{1}{2} *(3+\\frac{1}{2} * 2)))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  | multiplication | addition |\n",
    "|---|---|---|\n",
    "| Method 1 | 10 | 4 |\n",
    "| Method 2 | 7 | 4 |\n",
    "| Method 3 | 4 | 4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Efficiency of Horner)\n",
    "\n",
    "- Horner's algorithm does not transform the coefficients, but use those numbers as they are $\\longrightarrow$ less computations are purely a gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "Horner's algorithm for $p(x) = a_0+a_1 x+\\cdots+a_d x^d$ (degree $d$)\n",
    "\n",
    "- costs $d$ multiplications and $d$ additions\n",
    "- is based on rewriting: $a_0+x*\\left(a_1+x*\\left(a_2+\\cdots *\\left(a_{d-1}+x*\\left(a_d\\right)\\right)\\cdots\\right)\\right)$\n",
    "- A more general version uses base points $r_1, \\cdots, r_d$.\n",
    "  - $a_0+(x - r_1)*\\left(a_1 + (x - r_2)*\\left(a_2+\\cdots *\\left(a_{d-1}+(x - r_{d})*\\left(a_d\\right)\\right)\\cdots\\right)\\right)$\n",
    "  - This general version is useful in polynomial interpolation.\n",
    "  - $r_i=0$ ($i=1,2,\\cdots,d$) gives us the (plain) Horner form given above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Horner's algorithm)\n",
    "\n",
    "Write a code that evaluates polynomials at different points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def poly_eval(a, x, algorithm='Horner'):\n",
    "    \"\"\"\n",
    "    Evaluates a polynomial at a given point x.\n",
    "\n",
    "    Inputs:\n",
    "        a: 1D array of polynomial coefficients (ascending order). \n",
    "        x: 1D array of points at which to evaluate the polynomial.\n",
    "        algorithm: algorithm to use for polynomial evaluation. (default: 'Horner')\n",
    "    Output:\n",
    "        p: array of polynomial values at x.\n",
    "    \"\"\"\n",
    "    \n",
    "    if algorithm == 'Horner':\n",
    "        p = a[-1]*np.ones_like(x) # broadcasting is in effect \n",
    "        for i in range(len(a)-2, -1, -1):\n",
    "            p = x*p + a[i]\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4\n",
    "# a = np.ones(d, dtype=np.float64)\n",
    "a = np.random.rand(d)\n",
    "x = np.arange(10)\n",
    "p = poly_eval(a, x)\n",
    "\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot part is generated by Copilot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot of p at x\n",
    "plt.scatter(x, p, label='Horner algorithm')\n",
    "\n",
    "# Polynomial plot\n",
    "poly = np.poly1d(a[::-1])  # Create a polynomial object with coefficients in ascending order\n",
    "t = np.linspace(x.min(), x.max(), 100)\n",
    "plt.plot(t, poly(t), label='numpy package')\n",
    "\n",
    "# Set labels and legend\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- You might think the difference is not a big deal. What would you pursue if your numerical problem may end up evaluating polynomials millions or billions of times?\n",
    "  - This is indeed true when you solve nonlinear PDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating point representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take-aways\n",
    "\n",
    "- How computers carry out computations\n",
    "  - IEEE 754 Floating Point System\n",
    "- Loss of significance\n",
    "  - What the issue is and how we can avoid it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IEEE 754 Floating Point Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scientific representation of a real number](https://fastbitlab.com/wp-content/uploads/2022/07/Figure-2-7-1536x793.png)\n",
    "\n",
    "Figure: FastBitLab (Scientific notation of a real number)\n",
    "\n",
    "![IEEE 754 Single Precision](https://i0.wp.com/circuitcellar.com/wp-content/uploads/2023/07/0067-Floating-Point_Representation_Feature_Image.png?w=1123&ssl=1)\n",
    "\n",
    "Figure: Andrew Levido (IEEE 754 Single Precision)\n",
    "\n",
    "The two figures describe different numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- Real numbers are stored by a binary number system\n",
    "  - $\\pm 1 . b_1 b_2 \\ldots b_n \\times 2^e$\n",
    "  - IEEE Rounding to Nearest Rule applies when store real numbers. (See below)\n",
    "- Most common formats\n",
    "\n",
    "| precision | sign ($s$) | exponent ($e$)| mantissa or <br> fraction ($b$) | total bits |\n",
    "| :--- | :---: | :---: | :---: | :---: |\n",
    "| single | 1 | 8 | 23 | 32 |\n",
    "| double | 1 | 11 | 52 | 64 |\n",
    "| long double | 1 | 15 | 64 | 80 |\n",
    "\n",
    "Table source: Sauer (2017) p. 9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- From now on, we focus on double precision otherwise mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminology** \n",
    "\n",
    "- Radix point: $1010\\underbrace{.}_{\\text{this}}011_{(2)}$\n",
    "- Normalized (IEEE floating point number): The leading bit is 1.\n",
    "  - Subnormal: The leading bit is 0.\n",
    "- Left-justified: There is only one nonzero digit to the left of the radix point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Machine epsilon)\n",
    "\n",
    "The number *machine epsilon*, denoted $\\epsilon_{\\mathrm{mach}}$, is the distance between 1 and the smallest floating point number greater than 1. For the IEEE double precision floating point\n",
    "standard, $\\epsilon_{\\mathrm{mach}} = 2^{−52}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IEEE Rounding to Nearest Rule**\n",
    "\n",
    "For double precision, if the 53rd bit to the right of the binary point is 0, then round\n",
    "down (truncate after the 52nd bit). If the 53rd bit is 1, then round up (add 1 to the 52\n",
    "bit), unless all known bits to the right of the 1 are 0’s, in which case 1 is added to bit\n",
    "52 if and only if bit 52 is 1.\n",
    "\n",
    "Reference: Sauer (2017) p. 10.\n",
    "\n",
    "**Remark** \n",
    "\n",
    "- The last part starting with \"unless all known ...\" is to ensure that the rounding up and down have equal probabilities, hence no bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation**\n",
    "\n",
    "Denote the IEEE double precision floating point number associated to $x$, using the\n",
    "Rounding to Nearest Rule, by $\\mathrm{fl}(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathrm{fl}(x)$ is obtained as follows.\n",
    "\n",
    "1. Convert $x$ to binary number.\n",
    "2. Justify: Shift radix point to the right of the leftmost 1, and compensate with the exponent.\n",
    "2. Round: Apply a rounding rule, such as the IEEE Rounding to Nearest Rule, to reduce\n",
    "the mantissa to 52 bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Rounding errors)\n",
    "\n",
    "Let $x_c$ be a computed version of the exact quantity $x$. Then,\n",
    "$$\n",
    "\\text { (absolute) rounding error }=\\left|x_c-x\\right|,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\text { relative rounding error }=\\frac{\\left|x_c-x\\right|}{|x|},\n",
    "$$\n",
    "if the latter quantity exists. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Relative rounding error)\n",
    "\n",
    "In the IEEE machine arithmetic model, not necessarily double precision, the relative rounding error of $\\mathrm{fl}(x)$ is no more than one-half machine epsilon:\n",
    "\n",
    "$$\n",
    "\\frac{|\\mathrm{fl}(x)-x|}{|x|} \\leq \\frac{1}{2} \\epsilon_{\\text {mach }}\n",
    "$$\n",
    "\n",
    "Reference: Sauer (2017) p. 11\n",
    "\n",
    "See [Remark (Computations with small numbers)](#computations-with-small-numbers) for an example of this bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Rounding error](https://courses.engr.illinois.edu/cs357/sp2024/assets/img/figs/rounding_line.png)\n",
    "\n",
    "Figure: University of Illinois Urbana-Champaign CS 357"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let \n",
    "\n",
    "$$\n",
    "x = \\pm 1.b_1 b_2 b_3 ... b_n ... \\times 2^m ,\n",
    "$$\n",
    "\n",
    "where $n$ is the number of bits for mantissa and $m$ is a representable exponent.\n",
    "\n",
    "Then, \n",
    "\n",
    "$$\n",
    "\\mathrm{fl}(x) = \\begin{cases} \\pm 1.b_1 b_2 b_3 ... b_n \\times 2^m & (\\text{round down})\\\\\n",
    "            (\\pm 1.b_1 b_2 b_3 ... b_n + \\epsilon_{\\text {mach }}) \\times 2^m & (\\text{round up}),\\end{cases}\n",
    "$$\n",
    "\n",
    "where we note that $0.\\underbrace{000...0}_{n\\text{ bits}} b_{n+1}b_{n+2}\\cdots \\le \\epsilon_{\\text {mach }}/2$ when rounding down and similarly $\\epsilon_{\\text {mach }} - 0.\\underbrace{000...0}_{n\\text{ bits}} b_{n+1}b_{n+2}\\cdots \\le \\epsilon_{\\text {mach }}/2$ when rounding up (see the figure). Therefore, we have\n",
    "\n",
    "$$\\vert fl(x) - x \\vert \\le  \\frac{1}{2} \\epsilon_{\\text {mach }} \\times 2^m.$$\n",
    "\n",
    "Since $|x|=1.b_1 b_2 b_3 ... b_n ... \\times 2^m \\ge 2^m$, we have\n",
    "\n",
    "$$\n",
    "\\frac{ \\vert fl(x) - x \\vert }{ \\vert x \\vert } \\le \\frac{ \\epsilon_{\\text {mach }} \\times 2^m } { 2 \\vert x \\vert } \\le \\frac{1}{2} \\epsilon_{\\text {mach }}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Double precision**\n",
    "\n",
    "- $(-1)^{\\text {sign }}\\left(1 . b_{51} b_{50} \\ldots b_0\\right)_2 \\times 2^{e-1023}$\n",
    "\n",
    "![Double precision bits](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/IEEE_754_Double_Floating_Point_Format.svg/618px-IEEE_754_Double_Floating_Point_Format.svg.png)\n",
    "\n",
    "- Exponent ranges $-1022 \\le e - 1023 \\le 1023$.\n",
    "  - The subtracted 1023 is called *exponent bias*.\n",
    "  - This is a slightly shorter range than from $\\underbrace{00\\cdots0_{(2)}}_{11\\text{ bits}} - 1023$ through $\\underbrace{11\\cdots1_{(2)}}_{11\\text{ bits}} - 1023$.\n",
    "    - $\\underbrace{00\\cdots0_{(2)}}_{11\\text{ bits}}$ and $\\underbrace{11\\cdots1_{(2)}}_{11\\text{ bits}}$ are used in a special way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hexadecimal representation**\n",
    "\n",
    "- Binary nunmbers are often represented by hexadecimal numbers.\n",
    "- 4 bits $\\longleftrightarrow$ 1 hexadecimal digit\n",
    "\n",
    "|      Decimal | 0 | 1 |  2 |  3 |   4 |   5 |   6 |   7 |    8 |    9 |   10 |   11 |   12 |   13 |   14 |   15 |\n",
    "|-------------:|--:|--:|---:|---:|----:|----:|----:|----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|-----:|\n",
    "| 4-bit Binary | 0 | 1 | 10 | 11 | 100 | 101 | 110 | 111 | 1000 | 1001 | 1010 | 1011 | 1100 | 1101 | 1110 | 1111 |\n",
    "|  Hexadecimal | 0 | 1 |  2 |  3 |   4 |   5 |   6 |   7 |    8 |    9 |    A |    B |    C |    D |    E |    F |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** \n",
    "\n",
    "Find computer-friendly representation of 9.4. More specifically, find its IEEE double precision floating point representation in binary and hexadecimal format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Step 1: Convert to binary\n",
    "\n",
    "(For an algorithmic way, see Sauer (2017) p. 6.)\n",
    "\n",
    "- Integer part\n",
    "\n",
    "$$\n",
    "9 = 8 + 1 = 1000_{(2)} + 1_{(2)} = 1001_{(2)}\n",
    "$$\n",
    "\n",
    "- Fraction part \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "0.4 =\\frac{2}{5}&= \\underbrace{0\\cdot\\frac{1}{2} + 1\\cdot\\frac{1}{2^2} + 1\\cdot\\frac{1}{2^3} + 0\\cdot\\frac{1}{2^4}}_{3/8 = 0.0110_{(2)}} + (\\underbrace{\\frac{4}{10} - \\frac{3}{8}}_{1/40})\\\\\n",
    "&= \\underbrace{0.0110_{(2)} + \\frac{1}{16} \\cdot \\frac{2}{5}}_{2/5\\text{ (plug this in)}}\n",
    "\\\\\n",
    "&= 0.0110_{(2)} + 0.0001_{(2)}\\cdot\\left(\\underbrace{0.0110_{(2)} + \\frac{1}{16} \\cdot \\frac{2}{5}}_{2/5\\text{ (plugged in)}}\\right) \n",
    "\\\\\n",
    "&= 0.0110_{(2)} + 0.00000110_{(2)} + (0.0001_{(2)})^2\\cdot\\left( 0.0110_{(2)} + \\frac{1}{16} \\cdot \\frac{2}{5}\\right) %0.0110_{(2)} + \\underbrace{\\frac{1}{40}}_{(1/16)\\cdot (2/5)})\n",
    "\\\\\n",
    "&\\quad\\vdots\n",
    "\\\\\n",
    "&=0.\\overline{0110}_{(2)}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\n",
    "9.4 = 1001.\\overline{0110}_{(2)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Justify\n",
    "\n",
    "$$\n",
    "1001.\\overline{0110}_{(2)} = 1.001\\overline{0110}_{(2)}\\times 2^3 %= 1.001\\overline{0110}_{(2)}\\times 2^{11_{(2)}}\n",
    "$$\n",
    "\n",
    "- Sign: $+$  $\\longrightarrow$ $s=0$\n",
    "- Exponent\n",
    "  - $3=1026-1023=(1024+2) - \\underbrace{1023}_{\\text{bias}}$ $\\longrightarrow$ $e=2^{10} + 2=100,0000,0010_{(2)}$\n",
    "- Sign + Exponent\n",
    "  - $0100,0000,0010_{(2)}=402_{(16)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Round to obtain mantissa\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "&\\quad 1.001\\overline{0110}_{(2)} \n",
    "\\\\\n",
    "&= 1.\n",
    "\\begin{array}{|l|l|l|l|l|l|l|l|l|l|l|l|l|}\n",
    "\\hline 0010 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 \\\\\n",
    "\\hline\n",
    "\\end{array} 110\\cdots\n",
    "\\\\\n",
    "&\\approx 1.\n",
    "\\begin{array}{|l|l|l|l|l|l|l|l|l|l|l|l|l|}\n",
    "\\hline 0010 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1101 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Convert the mantissa to hexadecimal (excluding the leading 1)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\begin{array}{|l|l|l|l|l|l|l|l|l|l|l|l|l|}\n",
    "\\hline 0010 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1101 \\\\\n",
    "\\hline\n",
    "\\end{array}\\\\\n",
    "&\\rightarrow(2 C C C C C C C C C C C D)_{16} .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Altogether (sign bit) + (exponent bits) + (mantissa bits) reads\n",
    "\n",
    "- (Hexadecimal) $4022 C C C C C C C C C C C D$ \n",
    "- (Binary) $\\begin{array}{|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|}\n",
    "\\hline 0100 & 0000 & 0010 & 0010 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1100 & 1101 \\\\\n",
    "\\hline\n",
    "\\end{array}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** \n",
    "\n",
    "Use NumPy to check the IEEE 754 doulbe precision representation of 9.4.\n",
    "\n",
    "- `ndarray.view(dtype)`: Shows reinterpretation of the array in the format of `dtype`. ([Documentation](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.view.html))\n",
    "  - E.g., if `x = np.array(9.4)`, then `x.view(np.int64)` reinterpret the chunk of bit for 9.4, a 64-digit binary number, as an integer. In words, it says to NumPy \"View `x` as integer.\"\n",
    "- `numpy.binary_repr`: Return the binary representation of the input number as a string. ([Documentation](https://numpy.org/doc/stable/reference/generated/numpy.binary_repr.html))\n",
    "  - If `width=64` is passed, the output consists of all 64 bits, including the first consecutive 0's.\n",
    "- `numpy.base_repr(x.view(np.int64), base=16)`: Return the hexadecimal representation of the input, in this case, `x.view(np.int64)`. ([Documentation](https://numpy.org/doc/stable/reference/generated/numpy.base_repr.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array(9.4)\n",
    "print(f\"{'Usual print of '              :>27}{x}{':'            :>13}{x}\")\n",
    "print(f\"{'Float interpretation of '     :>27}{x}{':'            :>13}{x.view(np.float64)}\")\n",
    "print(f\"{'Integer interpretation of '   :>27}{x}{' (base  2) :' :>13}{np.binary_repr(x.view(np.int64), width=64)}\")\n",
    "print(f\"{'Integer interpretation of '   :>27}{x}{' (base 10) :' :>13}{x.view(np.int64)}\")\n",
    "print(f\"{'Integer interpretation of '   :>27}{x}{' (base 16) :' :>13}{np.base_repr(x.view(np.int64), base=16)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Special exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $e=2047=111,1111,1111_{(2)}$ is used to express abnormal quantities.\n",
    "  - $s=0$ $\\longrightarrow$ $se=0111,1111,1111_{(2)}=7FF_{(16)}$\n",
    "  - $s=1$ $\\longrightarrow$ $se=1111,1111,1111_{(2)}=FFF_{(16)}$\n",
    "\n",
    "| machine number | example | hex format |\n",
    "| :---: | ---: | :---: |\n",
    "| + Inf | $1 / 0$ | 7FF0000000000000 |\n",
    "| - Inf | $-1 / 0$ | FFF0000000000000 |\n",
    "| NaN | $0 / 0$ | FFFxxxxxxxxxxxxx |\n",
    "\n",
    "Here, x means any digit not all of which are zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- $e=0=000,0000,0000_{(2)}$ is used to represent very small numbers.\n",
    "  - In this case **only**, the omitted bit to the right of radix point is assumed to be 0, not 1. \n",
    "    - Consequently, the floating point representation means $\\pm 0 . b_1 b_2 \\ldots b_{52} \\times 2^{-1022}$. (Sauer (2017) p. 13)\n",
    "    - Those numbers are called *subnormal* floating point numbers.\n",
    "  - Subnormal numbers include $0$.\n",
    "    - In fact, many computing system uses two zeros: `0` and `-0` depending on the sign bit.\n",
    "  - The exponent is the same, -1022, in the case $e=1=000,000,0001_{(2)}$ after subtracting the exponent bias $1023$. However, the actual effect is as if $(\\text{significand})\\times 2^{-1023}$ because the leading digit is 0, not 1. (Instructor's comment)\n",
    "  - The smallest possible positive number of double precision is $2^{-52}\\times 2^{-1022}=2^{-1074}$, which is represented with 0 exponent\n",
    "  \n",
    "$$\n",
    "\\begin{array}{|l|l|l|l|}\n",
    "\\hline 0 & 00000000000 & 0000000000000000000000000000000000000000000000000001 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# smallest positive number: \n",
    "#   zero exponent, only one 1 in the last bit of the mantissa\n",
    "x = np.array(1, dtype=np.int64)\n",
    "y = x\n",
    "print(f\"{'Ex0   (bit): ':>13}{np.binary_repr(y, width=64)}\")\n",
    "print(f\"{'(float): '    :>13}{y.view(np.float64)}\")\n",
    "\n",
    "# flip the sign bit\n",
    "# 1 is at the first (sign) and the last bits (mantissa)\n",
    "y = x + np.left_shift(x, 63)\n",
    "print(f\"{'Ex1   (bit): ':>13}{np.binary_repr(y, width=64)}\")\n",
    "print(f\"{'(float): '    :>13}{y.view(np.float64)}\")\n",
    "\n",
    "# 0.1*2^(-1022) = 1.0*2^(-1023)\n",
    "# 1 is at the leftmost bit of the mantissa (0 exponent; subnormal)\n",
    "y = np.left_shift(x, 51)\n",
    "print(f\"{'Ex2   (bit): ':>13}{np.binary_repr(y, width=64)}\")\n",
    "print(f\"{'(float): '    :>13}{y.view(np.float64)}\")\n",
    "\n",
    "# 1.0*2^(-1022)\n",
    "# 1 is at the rightmost bit of the exponent (1 exponent; normalized)\n",
    "y = np.left_shift(x, 52)\n",
    "print(f\"{'Ex3   (bit): ':>13}{np.binary_repr(y, width=64)}\")\n",
    "print(f\"{'(float): '    :>13}{y.view(np.float64)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Machine epsilon and smallest numbers)\n",
    "\n",
    "- $\\epsilon_{\\text {mach }}=2^{-52}$ is the smallest distinguishable size between numbers in the order of unit, 1.\n",
    "- The smallest-representable number, $2^{-1074}$ is the smallest quantity that can ever be representable: smaller values than that are treated as 0.\n",
    "- $\\epsilon_{\\text {mach }}=2^{-52}$ and $2^{-1074}$ are different.\n",
    "  - $\\epsilon_{\\text {mach }}$ originates from the mantissa while $2^{-1074}$ is determined by the mantissa and the exponent.\n",
    "  - There are many numbers that are smaller than $\\epsilon_{\\text {mach }}$ and represented by IEEE 754 double precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Overflow and underflow)\n",
    "\n",
    "- Overflow refers to when the result is too large to be stored. \n",
    "  - They are usually stored as `Inf`, `-Inf`, or `NaN`.\n",
    "- Underflow refers to when the result is too small te be represented.\n",
    "  - They are usually stored as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addtion of floating point numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm** (Machine addition)\n",
    "\n",
    "- Given two numbers\n",
    "- Line up the decimal points\n",
    "- Add the two numbers\n",
    "- Store the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Machine addition)\n",
    "\n",
    "- Actual addition can be conducted in higher precision than 52 bits: it takes place in a dedicated register. (Sauer (2017) p. 14)\n",
    "- But the result is rounded to 52 bits of mantissa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "$1 + 2^{-53}=1$ in double precision.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& =1.\\begin{array}{|l|lr|}\n",
    "\\hline 0000000000000000000000000000000000000000000000000000 & \\  &\\times 2^0 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\\\\n",
    "& + 0.\\begin{array}{|l|ll|}\n",
    "\\hline\n",
    "0000000000000000000000000000000000000000000000000000 & 1 & \\times 2^0 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\\\\n",
    "& =1.\\begin{array}{|l|ll|}\n",
    "\\hline\n",
    "0000000000000000000000000000000000000000000000000000 & 1 & \\times 2^0 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1.0\n",
    "y = 2**(-53)\n",
    "z = 2**(-52)\n",
    "\n",
    "print(f\"{'x: ' :>5}{x}\")\n",
    "print(f\"{'y: ' :>5}{y}\")\n",
    "print(f\"{'z: ' :>5}{z}\")\n",
    "print(f\"{'x+y: ' :>5}{x+y}\")\n",
    "print(f\"{'x+z: ' :>5}{x+z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Computation errors)\n",
    "\n",
    "- Due to rounding and truncations, computer arithmetic sometimes gives surprising results. For example, if a double precision computer with IEEE rounding to nearest is asked to store 9.4, then subtract 9, and then subtract 0.4, the result will be something other than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 9.4\n",
    "y = x - 9.4\n",
    "z = x - 9.0\n",
    "z = z - 0.4\n",
    "\n",
    "print(f\"{'x: ' :>5}{x}\")\n",
    "print(f\"{'y: ' :>5}{y}\")\n",
    "print(f\"{'z: ' :>5}{z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Computations with small numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Computations with small numbers)\n",
    "\n",
    "- The fact that $\\epsilon_{\\text {mach }}=2^{-52}$ does not mean that numbers smaller than $\\epsilon_{\\text {mach }}$  are negligible in the IEEE model. As long as they are representable in the model, computations with numbers of this size are just as accurate, assuming that they are not added or subtracted to numbers of unit size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 1e-20 # << 2^(-52)=2.220446049250313e-16\n",
    "x = 9.4 * r\n",
    "y = x - (9.4 * r)\n",
    "z = x - (9.0 * r)\n",
    "z = z - (0.4 * r)\n",
    "\n",
    "print(f\"{'x: ' :>5}{x}\")\n",
    "print(f\"{'y: ' :>5}{y}\")\n",
    "# The computing error is not of order of machine epsilon,\n",
    "# but it is compatible with (x * e_mach)\n",
    "print(f\"{'z: ' :>5}{z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss of significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- Subtraction of nearly equal numbers reduce the significant digits.\n",
    "- In many cases, we can avoid this by using different calculuation in the mathematical problem stage.\n",
    "  - The programmer must be aware of this issue and pay attention to that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Tack down significant digits when subtracting two nearly equal numbers $x= 123.4567$ and $y= 123.4566$.\n",
    "\n",
    "0. Let us examine decimal digits, expecting that something similar happens in binary numbers on computers.\n",
    "\n",
    "1. Subtract them.\n",
    "\n",
    "$$\n",
    "\\begin{array}{r}\n",
    "x &= 123.4567 \\\\\n",
    "y &= 123.4566 \\\\\n",
    "\\hline \n",
    "x-y &= 000.0001\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "2. Store the result in left-justified form.\n",
    "\n",
    "$$\n",
    "x-y = 1.0\\times10^{-4}\n",
    "$$\n",
    "\n",
    "3. Compare significant digits.\n",
    "\n",
    "| number | No. significant digits |\n",
    "|---|---|\n",
    "| $x= 1.234567\\times 10^2$ | 7 |\n",
    "| $y= 1.234566\\times 10^2$ | 7 |\n",
    "| $x-y=1.0\\times10^{-4}$ | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Rewriting to avoid loss of significand)\n",
    "\n",
    "Suppose that your project involves the following computation near $x = 0$ for a fixed value $a > 0$: \n",
    "\n",
    "$$\n",
    "\\sqrt{x^2 + a^2} - a .\n",
    "$$\n",
    "\n",
    "Rewrite this expression to avoid loss of significant digits along the computation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "The quantity can be rewritten as \n",
    "\n",
    "$$\n",
    "\\left(\\sqrt{x^2+a^2}-a\\right)\\left(\\frac{\\sqrt{x^2+a^2}+a}{\\sqrt{x^2+a^2}+a}\\right)=\\frac{x^2}{\\sqrt{x^2+a^2}+a}\n",
    "$$\n",
    "\n",
    "We can use the last term in place of what's given in the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Sauer (2017) p. 18 shows the following dramatic example of this issue. Suppose we want to compute $E_1$, which is equivalent to $E_2$.\n",
    "\n",
    "$$\n",
    "E_1=\\frac{1-\\cos x}{\\sin ^2 x} \\quad \\text { and } \\quad E_2=\\frac{1}{1+\\cos x}.\n",
    "$$\n",
    "\n",
    "1. Implement a series of evaluations of $E_1$ and $E_2$ at $x=1/10^j$ with $j=0, 1, 2, \\cdots, 14$. \n",
    "2. Which one is more credible? Why do you think so?\n",
    "3. Explain the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               x        E1        E2\n",
      "0   1.000000e+00  0.649223  0.649223\n",
      "1   1.000000e-01  0.501252  0.501252\n",
      "2   1.000000e-02  0.500013  0.500013\n",
      "3   1.000000e-03  0.500000  0.500000\n",
      "4   1.000000e-04  0.500000  0.500000\n",
      "5   1.000000e-05  0.500000  0.500000\n",
      "6   1.000000e-06  0.500044  0.500000\n",
      "7   1.000000e-07  0.499600  0.500000\n",
      "8   1.000000e-08  0.000000  0.500000\n",
      "9   1.000000e-09  0.000000  0.500000\n",
      "10  1.000000e-10  0.000000  0.500000\n",
      "11  1.000000e-11  0.000000  0.500000\n",
      "12  1.000000e-12  0.000000  0.500000\n",
      "13  1.000000e-13  0.000000  0.500000\n",
      "14  1.000000e-14  0.000000  0.500000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "j = np.arange(15)\n",
    "x = 10.**(-j)\n",
    "E1 = lambda x: (1- np.cos(x))/(np.sin(x)*np.sin(x))\n",
    "E2 = lambda x: 1/(1+np.cos(x))\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'E1': E1(x), 'E2': E2(x)})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Aside: Quadratic formula for machine computations)\n",
    "\n",
    "- Sauer (2017) pp. 19--20 discusses quadratic formula that can result in a better accuracy than what we usually use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Higher internal precision)\n",
    "\n",
    "- Computers that implement floating-point arithmetic according to the current *official* standard use 80 Ьits for intemal calculations. [Kincaid, Cheney (2002) Numerical Analysis p. 44]\n",
    "  - There are many additional concepts-guard blt, rounding blt, sticky blt, denonnalized numbers, unnor­malized numbers, douЫe rounding, and others-that enter into any detailed dis­cussion of this subject. \n",
    "  - Further references are found on the same page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Stability and Conditioning)\n",
    "\n",
    "- Speaking informally, we say that а numerical process is *unstable* if small errors made at one stage of the process are magnified in subsequent stages and seriously degrade the accuracy of the overall calculation. [Kincaid, Cheney (2002) p. 64]\n",
    "- The word *condition* or *conditioning* are used informally to indicate how sensitive the solution of а proЫem may Ье to small relative changes in the input data. [Kincaid, Cheney (2002) p. 66]\n",
    "  - For *certain* problems, condition numbers *can* be defined. [Kincaid, Cheney (2002) p. 66]\n",
    "  - One such example is the condition number $\\kappa(A)=\\| A\\| \\|A^{-1}\\|$ of a square matrix $A$ for a linear system $Ax = b$. \n",
    "- We speak of conditioning about problems while stability about numerical methods.\n",
    "  - There is no way around ill-conditioned problem, but we may be able to choose more stable methods.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
