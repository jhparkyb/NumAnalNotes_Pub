{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we care?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- QR factorization gives a better way to find least square solution.\n",
    "- It is used to compute eigenvalues. (a future topic)\n",
    "- It gives a direct method to solve system of linear equations. (But not as efficient as LU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gram-Schmidt orthogonalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem of interes**\n",
    "\n",
    "Given linearly independent vectors $A_1, \\cdots, A_n \\in\\mathbb{R}^m$ ($n\\le m$) find orthonormal vectors $q_1, \\cdots, q_n \\in\\mathbb{R}^m$ that span the same space as $A_1, \\cdots, A_n$ do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation and settings\n",
    "\n",
    "- Switch back and forth between column and row vector depending on convenience of writing/typing. Ask anytime when there is any confusion due to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What we can do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea**: Recursively subtract out perpendicular components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** \n",
    "\n",
    "Find orthonormal vectors that span the same space as \n",
    "$$\n",
    "A_1 = (1,-1,1)^T,\n",
    "A_2 = (1,0,1)^T,\n",
    "A_3 = (1,1,2)^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TBA**: CalcPlot3D desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "y_1 &= A_1=(1,-1,1)\n",
    "\\\\\n",
    "q_1 &= \\frac{y_1}{\\left\\|y_1\\right\\|_2} \n",
    "= \\frac{1}{\\sqrt{3}} \\left(1, -1, 1\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "y_2 &= A_2-q_1\\left(q_1^T A_2\\right)  =(1,0,1)-\\frac{2}{3}(1,-1,1) =\\left(\\frac{1}{3}, \\frac{2}{3}, \\frac{1}{3}\\right)\n",
    "\\\\\n",
    "q_2 &= \\frac{y_2}{\\left\\|y_2\\right\\|_2} = \\frac{1}{\\sqrt{6}} (1, 2, 1)\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "y_3 &= A_3-q_1\\left(q_1^T A_3\\right) -q_2\\left(q_2^T A_3\\right) \n",
    "    \\\\\n",
    "& =(1,1,2)-\\frac{2}{3}(1,-1,1)-\\frac{5}{6}\\left(1, 2, 1\\right) \n",
    "    \\\\\n",
    "& =\\left(\\frac{-1}{2}, 0, \\frac{1}{2}\\right)\n",
    "    \\\\\n",
    "q_3 &= \\frac{y_3}{\\left\\|y_3\\right\\|_2} = \\frac{1}{\\sqrt{2}} \\left(-1, 0, 1\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orthogonal system\n",
    "\n",
    "$$\n",
    "\\{y_1, y_2, y_3\\}=\\left\\{ (1,-1,1),\n",
    "\\left(\\frac{1}{3}, \\frac{2}{3}, \\frac{1}{3}\\right),\n",
    "\\left(\\frac{-1}{2}, 0, \\frac{1}{2}\\right) \\right\\}\n",
    "$$\n",
    "\n",
    "Orthonormal system\n",
    "\n",
    "$$\n",
    "\\{q_1, q_2, q_3\\}=\\left\\{\\left(\\frac{\\sqrt{3}}{3}, \\frac{-\\sqrt{3}}{3}, \\frac{\\sqrt{3}}{3}\\right),\n",
    "\\left(\\frac{\\sqrt{6}}{6}, \\frac{\\sqrt{6}}{3}, \\frac{\\sqrt{6}}{6}\\right),\n",
    "\\left(\\frac{-\\sqrt{2}}{2}, 0, \\frac{\\sqrt{2}}{2}\\right) \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General case**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_1=A_1\n",
    "\\quad \\text { and } \\quad \n",
    "q_1 = \\frac{y_1}{\\left\\|y_1\\right\\|_2} \n",
    "$$\n",
    "\n",
    "$$\n",
    "y_2=A_2-q_1\\left(q_1^T A_2\\right)  \n",
    "\\quad \\text { and } \\quad \n",
    "q_2=\\frac{y_2}{\\left\\|y_2\\right\\|_2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_j=A_j-q_1\\left(q_1^T A_j\\right)-q_2\\left(q_2^T A_j\\right)-\\ldots-q_{j-1}\\left(q_{j-1}^T A_j\\right) \\quad \\text { and } \\quad q_j=\\frac{y_j}{\\left\\|y_j\\right\\|_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduced QR factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea**: Reorganize Gram-Schmidt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notation** \n",
    "\n",
    "- $\\hat Q$, $\\hat R$: matrices of reduced QR factorization\n",
    "- $Q$, $R$: matrices of full QR factorization (see the next section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Notice Gram-Schmidt can be **rearranged for $A_j$'s**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "A_1 &= r_{11} q_1 \\\\\n",
    "A_2 &= r_{12} q_1+r_{22} q_2\\\\\n",
    "&\\vdots \\\\\n",
    "A_j &= r_{1 j} q_1+\\cdots+r_{j-1, j} q_{j-1}+r_{j j} q_j\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Recall **linear combinations** of same set of vectors can be expressed as a **matrix multiplication**.\n",
    "\n",
    "$$\n",
    "\\underbrace{\\left(A_1|\\cdots| A_n\\right)}_{A}\n",
    "=\n",
    "\\underbrace{\\left(q_1|\\cdots| q_n\\right)}_{\\hat Q}\n",
    "\\underbrace{\n",
    "\\left[\\begin{array}{cccc}\n",
    "r_{11} & r_{12} & \\cdots & r_{1 n} \\\\\n",
    "& r_{22} & \\cdots & r_{2 n} \\\\\n",
    "& & \\ddots & \\vdots \\\\\n",
    "& & & r_{n n}\n",
    "\\end{array}\\right]\n",
    "}_{\\hat R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $A$: starting point\n",
    "  - Size: $m\\times n$ $(m\\ge n)$\n",
    "- $\\hat Q$: main product\n",
    "  - Size: $m\\times n$ $(m\\ge n)$ (same as $A$)\n",
    "  - Each column is of unit length (in 2-norm).\n",
    "  - Each pair of columns are orthogonal.\n",
    "- $\\hat R$: by-product\n",
    "  - Size: $n\\times n$\n",
    "  - Shape: upper triangular\n",
    "  - non-zero diagonals if $A_j$'s are linearly independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QR factorization algorithm using classical Gram-Schmidt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Given $A_j$ ($j=1,\\cdots,n$) that are linearly independent\n",
    "\n",
    "- **for** $j=1,2, \\ldots, n$\n",
    "  - $y=A_j$\n",
    "  - **for** $i=1,2, \\ldots, j-1$\n",
    "    - $r_{i j}=q_i^T A_j$\n",
    "    - $y=y-r_{i j} q_i$\n",
    "  - $r_{j j}=\\|y\\|_2$\n",
    "  - $q_j=y / r_{j j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QR factorization algorithm using modified Gram-Schmidt\n",
    "\n",
    "Given $A_j, j=1, \\ldots, n$ be linearly independent vectors.\n",
    "\n",
    "- **for** $j=1,2, \\ldots, n$\n",
    "  - $y=A_j$\n",
    "  - **for** $i=1,2, \\ldots, j-1$\n",
    "    - $r_{i j}=q_i^T y$\n",
    "    - $y=y-r_{i j} q_i$\n",
    "  - $r_{j j}=\\|y\\|_2$\n",
    "  - $q_j=y / r_{j j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- Difference from the original Gram-Schmidt\n",
    "  - $r_{i j}=q_i^T A_j$ $\\gets$ $r_{i j}=q_i^T y$\n",
    "  - That is, when $q_i$ component is substracted from $A_j$, we dot-product $q_i$ with $A_j - r_{1j}q_1 - r_{2j}q_2 - \\cdots - r_{i-1,j}q_{i-1}$ (most recent, trimmed vector) instead of with the whole vector $A_j$.\n",
    "  - They are mathematically equivalent (due to orthogonality of $q_j$'s), but the modified one is known to show more accuracy. (Sauer (2017) p. 227)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Gram-Schmidt via manual QR)\n",
    "\n",
    "Find the **reduced** QR factorization of the following matrix on computers. Use the result to give the answers to a mathematical question: Give an orthogonal system (not necessarily unit length) of vectors that spans the same as $\\mathrm{span}\\{(1, 2, 2), (-4, 3, 2)\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def qr_red_GS(A):\n",
    "    \"\"\"\n",
    "    Retrun reduced QR factroization of a matrix using (modified) Gram-Schmidt orthogonalization.\n",
    "    \n",
    "    Input:\n",
    "        A (array): matrix to be factored\n",
    "    Output:\n",
    "        Q (array): orthogonal matrix\n",
    "        R (array): upper triangular matrix\n",
    "    \"\"\"\n",
    "    (m, n) = A.shape\n",
    "    if m < n:\n",
    "        raise ValueError(\"The number of rows must be greater than or equal to the number of columns\")\n",
    "\n",
    "    Q = np.zeros((m, n))\n",
    "    R = np.zeros((n, n))\n",
    "\n",
    "    for j in range(n):\n",
    "        y = A[:, j].copy()\n",
    "\n",
    "        # See below why `range(j)` not `range(j-1)`\n",
    "        for i in range(j):\n",
    "            R[i, j] = np.dot(Q[:, i], y)\n",
    "            y = y - R[i, j] * Q[:, i]\n",
    "        R[j, j] = np.linalg.norm(y)\n",
    "        Q[:, j] = y / R[j, j]\n",
    "        \n",
    "    return Q, R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Toggle comment to test different matrices\"\"\"\n",
    "A = np.array([[1,2,2],[-4,3,2]], dtype=np.float64)\n",
    "# A = np.array([[1,2,2, -1],[-4,3,2, 3], [1, 1, 1, 1]], dtype=np.float64)\n",
    "# A = np.array([[1,2,2, -1, 4],[-4,3,2, 3, -2], [1, 1, -1, 1,0]], dtype=np.float64)\n",
    "\n",
    "A = A.T\n",
    "\n",
    "Q, R = qr_red_GS(A)\n",
    "\n",
    "print(\"A = QR? --->\", np.allclose(A, Q @ R))\n",
    "print(\"A\\n\", A)\n",
    "print(\"Q\\n\", Q)\n",
    "print(\"R\\n\", R)\n",
    "print(\"QR\\n\", Q @ R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inner loop must be `for i in range(j)` instead of `for i in range(j-1)`.\n",
    "\n",
    "Since j starts from 0, not 1, we shouldn't use j-1 (this leads to j-2 in effect)\n",
    "e.g., if j = 2 (3rd iteration) --> for i in range(j) <==> i in [0, 1] \n",
    "  (two inner iterations; correct b/c we are subtracting q1, q2 components) \n",
    "When `for i in range(j-1)` is used instead of `for i in range(j)`, \n",
    "it is missing one last iteration, and gives wrong result.\n",
    "In particular, Q is not orthogonal anymore, i.e., Q^T Q != I.\n",
    "\"\"\"\n",
    "print(\"Q^T Q\\n\", Q.T @ Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computational issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Modified Gram-Schmidt algorithm is more stable than the classical version.\n",
    "\n",
    "Computation: HW problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full QR factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea**: Append extra columns $q_j$'s until $\\hat Q$ becomes a square matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- We will see appending $q_j$'s is conducted by appending $A_j$'s behind the scenes, and discarding them.\n",
    "  - Why? There is no simple way to choose the right mutually orthonormal vectors while we can continue if more $A_j$'s are available.\n",
    "- We will see those extra $A_j$'s don't change things in an essential way.\n",
    "  - There is no simple way to choose linearly independent $A_j$'s either. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left(A_1|\\cdots| A_n\\right)=\\left(q_1|\\cdots| q_n | \\underbrace{q_{n+1}|\\cdots| q_m}_{\\text{extra}}\\right)\n",
    "\\left[\\begin{array}{cccc}\n",
    "r_{11} & r_{12} & \\cdots & r_{1 n} \\\\\n",
    "& r_{22} & \\cdots & r_{2 n} \\\\\n",
    "& & \\ddots & \\vdots \\\\\n",
    "0 & \\cdots & \\cdots & 0 \\\\\n",
    "\\vdots & & & \\vdots \\\\\n",
    "0 & \\cdots & \\cdots & 0\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In block form, this reads\n",
    "\n",
    "$$\n",
    "A = \\underbrace{\\left( \\hat Q | Q_{[k]} \\right)}_{Q}\n",
    "\\underbrace{\n",
    "    \\begin{bmatrix}\n",
    "    \\hat R \\\\\n",
    "    O_{[k]}\n",
    "    \\end{bmatrix}\n",
    "}_{R}\n",
    "=QR = \\hat Q \\hat R\n",
    "$$\n",
    "\n",
    "Here,\n",
    "- $Q_{[k]}=\\left(q_{n+1}|\\cdots| q_m\\right)$ is $m$-by-$k$ ($k=m-n$) matrix of $k$ orthonormal columns \n",
    "- $O_{[k]}$ is $k$-by-$n$ zero matrix\n",
    "- The last equality can be verified via block multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Gram-Schmidt via manual QR revisited)\n",
    "\n",
    "Find the **full** QR factorization of the following matrix, whose columns are $(1, 2, 2), (-4, 3, 2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pad $A$ with columns of identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A full QR factorixation that recycles reduced QR factorization\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def pad_matrix_eye(A):\n",
    "    \"\"\"\n",
    "    Pad the input matrix with columns of identity matrix.\n",
    "    input:\n",
    "        A (array): matrix to be padded\n",
    "    \"\"\"\n",
    "    (m, n) = A.shape\n",
    "    if m < n:\n",
    "        TRANSPOSE = True\n",
    "        A = A.T\n",
    "        (m, n) = A.shape\n",
    "    else:\n",
    "        TRANSPOSE = False\n",
    "\n",
    "    A_ = np.eye(A.shape[0])\n",
    "    A_[:, :n] = A\n",
    "\n",
    "    if TRANSPOSE == True:\n",
    "        A_ = A_.T\n",
    "\n",
    "    return A_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  2. -1.]\n",
      " [-4.  3.  2.  3.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# pad_matrix_eye sanity check\n",
    "\n",
    "A = np.array([[1,2,2, -1],\n",
    "              [-4,3,2, 3],\n",
    "              #[1, 1, 1, 1]\n",
    "              ], dtype=np.float64)\n",
    "# A = A.T\n",
    "\n",
    "print(pad_matrix_eye(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Full QR factorization\n",
    "\n",
    "- Recycle reduced QR factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def qr_GS(A):\n",
    "    \"\"\"\n",
    "    Return full QR factroization of a matrix using \n",
    "    Gram-Schmidt orthogonalization.\n",
    "\n",
    "    Input:\n",
    "        A (array): matrix to be factored (row-major assumed)\n",
    "    Output:\n",
    "        Q (array): orthogonal matrix\n",
    "        R (array): upper triangular matrix\n",
    "    Note:\n",
    "        Input matrix is padded with columns of identity matrix.\n",
    "    \"\"\"\n",
    "    (m, n) = A.shape\n",
    "    if m < n:\n",
    "        raise ValueError(\"The number of rows must be greater than or equal to the number of columns\")\n",
    "    \n",
    "    A_ = pad_matrix_eye(A)\n",
    "    R = np.zeros((m, n))\n",
    "\n",
    "    Q, R_ = qr_red_GS(A_)\n",
    "\n",
    "    # Construct matrix R: \n",
    "    #   R_ is computed to be m x m, but R take only n x n part (m >= n)\n",
    "    R[:n, :] = R_[:n, :n]\n",
    "\n",
    "    return Q, R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = QR? ---> True\n",
      "Q^T Q = I? ---> True\n",
      "A\n",
      " [[ 1. -4.  1.]\n",
      " [ 2.  3.  1.]\n",
      " [ 2.  2.  1.]\n",
      " [-1.  3.  1.]]\n",
      "Q\n",
      " [[ 0.31622777 -0.70596229  0.6249268  -0.10527936]\n",
      " [ 0.63245553  0.39402546  0.01275361 -0.66676929]\n",
      " [ 0.63245553  0.22984819  0.06376804  0.73695553]\n",
      " [-0.31622777  0.54178501  0.7779701   0.03509312]]\n",
      "R\n",
      " [[3.16227766 0.9486833  1.26491106]\n",
      " [0.         6.09097693 0.45969637]\n",
      " [0.         0.         1.47941855]\n",
      " [0.         0.         0.        ]]\n",
      "QR\n",
      " [[ 1. -4.  1.]\n",
      " [ 2.  3.  1.]\n",
      " [ 2.  2.  1.]\n",
      " [-1.  3.  1.]]\n",
      "Q^T Q\n",
      " [[ 1.00000000e+00  2.77555756e-17  1.11022302e-16 -2.64718802e-15]\n",
      " [ 2.77555756e-17  1.00000000e+00 -5.55111512e-17  1.95329863e-15]\n",
      " [ 1.11022302e-16 -5.55111512e-17  1.00000000e+00  2.56045185e-15]\n",
      " [-2.64718802e-15  1.95329863e-15  2.56045185e-15  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# A = np.array([[1,2,2],[-4,3,2]], dtype=np.float64)\n",
    "A = np.array([[1,2,2, -1],[-4,3,2, 3], [1, 1, 1, 1]], dtype=np.float64)\n",
    "# A = np.array([[1,2,2, -1, 4],[-4,3,2, 3, -2], [1, 1, -1, 1,0]], dtype=np.float64)\n",
    "# d = 1e-10\n",
    "# A = np.array([[1,d,0,0],[-4,3,2, 3], [1, 1, 1, 1]], dtype=np.float64)\n",
    "\n",
    "A = A.T\n",
    "\n",
    "Q, R = qr_GS(A)\n",
    "\n",
    "print(\"A = QR? --->\", np.allclose(A, Q @ R))\n",
    "print(\"Q^T Q = I? --->\", np.allclose(Q.T @ Q, np.eye(A.shape[0])))\n",
    "print(\"A\\n\", A)\n",
    "print(\"Q\\n\", Q)\n",
    "print(\"R\\n\", R)\n",
    "print(\"QR\\n\", Q @ R)\n",
    "print(\"Q^T Q\\n\", Q.T @ Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an $m$-by-$n$ matrix $A$, \n",
    "\n",
    "- Product\n",
    "  - both $QR$ (full QR factorization) and $\\hat Q \\hat R$ (reduced QR factorization) result in $A$.\n",
    "- Size of reduced QR\n",
    "  - (size of $\\hat Q$)$=$(size of $A$)\n",
    "  - $\\hat R$ has the compatible square size $n$-by-$n$ (for matrix multiplication).\n",
    "- Size of full QR\n",
    "  - (size of $R$)$=$(size of $A$) \n",
    "  - $Q$ has the compatible square size $m$-by-$m$ (for matrix multiplication)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- LU encodes Gaussian elimination.\n",
    "- QR encodes orthogonalization.\n",
    "- Orthogonal matrices are good because\n",
    "  - easy to invert\n",
    "  - does not amplify errors (by norm preservation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fact** (Complexity of QR factorization)\n",
    "\n",
    "- QR factorization of $A\\in\\mathbb{R}^{m\\times m}$ takes approximately $m^3$ multiplication or division and the same number of additions. (Sauer (2017) p. 224)\n",
    "  - Proof: HW problem\n",
    "- This is 3 times more than LU decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What to be careful of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Linear dependence)\n",
    "\n",
    "- If columns of $A$ are linearly dependent, Gram-Schmidt fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least square solution via QR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition** (Orthogonal matrix)\n",
    "\n",
    "A square matrix $Q$ is called *orthogonal* if $Q^T = Q^{-1}$. That is, $Q^T Q = QQ^T = I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (2-norm preserved by orthogonal matrix)\n",
    "\n",
    "If $Q$ is an orthogonal $m \\times m$ matrix and $x$ is an $m$-dimensional vector, then $\\|Q x\\|_2=\\|x\\|_2$.\n",
    "Proof. $\\|Q x\\|_2^2=(Q x)^T Q x=x^T Q^T Q x=x^T x=\\|x\\|_2^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** \n",
    "\n",
    "A square matrix is orthogonal if and only if its columns are pairwise orthogonal unit vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: HW problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We know \n",
    "   - Least square solution $\\bar x$ mimimizes $\\|A x -b\\|_2$\n",
    "   - $A=QR$\n",
    "   - $\\| Qy \\|_2 = \\| y \\|_2$\n",
    "   - $QQ^T=I$, hence $b = QQ^T b$\n",
    "2. Combining, we have\n",
    "   - Least square solution $\\bar x$ mimimizes $\\| Rx -Q^T b\\|_2$\n",
    "   - letting $d=Q^T b$, $\\bar x$ mimimizes 2-norm of the following\n",
    "\n",
    "$$\n",
    "e=R \\bar x -Q^T b \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "\\hat R \\\\\n",
    "O_{[k]}\n",
    "\\end{bmatrix} \\bar x - d \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "\\hat R  \\bar x - d_{[n]}\\\\\n",
    "- d_{[k]}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where $d_{[n]}$ and $d_{[n]}$ are vectors of first $n$ and last $k=m-n$ entries of $d$ respectively. That is,\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{c}\n",
    "e_1 \\\\\n",
    "\\vdots \\\\\n",
    "e_n \\\\\n",
    "\\hdashline e_{n+1} \\\\\n",
    "\\vdots \\\\\n",
    "e_m\n",
    "\\end{array}\\right]=\\left[\\begin{array}{cccc}\n",
    "r_{11} & r_{12} & \\cdots & r_{1 n} \\\\\n",
    "& r_{22} & \\cdots & r_{2 n} \\\\\n",
    "& & \\ddots & \\vdots \\\\\n",
    "& & & r_{n n} \\\\\n",
    "\\hdashline 0 & \\cdots & \\cdots & 0 \\\\\n",
    "\\vdots & & & \\vdots \\\\\n",
    "0 & \\cdots & \\cdots & 0\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{array}\\right]-\\left[\\begin{array}{c}\n",
    "d_1 \\\\\n",
    "\\vdots \\\\\n",
    "d_n \\\\\n",
    "\\hdashline d_{n+1} \\\\\n",
    "\\vdots \\\\\n",
    "d_m\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "3. Tackle upper and lower block separetely.\n",
    "   - Upper part $\\hat R  \\bar x - d_{[n]}$ can be made zero by choosing $\\bar x = \\hat R^{-1} \\bar x - d_{[n]}$ $\\longrightarrow$ Lease square solution.\n",
    "   - Lower part cannot cannot be controled by $\\bar x$ $\\longrightarrow$ backward error of least square $\\| d_{[k]} \\|_2 \\le \\| e \\|_2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to solve least square using QR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Given $m$-by-$n$ system $Ax = b$,\n",
    "\n",
    "1. Find full QR factorization of $A$.\n",
    "2. Extract essential blocks:\n",
    "   - $\\hat R$: upper $n$-by-$n$ submatrix of $R$, and\n",
    "   - $\\hat d$: first $n$ entries of $d=Q^T b$.\n",
    "3. Solve the system $\\hat R \\bar x = \\hat d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Data fitting revisited)\n",
    "\n",
    "Find the best line and best parabola for the four data points (−1, 1), (0, 0),\n",
    "(1,0),(2,−2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Decide a model\n",
    "   - linear ($c_1 + c_2 t$)\n",
    "   - parabola ($c_1 + c_2 t + c_3 t^2$)\n",
    "2. Force the model to fit data\n",
    "3. Find least square solution using QR factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# data points\n",
    "data = np.array([[-1, 1], [0, 0], [1,0], [2, -2]], dtype=np.float64)\n",
    "\n",
    "# number of data points/observations\n",
    "m = data.shape[0]\n",
    "\n",
    "# vector of unknowns (model 1)\n",
    "n1 = 2\n",
    "c1 = np.zeros(n1)\n",
    "\n",
    "# matrix of least squares (model 1)\n",
    "A1 = np.zeros((m, n1))\n",
    "ones = np.ones(m)\n",
    "A1 = np.column_stack((ones, data[:, 0]))\n",
    "\n",
    "# vector of observations\n",
    "b = data[:, 1]\n",
    "\n",
    "# Step 1: QR factorization\n",
    "Q, R = qr_GS(A1)\n",
    "\n",
    "# Step 2: Extract blocks\n",
    "R_ = R[:n1, :n1]\n",
    "d_ = (Q.T @ b)[:n1]\n",
    "\n",
    "# Step 3: Solve the upper triangular system\n",
    "c1 = np.linalg.solve(R_, d_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI usage: plotting is doen with copilot\n",
    "\n",
    "# Calculate the predicted values\n",
    "y_pred = c1[0] + c1[1] * data[:, 0]\n",
    "\n",
    "# Calculate the root mean squared error\n",
    "rmse = np.sqrt(np.mean((data[:, 1] - y_pred)**2))\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(data[:, 0], data[:, 1], label='Data')\n",
    "\n",
    "# Plot the linear model\n",
    "x = np.linspace(-2, 3, 100)\n",
    "y = c1[0] + c1[1] * x\n",
    "plt.plot(x, y, color='red', label='Linear Model')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Linear Model with Least Square Solution\\nRoot Mean Squared Error: {rmse:.3g}')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector of unknowns (model 2)\n",
    "n2 = 3\n",
    "c2 = np.zeros(n2)\n",
    "\n",
    "# matrix of least squares (model 2)\n",
    "A2 = np.zeros((m, n2))\n",
    "A2 = np.column_stack((ones, data[:, 0], data[:, 0]**2))\n",
    "\n",
    "# Step 1: QR factorization\n",
    "Q, R = qr_GS(A2)\n",
    "\n",
    "# Step 2: Extract blocks\n",
    "R_ = R[:n2, :n2]\n",
    "d_ = (Q.T @ b)[:n2]\n",
    "\n",
    "# Step 3: Solve the upper triangular system\n",
    "c2 = np.linalg.solve(R_, d_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI usage: plotting is doen with copilot\n",
    "\n",
    "# Calculate the predicted values\n",
    "y_pred = c2[0] + c2[1] * data[:, 0] + c2[2] * data[:, 0]**2\n",
    "\n",
    "# Calculate the root mean squared error\n",
    "rmse = np.sqrt(np.mean((data[:, 1] - y_pred)**2))\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(data[:, 0], data[:, 1], label='Data')\n",
    "\n",
    "# Plot the parabola model\n",
    "x = np.linspace(-2, 3, 100)\n",
    "y = c2[0] + c2[1] * x + c2[2] * x**2\n",
    "plt.plot(x, y, color='green', label='Parabola Model')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Parabola Model with Least Square Solution\\nRoot Mean Squared Error: {rmse:.3g}')\n",
    "\n",
    "# Show legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Householder reflector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Idea 1: If we have two vectors with the same length, we can find a reflection that sends one vector to the other. \n",
    "- Idea 2: Reflection can be expressed as orthogonal matrix.\n",
    "\n",
    "**Remark**\n",
    "\n",
    "- Idea 2 can be guessed from being invertible, length-preserving, and nice geometric meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Householder reflector](https://vzahorui.net/assets/images/linear_algebra/vector_reflection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma** (Rhombus)\n",
    "\n",
    "Assume that $x$ and $w$ are vectors of the same Euclidean length, $\\|x\\|_2=\\|w\\|_2$. Then $w-x$ and $w+x$ are perpendicular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof**\n",
    "\n",
    "$$\n",
    "(w-x)^T(w+x)=w^T w-x^T w+w^T x-x^T x=\\|w\\|^2-\\|x\\|^2=0.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defintion** (Projection)\n",
    "\n",
    "A map $P:\\mathbb{R}^m \\to \\mathbb{R}^m$ is called *projection* if $P^2 = P$, that is, $P(Pv)=Pv$ for any $v\\in\\mathbb{R}^m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Projection matrix)\n",
    "\n",
    "For a matrix $A\\in\\mathbb{R}^{m\\times n}$, \n",
    "\n",
    "$$\n",
    "P=A\\left(A^T A\\right)^{-1} A^T\n",
    "$$ \n",
    "\n",
    "defines the orthogonal projection from $\\mathbb{R}^{m}$ onto column space of $A$. In particular, if $A=v$ (a single column), then \n",
    "\n",
    "$$\n",
    "P=\\frac{v v^T}{v^T v}\n",
    "$$\n",
    "\n",
    "defines the orthogonal projection from ${R}^{m}$ onto the line $\\{\\lambda v \\ : \\  \\lambda \\in \\mathbb{R} \\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof: HW problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Projection matrix) \n",
    "\n",
    "Let $V\\in\\mathbb{R}^{m\\times n}$ be\n",
    "\n",
    "$$\n",
    "P=\\frac{v v^T}{v^T v} .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Householder reflector](https://www.cs.utexas.edu/users/flame/laff/alaff-beta/images/Chapter03/reflector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Householder reflector](https://blogs.mathworks.com/cleve/files/house_blog_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditioning (No improvement observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Conditioning of least square)\n",
    "\n",
    "- Solving least square problem using QR factorization shows better conditioning than normal equation.\n",
    "\n",
    "**Note**\n",
    "\n",
    "- The expected improvement was not observed.\n",
    "  - A closer look revealed the results of QR decomposition of Matlab and our current algorithm are different after 8-th column.\n",
    "  - This seems due to (a) different padding of columns, and (b) different QR algorithm.\n",
    "  - Try again after Householder-QR algorithm is established."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (ill-conditioned least square problem revisited)\n",
    "\n",
    "Let $x_0=2.0, x_2=2.2, x_3=2.4, \\ldots, x_{10}=4.0$ be equally spaced points in $[2,4]$, and set $y_i=1+x_i+x_i^2+x_i^3+x_i^4+x_i^5+x_i^6+x_i^7$ for $0 \\leq i \\leq 10$. Use the normal equations to find the least squares polynomial $P(x)=c_1+c_2 x+\\cdots+c_8 x^7$ fitting the $\\left(x_i, y_i\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from internallib import poly_eval\n",
    "\n",
    "# problem parameters\n",
    "# Suggestions: To check sanity, set n = 2, 3, 4, \n",
    "#   Things get wild soon after n = 5\n",
    "n = 8       # number of unknowns\n",
    "\n",
    "# right-hand side vector\n",
    "x = np.linspace(2., 4., 11)\n",
    "c_true = np.ones(n)\n",
    "y = poly_eval(c_true, x)\n",
    "\n",
    "# matrix\n",
    "pow = np.arange(n)\n",
    "# A = x[:, np.newaxis] ** pow       # broadcasting in effect\n",
    "A = x.reshape(-1, 1) ** pow         # equivalent to the previous line\n",
    "\n",
    "# Step 1: QR factorization\n",
    "Q, R = np.linalg.qr(A)\n",
    "\n",
    "# Step 2: Extract blocks\n",
    "R_ = R[:n, :n]\n",
    "d_ = (Q.T @ y)[:n]\n",
    "\n",
    "# Step 3: Solve the upper triangular system\n",
    "c = np.linalg.solve(R_, d_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'c_true':<10}{': '}{c_true}\")\n",
    "print(f\"{'c':<10}{': '}{c}\")\n",
    "print(f\"{'y':<10}{': '}{y}\")\n",
    "print(f\"{'x':<10}{': '}{x}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to be careful of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "This work is licensed under [Creative Commons Attribution-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrch",
   "language": "python",
   "name": "ptrch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
