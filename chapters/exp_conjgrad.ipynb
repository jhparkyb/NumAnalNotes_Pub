{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem of interest**\n",
    "\n",
    "Given an $n$-by-$n$ symmetric positive definite (SPD) matrix $A$ and a $\\mathbb{R}^n$-vector $b$, find $\\mathbb{R}^n$-vector $x$ such that\n",
    "\n",
    "$$ Ax = b. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method: Conjugate gradient method\n",
    "\n",
    "**Data**\n",
    "\n",
    "- $x_0 \\in \\mathbb{R}^n$: initial guess\n",
    "\n",
    "**Initialize**\n",
    "\n",
    "- $d_0=r_0=b-Ax_0$\n",
    "\n",
    "**Main computation**\n",
    "\n",
    "- **for** k = 0, 1, 2, ..., $n$ - 1\n",
    "    - **if** $r_k = 0$ **stop**, **end**\n",
    "    - $\\alpha_{k}=\\frac{r_{k}^{T} r_{k}}{d_{k}^{T} A d_{k}}$ (compute $d_k$ component of error)\n",
    "    - $x_{k+1}=x_{k}+\\alpha_{k} d_{k}$ (subtract it out)\n",
    "    - $r_{k+1}=r_{k}-\\alpha_{k} A d_{k}$ (compute the new residual)\n",
    "    - $\\beta_{k}=\\frac{r_{k+1}^{T} r_{k+1}}{r_{k}^{T} r_{k}}$ (compute $d_k$ component of the residual)\n",
    "    - $d_{k+1}=r_{k+1}+\\beta_{k} d_{k}$ (conduct Gram-Schmidt with respect to $A$-inner product)\n",
    "**end**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**History**\n",
    "\n",
    "1. First proposed by Schmidt (1908) [^1] (*the* Schmidt in Gram-Schmidt)\n",
    "1. Independently re-invented by Fox, Huskey, and Wilkinson (1948) [^2]\n",
    "1. Hestenes and Stiefel (1952) made this idea explicit and practical. [^3]\n",
    "1. CG does not reach the solution in $n$ steps in practice due to round off errors. It became popular only after Reid (1971) showed its value as an iterative method for large, sparse matrices. [^4] \n",
    "\n",
    "[^1]: Schmidt (1908) Uber die Auflosung linearer Gleichungen mit Unendlich vielen unbekannten (accent removed)\n",
    "\n",
    "[^2]: Fox, Huskey, and Wilkinson (1948) Notes on the solution of algebraic linear simultaneous equations\n",
    "\n",
    "[^3]: Hestenes and Stiefel (1952) Methods of conjugate gradients for solving linear systems \n",
    "\n",
    "[^4]: Reid (1971) On the method of conjugate gradients for the solution of large\n",
    "\tsparse systems of linear equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation/Settings\n",
    "\n",
    "| expression | meaning |\n",
    "|---|---|\n",
    "| $x$ | true solution ($Ax=b$) |\n",
    "| $x_k$ | $k$-th approximate solution by the conjugate gradient method ($k=0,1,2,\\cdots$)|\n",
    "| $e_k$ | $=x - x_k$ (error caused by $x_k$) |\n",
    "| $r_k$ | $=b-Ax_k$ (residual caused by $x_k$) |\n",
    "| $d_k$ | conjugate directions |\n",
    "| $(u,v)$ | $=v^T u$ (standard inner product) |\n",
    "| $(u,v)_A$ | $=v^T A u$ (A-inner product, where $A$ is a SPD matrix) |\n",
    "| $v \\perp_A u$ | $(u,v)_A =0$ (A-orthogonality) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea behind the conjugate gradient method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Warm up (Gaussian elimination)\n",
    "\n",
    "1. Change the perspective\n",
    "\n",
    "View the process of finding the solution as removing components of error one by one. Unless we are extremely lucky, our error caused by the initial guess will be full of possible components. \n",
    "\n",
    "For the moment, to avoid introducing too many symbols, let us override the notations and let $d_k$'s be canonical basis of $\\mathbb{R}^{n}$. \n",
    "\n",
    "We can expand the initial error in the canonical basis.\n",
    "\n",
    "$$\n",
    "e_{0}=\\sum_{k=0}^{n-1} \\eta_k d_k\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Remove component one by one\n",
    "\n",
    "Then the back substitution step can be seen as removing $d_{n-2}$ component from the error, then $d_{n-3}$ component, all the way to $d_{0}$. ($d_{n-1}$ component is already absent since the last component of $x$ is precise.)\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1&-\\frac{1}{2}&\\frac 3 4 & \\frac 7 4 \\\\\n",
    "0&1& \\frac 3 5 & \\frac {26} 5\\\\\n",
    "0&0&1& \\underbrace{2}_{x_1} \n",
    "\\end{bmatrix}\n",
    "\\longrightarrow\t\n",
    "\\begin{bmatrix}\n",
    "1&-\\frac{1}{2}&0 & -1 \\\\\n",
    "0&1& 0& 4\\\\\n",
    "0&0&1& \\underbrace{2}_{x_2}  \n",
    "\\end{bmatrix}\n",
    "\\longrightarrow\t\n",
    "\\begin{bmatrix}\n",
    "1&0&0 & 1 \\\\\n",
    "0&1& 0& 4\\\\\n",
    "0&0&1& \\underbrace{2}_{x_3}  \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "![Gaussian elimination from error improvement point of view](../images/CG01.png)\n",
    "\n",
    "**Remark**\n",
    "\n",
    "\n",
    "- Removing $d_k$ from the approximate solution $x_k$ and removing it from the error $e_k$ are equivalent because they differ only by a fixed vector, the true solution $x$, that is, $e_k = x - x_k$. But the signs of $e_k$ and $x_k$ are opposite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Algorithm-friendly summary\n",
    "\n",
    "\n",
    "Given $x_k$, hence $e_k = x - x_k$, take exact step to remove $d_k$ component each time\n",
    "\n",
    "$$\n",
    "e_{k+1}=e_k - \\eta_k d_k.\n",
    "$$\n",
    "\n",
    "or equivalently, \n",
    "\n",
    "$$\n",
    "x_{k+1}=x_k + \\eta_k d_k,\n",
    "$$\n",
    "\n",
    "The exact component $\\eta_k$ in $d_k$ direction can be computed by the condition $e_{k+1} \\perp d_k$ since we just subtracted out $d_k$:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "e_{1}&=\\sum_{j=1}^{n-1} \\eta_k d_j\n",
    "\\\\\n",
    "e_{2}&=\\sum_{j=2}^{n-1} \\eta_k d_j\n",
    "\\\\\n",
    "&\\vdots\n",
    "\\\\\n",
    "e_{k+1}&=\\sum_{j=k+1}^{n-1} \\eta_k d_j.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "By taking dot product with $d_k$ on both sides of the error update rule $e_{k+1}=e_k - \\eta_k d_k$, we can find\n",
    "\n",
    "$$\n",
    "\\eta_k = \\frac{d_k^T e_{k}}{d_k^T d_k}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "- $e_k$ is not computable since we do not know the true solution $x$.\n",
    "- The residual $r_k:=b - Ax_k$ is computable. \n",
    "- Since $r_k = b - A x_k = A x - A x_k = A(x - x_k) = Ae_k$, the residual and the error are related:\n",
    "\n",
    "$$\n",
    "r_k = Ae_k\n",
    "$$\n",
    "\n",
    "- Though not at all trivial, these observations suggest a possibility to play between A-orthogonality and orthogonality for our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Devising conjugate gradient method\n",
    "\n",
    "**Notation**\n",
    "\n",
    "From now on, $\\{d_k\\}$ is an A-orthogonal basis.\n",
    "\n",
    "1. View finding the solution as removing $d_k$ component at a time. \n",
    "\n",
    "2. Suppose we have an A-orthogonal basis $\\{d_k\\}_{k=0}^{n-1}$. (We will discuss how to obtain it. For now, assume it is possible.) We can expand the initial error in this basis. Again, it is likely that the initial guess contains all components of $d_k$'s:\n",
    "\n",
    "$$\n",
    "e_{0}=\\sum_{k=0}^{n-1} \\eta_k d_k\n",
    "$$\n",
    "\n",
    "3. Suppose we can remove $d_k$ component from the error each time (this can be achieved by finding $\\eta_k$ momentarily):\n",
    "\n",
    "$$\n",
    "e_{k+1}=e_k - \\eta_k d_k,\n",
    "$$\n",
    "\n",
    "or equivalently from approximate solutions\n",
    "\n",
    "$$\n",
    "x_{k+1}=x_k - \\eta_k d_k.\n",
    "$$\n",
    "\n",
    "\n",
    "Then, we are left with less and less $d_k$ components in the error:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "e_{1}&=\\sum_{j=1}^{n-1} \\eta_k d_j\n",
    "\\\\\n",
    "e_{2}&=\\sum_{j=2}^{n-1} \\eta_k d_j\n",
    "\\\\\n",
    "&\\vdots\n",
    "\\\\\n",
    "e_{k+1}&=\\sum_{j=k+1}^{n-1} \\eta_k d_j.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Thus, since $e_{k+1}$ has no $d_k$ component (it has just been removed), we must have $e_{k+1} \\perp_{A} d_k$. Use this fact to the error update rule, then we can compute $\\eta_k$:\n",
    "\n",
    "$$\n",
    "\\eta_k = \\frac{d_k^T A e_k}{d_k^T A d_k}=\\frac{d_k^T r_k}{d_k^T A d_k} \\quad \\text{(computable)}.\n",
    "$$\n",
    "\n",
    "This is where $A$-orthogonality is used in a nice way.\n",
    "\n",
    "In the algorithm, we use $\\eta_k$ formula to update $x_k$'s since $e_k$ is not computable during implementation.\n",
    "\n",
    "4. New residuals guarantee new direction to explore to remove further $d$-components. In fact, it is orthogonal to all the past directions $d_k$'s:\n",
    "\n",
    "$$ \n",
    "r_{k+1} \\perp \\mathrm{span} \\{ d_j \\ : \\  0 \\le j \\le k \\}\n",
    "$$\n",
    "\n",
    "since, by mutual A-orthogonality between $d$'s, \n",
    "\n",
    "$$\n",
    "d_j^T r_{k+1} = d_j^T A e_{k+1} = \\sum_{i=k+1}^{n-1} \\eta_k d_j^T A  d_i = 0.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrch",
   "language": "python",
   "name": "ptrch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
