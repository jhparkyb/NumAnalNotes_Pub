{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take-aways \n",
    "\n",
    "(Method and implementation)\n",
    "\n",
    "- The algorithm of the conjugate gradient method for a system of linear equations.\n",
    "- Brief history.\n",
    "- Computation 1: 3-by-3 example\n",
    "- Computation 2: 1000-by-1000 example (Poisson problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem of interest**\n",
    "\n",
    "Given an $n$-by-$n$ symmetric positive definite (SPD) matrix $A$ and a $\\mathbb{R}^n$-vector $b$, find $\\mathbb{R}^n$-vector $x$ such that\n",
    "\n",
    "$$ Ax = b. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method: Conjugate gradient method\n",
    "\n",
    "**Data**\n",
    "\n",
    "- $x_0 \\in \\mathbb{R}^n$: initial guess\n",
    "\n",
    "**Initialization**\n",
    "\n",
    "- $d_0=r_0=b-Ax_0$\n",
    "\n",
    "**Main computation**\n",
    "\n",
    "- **for** k = 0, 1, 2, ..., $n$ - 1\n",
    "    - **if** $r_k = 0$ **stop**, **end**\n",
    "    - $\\alpha_{k}=\\frac{r_{k}^{T} r_{k}}{d_{k}^{T} A d_{k}}$ (compute $d_k$ component of error)\n",
    "    - $x_{k+1}=x_{k}+\\alpha_{k} d_{k}$ (subtract it out)\n",
    "    - $r_{k+1}=r_{k}-\\alpha_{k} A d_{k}$ (compute the new residual)\n",
    "    - $\\beta_{k}=\\frac{r_{k+1}^{T} r_{k+1}}{r_{k}^{T} r_{k}}$ (compute $d_k$ component of the residual)\n",
    "    - $d_{k+1}=r_{k+1}+\\beta_{k} d_{k}$ (conduct Gram-Schmidt with respect to $A$-inner product)\n",
    "\n",
    "    **end**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "- The conjugate gradient (CG) method can solve $Ax=b$, where $A$ is symmetric positive definite.\n",
    "- In theory, it finds the solution in at most $n$ steps, where $n$ is the dimension of the vectors $x$ and $b$. \n",
    "  - Hence, it can be seen as a direct method, i.e., yielding the true solution in finite number of steps. \n",
    "  - (Downside as a direct method) In terms of complexity, the CG requires about three times computations than Gaussian elimination as a direct method. ([^1] p. 131)\n",
    "  - (Downside as direct method) It is known that if $A$ is ill-conditioned the round-off errors prevent it from giving the accurate solution. In fact, its performance is worse than Gaussian elimination. ([^1] p. 131)\n",
    "- In practice, it can be better used as an iterative method.\n",
    "  - If $A$ is sparse, the computational cost of the CG siginficantly drops since its main workhorse is matrix-vector multiplication ($Ad_k$ in the algorithm).\n",
    "  - Introducing preconditioner enhance the performance of the CG for ill-conditioned matrices. \n",
    "\n",
    "[^1]: Sauer (2017) Numerical Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**History**\n",
    "\n",
    "1. First proposed by Schmidt (1908) [^1] (*the* Schmidt in Gram-Schmidt)\n",
    "1. Independently re-invented by Fox, Huskey, and Wilkinson (1948) [^2]\n",
    "1. Hestenes and Stiefel (1952) made this idea explicit and practical. [^3]\n",
    "1. It became popular only after Reid (1971) showed its value as an iterative method for large, sparse matrices. (CG is supposed to reach the solution in $n$ steps in theory, but it does not in practice due to round off errors. Reid founds its value in another way.)[^4] \n",
    "\n",
    "[^1]: Schmidt (1908) Uber die Auflosung linearer Gleichungen mit Unendlich vielen unbekannten (accent removed)\n",
    "\n",
    "[^2]: Fox, Huskey, and Wilkinson (1948) Notes on the solution of algebraic linear simultaneous equations\n",
    "\n",
    "[^3]: Hestenes and Stiefel (1952) Methods of conjugate gradients for solving linear systems \n",
    "\n",
    "[^4]: Reid (1971) On the method of conjugate gradients for the solution of large\n",
    "\tsparse systems of linear equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def CG(A, b, x0, tol=1e-9, max_iter=None):\n",
    "    \"\"\"\n",
    "    Conjugate Gradient method for solving linear systems of equations.\n",
    "    \n",
    "    Parameters:\n",
    "        A (ndarray): The coefficient matrix of the linear system.\n",
    "        b (ndarray): The right-hand side vector of the linear system.\n",
    "        x0 (ndarray): The initial guess for the solution.\n",
    "        max_iter (int): The maximum number of iterations (default: 100).\n",
    "        tol (float): The tolerance for convergence (default: 1e-9).\n",
    "    \n",
    "    Returns:\n",
    "        x (ndarray): The approximate solution to the linear system.\n",
    "        i (int): The number of iterations performed.\n",
    "    \"\"\"\n",
    "    # default number of iterations is the dimension of the matrix\n",
    "    if max_iter is None:\n",
    "        n = A.shape[0]\n",
    "        max_iter = n\n",
    "\n",
    "    # initialize\n",
    "    x = x0\n",
    "    r = b - A @ x\n",
    "    d = r\n",
    "\n",
    "    r_nrm2 = np.dot(r, r)\n",
    "\n",
    "    for i in range(1, max_iter + 1): \n",
    "        # intermediate computations part 1 \n",
    "        Ad = A @ d\n",
    "\n",
    "        # main conjugate gradient iteration part 1\n",
    "        alpha = r_nrm2 / np.dot(d, Ad)\n",
    "        x = x + alpha * d\n",
    "        r_new = r - alpha * Ad\n",
    "        \n",
    "        # intermediate computations part 2\n",
    "        r_new_nrm2 = np.dot(r_new, r_new)\n",
    "        \n",
    "        # stopping criterion\n",
    "        if np.sqrt(r_new_nrm2) < tol:\n",
    "            break\n",
    "\n",
    "        # main conjugate gradient iteration part 2\n",
    "        beta = r_new_nrm2 / r_nrm2\n",
    "        d = r_new + beta * d\n",
    "        \n",
    "        # updata\n",
    "        r = r_new\n",
    "        r_nrm2 = r_new_nrm2\n",
    "    \n",
    "    return x, i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (CG) =  [-3.75 -2.5  -7.25] in 3 iterations\n",
      "x_true =  [-3.75 -2.5  -7.25]\n",
      "error =  3.4684476073050936e-15\n"
     ]
    }
   ],
   "source": [
    "# A is of full rank\n",
    "A = np.array([  [1, 2 , -1],\n",
    "                [2, 1 , -2],\n",
    "                [-3, 1,  1]])\n",
    "\n",
    "# take a SPD matrix\n",
    "A = A.T @ A\n",
    "\n",
    "b = np.array([3, 3, -6])\n",
    "\n",
    "x0 = np.array([0, 0, 0])\n",
    "\n",
    "x, iter = CG(A, b, x0)\n",
    "x_true = np.linalg.solve(A, b)\n",
    "\n",
    "print(\"x (CG) = \", x, \"in\", iter, \"iterations\")\n",
    "print(\"x_true = \", x_true)\n",
    "print(\"error = \", np.linalg.norm(x - x_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Poisson problem)\n",
    "\n",
    "Let $L > 0$ and $\\Omega =(0, L)$. Consider the problem: find $u:\\Omega \\to \\mathbb{R}$ such that\n",
    "\n",
    "$$\n",
    "-\\frac{d^2 u}{dx^2} = f(x), \\quad \\text{and} \\quad u(0)=u(L)=0\n",
    "$$\n",
    "\n",
    "What is called *finite difference method* involving $n$ uniform nodes (excluding boundary points) leads to the following system of linear equation: $Aw=b$, where $A$ and $b$ are $n$-by-$n$ matrix and size $n$ vector give by\n",
    "\n",
    "$$\n",
    "A=\\frac{1}{h^2}\\left[\\begin{array}{ccccc}\n",
    "2 & -1 & 0 & \\ldots & 0 \\\\\n",
    "-1 & 2 & \\ddots & & \\vdots \\\\\n",
    "0 & \\ddots & \\ddots & -1 & 0 \\\\\n",
    "\\vdots & & -1 & 2 & -1 \\\\\n",
    "0 & \\ldots & 0 & -1 & 2\n",
    "\\end{array}\\right]\n",
    "\\quad \\text{and} \\quad b = \n",
    "\\left[\\begin{array}{c}\n",
    "f(x_1) \\\\\n",
    "f(x_2) \\\\\n",
    "f(x_3) \\\\\n",
    "\\vdots \\\\\n",
    "f(x_n) \\\\\n",
    "\\end{array}\\right],\n",
    "$$\n",
    "\n",
    "$x_i = ih$ and $h = L/(n+1)$.\n",
    "\n",
    "If $f(x)=1$, then the analytic solution is available. By integrating the differential equation twice and plugging in the two boundary values, we obtain \n",
    "\n",
    "$$\n",
    "u(x)= - \\frac{1}{2} x^2 + \\frac{L}{2} x.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CG iterations =  501\n",
      "total CG error =  0.009135555538667423\n",
      "discretization error =  0.009135555538379558\n",
      "algebraic error =  5.855469175306516e-13\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.linalg import toeplitz\n",
    "\n",
    "# problem settings\n",
    "L = 1\n",
    "n = 1000\n",
    "\n",
    "# discrete domain\n",
    "h = L / n\n",
    "x = np.linspace(h, L, n)\n",
    "\n",
    "# create the matrix\n",
    "col = np.zeros(n)\n",
    "col[0:2] = np.array([2, -1]) * (1 / (h*h))\n",
    "A = toeplitz(col)\n",
    "\n",
    "# create the right-hand side\n",
    "b = np.ones(n)\n",
    "x0 = np.zeros(n)\n",
    "\n",
    "w, iter = CG(A, b, x0, tol=1e-12)\n",
    "u_true = - 0.5 * x * (x - L)\n",
    "w_direct = np.linalg.solve(A, b)\n",
    "\n",
    "#%% reporting\n",
    "print(\" CG iterations = \", iter)\n",
    "print(\"total CG error = \", np.linalg.norm(w - u_true))\n",
    "print(\"discretization error = \", np.linalg.norm(w_direct - u_true))\n",
    "print(\"algebraic error = \", np.linalg.norm(w - w_direct))\n",
    "\n",
    "#%% plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, w, label=\"CG\")\n",
    "ax.plot(x, u_true, label=\"true\")\n",
    "ax.plot(x, w_direct, label=\"direct\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"u\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- `numpy.linalg.solve` uses a very reliable library LAPACK. ([NumPy Documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html), [LAPACK Wikipedia page](https://en.wikipedia.org/wiki/LAPACK), [LAPACK webpage](https://www.netlib.org/lapack/)). We use its result as \"true solution.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation/Settings\n",
    "\n",
    "| expression | meaning |\n",
    "|---|---|\n",
    "| $x$ | true solution ($Ax=b$) |\n",
    "| $x_k$ | $k$-th approximate solution by the conjugate gradient method ($k=0,1,2,\\cdots$)|\n",
    "| $e_k$ | $=x - x_k$ (error caused by $x_k$) |\n",
    "| $r_k$ | $=b-Ax_k$ (residual caused by $x_k$) |\n",
    "| $d_k$ | conjugate directions |\n",
    "| $(u,v)$ | $=v^T u$ (standard inner product) |\n",
    "| $(u,v)_A$ | $=v^T A u$ (A-inner product, where $A$ is a SPD matrix) |\n",
    "| $v \\perp_A u$ | $(u,v)_A =0$ (A-orthogonality) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea behind the conjugate gradient method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "A minimization approach to conjugate gradient method is also helpful, where we use the equivalence\n",
    "\n",
    "$$\n",
    "Ax = b \\quad \\leftrightarrow \\quad \\mathrm{minimize} ~ \\frac{1}{2} x^T A x - b^T x, \\quad \\text{subject to} ~ x \\in \\mathbb{R}^n.\n",
    "$$\n",
    "\n",
    "However, this involves more discussions from different angles. Between the trade-off of richness and simplicity in presentation, these notes have chosen the simplicity, and do not discuss minimization approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Warm up (Gaussian elimination)\n",
    "\n",
    "1. Change the perspective\n",
    "\n",
    "View the process of finding the solution as removing components of error one by one. Unless we are extremely lucky, our error caused by the initial guess will be full of possible components. \n",
    "\n",
    "For the moment, to avoid introducing too many symbols, let us override the notations and let $d_k$'s be canonical basis of $\\mathbb{R}^{n}$. \n",
    "\n",
    "We can expand the initial error in the canonical basis.\n",
    "\n",
    "$$\n",
    "e_{0}=\\sum_{k=0}^{n-1} \\eta_k d_k\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Remove component one by one\n",
    "\n",
    "Then the back substitution step can be seen as removing $d_{n-2}$ component from the error, then $d_{n-3}$ component, all the way to $d_{0}$. ($d_{n-1}$ component is already absent since the last component of $x$ is precise.)\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1&-\\frac{1}{2}&\\frac 3 4 & \\frac 7 4 \\\\\n",
    "0&1& \\frac 3 5 & \\frac {26} 5\\\\\n",
    "0&0&1& \\underbrace{2}_{x_1} \n",
    "\\end{bmatrix}\n",
    "\\longrightarrow\t\n",
    "\\begin{bmatrix}\n",
    "1&-\\frac{1}{2}&0 & -1 \\\\\n",
    "0&1& 0& 4\\\\\n",
    "0&0&1& \\underbrace{2}_{x_2}  \n",
    "\\end{bmatrix}\n",
    "\\longrightarrow\t\n",
    "\\begin{bmatrix}\n",
    "1&0&0 & 1 \\\\\n",
    "0&1& 0& 4\\\\\n",
    "0&0&1& \\underbrace{2}_{x_3}  \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "![Gaussian elimination from error improvement point of view](../images/CG01.png)\n",
    "\n",
    "**Remark**\n",
    "\n",
    "\n",
    "- Removing $d_k$ from the approximate solution $x_k$ and removing it from the error $e_k$ are equivalent because they differ only by a fixed vector, the true solution $x$, that is, $e_k = x - x_k$. But the signs of $e_k$ and $x_k$ are opposite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Algorithm-friendly summary\n",
    "\n",
    "\n",
    "Given $x_k$, hence $e_k = x - x_k$, take exact step to remove $d_k$ component each time\n",
    "\n",
    "$$\n",
    "e_{k+1}=e_k - \\eta_k d_k.\n",
    "$$\n",
    "\n",
    "or equivalently, \n",
    "\n",
    "$$\n",
    "x_{k+1}=x_k + \\eta_k d_k,\n",
    "$$\n",
    "\n",
    "The exact component $\\eta_k$ in $d_k$ direction can be computed by the condition $e_{k+1} \\perp d_k$ since we just subtracted out $d_k$:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "e_{1}&=\\sum_{j=1}^{n-1} \\eta_k d_j\n",
    "\\\\\n",
    "e_{2}&=\\sum_{j=2}^{n-1} \\eta_k d_j\n",
    "\\\\\n",
    "&\\vdots\n",
    "\\\\\n",
    "e_{k+1}&=\\sum_{j=k+1}^{n-1} \\eta_k d_j.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "By taking dot product with $d_k$ on both sides of the error update rule $e_{k+1}=e_k - \\eta_k d_k$, we can find\n",
    "\n",
    "$$\n",
    "\\eta_k = \\frac{d_k^T e_{k}}{d_k^T d_k}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "- $e_k$ is not computable since we do not know the true solution $x$.\n",
    "- The residual $r_k:=b - Ax_k$ is computable. \n",
    "- Since $r_k = b - A x_k = A x - A x_k = A(x - x_k) = Ae_k$, the residual and the error are related:\n",
    "\n",
    "$$\n",
    "r_k = Ae_k\n",
    "$$\n",
    "\n",
    "- Though not at all trivial, these observations suggest a possibility to play between A-orthogonality and orthogonality for our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Devising conjugate gradient method: Part 1 (main idea)\n",
    "\n",
    "1. Suppose we have an A-orthogonal basis $\\{d_k\\}_{k=0}^{n-1}$ for $\\mathbb{R}^n$. (We will discuss how to obtain it. For now, assume it is possible.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. View finding the solution as removing $d_k$ component at a time. We can expand the initial error in this basis. \n",
    "\n",
    "$$\n",
    "e_{0}=\\sum_{k=0}^{n-1} \\alpha_k d_k \\quad \\text{(error update)}\n",
    "$$\n",
    "\n",
    "It is likely that the initial guess contains all components of $d_k$'s.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Suppose we can remove $d_k$ component from the previous error each time (this can be achieved by finding $\\alpha_k$ momentarily):\n",
    "\n",
    "$$\n",
    "e_{k+1}=e_k - \\alpha_k d_k.\n",
    "$$\n",
    "\n",
    "Equivalently, we can write this in terms of $x_k$'s\n",
    "\n",
    "$$\n",
    "x_{k+1}=x_k + \\alpha_k d_k.\n",
    "$$\n",
    "\n",
    "Then, we are left with less and less $d_k$ components in the error:\n",
    "\n",
    "$$\n",
    "e_{k+1}=\\sum_{j=k+1}^{n-1} \\alpha_k d_j\n",
    "$$\n",
    "\n",
    "since\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "e_{1}&=\\sum_{j=1}^{n-1} \\alpha_k d_j\n",
    "\\\\\n",
    "e_{2}&=\\sum_{j=2}^{n-1} \\alpha_k d_j\n",
    "\\\\\n",
    "&\\vdots\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Thus, since $e_{k+1}$ has no $d_k$ component (it has just been removed), we must have $e_{k+1} \\perp_{A} d_k$. Use this fact to the error update rule, then we can compute $\\alpha_k$:\n",
    "\n",
    "$$\n",
    "\\alpha_k = \\frac{d_k^T A e_k}{d_k^T A d_k}=\\frac{d_k^T r_k}{d_k^T A d_k} \\quad \\text{(computable)}.\n",
    "$$\n",
    "\n",
    "**Remark**\n",
    "\n",
    "- This is where $A$-orthogonality is used in a nice way.\n",
    "- In the algorithm, we use $\\alpha_k$ formula to update $x_k$'s since $e_k$ is not computable during implementation.\n",
    "- Since there are only $n$ linearly independent directions $d_k$'s, in theory, the conjugate gradient will eventually exhaust all directions $d_k$'s at most $n$ steps and yield the solution. From this point of view, it can be seen as a direct method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Devising conjugate gradient method: Part 2(conjugate directions)\n",
    "\n",
    "**Question**: How to find the $A$-orthogonal basis $\\{d_k\\}$?\n",
    "\n",
    "**Idea**: Gram-Schmidt process with respect to $A$-inner product. We will construct $d_k$'s dynamically as the algorithm proceeds.\n",
    "\n",
    "**Observation 1**\n",
    "\n",
    "Computation dependency (incomplete):\n",
    "\n",
    "$$\n",
    "x_k, d_k, r_k \\longrightarrow \\alpha_k \\longrightarrow x_{k+1}, r_{k+1} \\longrightarrow  d_{k+1} ~ (?)\n",
    "$$\n",
    "\n",
    "**Observation 2-(a)**\n",
    "\n",
    "New residual $r_{k+1}$ guarantees a new direction outside of $\\mathrm{span}\\{d_0, d_1, \\cdots, d_k \\}$. In fact, if everything works out as we planned, it is orthogonal to all the past directions:\n",
    "\n",
    "$$ \n",
    "r_{k+1} \\perp \\mathrm{span} \\{ d_j \\ : \\  0 \\le j \\le k \\}\n",
    "$$\n",
    "\n",
    "since, by mutual A-orthogonality between $d_k$'s, \n",
    "\n",
    "$$\n",
    "d_j^T r_{k+1} = d_j^T A e_{k+1} = \\sum_{i=k+1}^{n-1} \\eta_k d_j^T A  d_i = 0.\n",
    "$$\n",
    "\n",
    "**Observation 2-(b)**\n",
    "\n",
    "![Gram-Schmidt](https://upload.wikimedia.org/wikipedia/commons/7/7b/Linalg_orth_proj_R3.png)\n",
    "\n",
    "We can extract $d_{k+1}$ from $r_{k+1}$ by filtering out $\\{d_0, d_1, \\cdots, d_k \\}$: Gram-Schmidt.\n",
    "\n",
    "$$\n",
    "d_{k+1} = r_{k+1} +\\sum_{j=0}^{k}\\beta_{j} d_j,\n",
    "$$\n",
    "\n",
    "where by taking $A$-inner product with each of $d_{j}$'s, we know, for $j=0,1,\\cdots,k$\n",
    "\n",
    "$$\n",
    "\\beta_j = - \\frac{d_j^T A r_{k+1}}{d_j^T A d_j}.\n",
    "$$\n",
    "\n",
    "**Remark**\n",
    "\n",
    "- This turns out not to be practical in computation. We need to store $d_j$'s, but this runs out of memory quickly for large problems. However, surprisingly, we only need to store the last one. (See below)\n",
    "\n",
    "**Observation 3**\n",
    "\n",
    "A close look reveals $\\beta_j=0$ for $j=0,1,\\cdots,k-1$, meaning, we need to store only $d_k$, and the Gram-Schmidt process step reduces to \n",
    "\n",
    "$$\n",
    "d_{k+1} = r_{k+1} +\\beta_{k} d_k,\n",
    "$$\n",
    "\n",
    "where, as in the previous formular for $\\beta_j$'s,\n",
    "\n",
    "$$\n",
    "\\beta_k = - \\frac{r_{k+1}^T A d_k^T}{d_{k}^T A d_k^T}.\n",
    "$$\n",
    "\n",
    "**Remark**\n",
    "\n",
    "- This observation involves a fair amount of details. Hence, we skip it for now in favor of a better bigger picture.\n",
    "\n",
    "**Complete computation dependency**\n",
    "\n",
    "Now that we know how to compute $d_{k+1}$, we have completed a working loop:\n",
    "\n",
    "$$\n",
    "x_k, d_k, r_k \\longrightarrow \\alpha_k \\longrightarrow x_{k+1}, r_{k+1} \\longrightarrow \\beta_k \\longrightarrow d_{k+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some computational optimizations\n",
    "\n",
    "The formulas for $\\alpha_k$, $\\beta_k$, and $r_{k+1}$ from the discussion of \"devising conjugate gradient method\" convey the idea behind it. However, there are equivalent expressions that are computationally more efficient.\n",
    "\n",
    "| formula from idea | formula for computation | Advantage |\n",
    "|---|---|---|\n",
    "| $$\\alpha_k=\\frac{d_k^T r_k}{d_k^T A d_k}$$ | $$\\alpha_k=\\frac{r_k^T r_k}{d_k^T A d_k}$$ | recycle $r_k^T r_k$ (this is the same as $r_{k+1}^T r_{k+1}$ from the previous iteration) |\n",
    "| $$x_{k+1}=x_k+\\alpha_k d_k$$ | Same |\n",
    "| $$r_{k+1}=b - Ax_{k+1}$$ | $$r_{k+1}=r_k-\\alpha_k A d_k$$ | avoid $Ax_{k+1}$ and recycle $A d_k$ |\n",
    "| $$\\beta_k = - \\frac{r_{k+1}^T A d_k^T}{d_{k}^T A d_k^T}$$| $$\\beta_k=\\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}$$ | recycle $r_k^T r_k$ and avoid $r_{k+1}^T (A d_k^T)$ |\n",
    "| $$d_{k+1}=r_{k+1}+\\beta_k d_k$$ | Same |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrch",
   "language": "python",
   "name": "ptrch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
