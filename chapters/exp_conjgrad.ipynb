{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem of interest**\n",
    "\n",
    "Given an $n$-by-$n$ symmetric positive definite (SPD) matrix $A$ and a $\\mathbb{R}^n$-vector $b$, find $\\mathbb{R}^n$-vector $x$ such that\n",
    "\n",
    "$$ Ax = b. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method: Conjugate gradient method\n",
    "\n",
    "**Data**\n",
    "\n",
    "- $x_0 \\in \\mathbb{R}^n$: initial guess\n",
    "\n",
    "**Initialize**\n",
    "\n",
    "- $d_0=r_0=b-Ax_0$\n",
    "\n",
    "**Main computation**\n",
    "\n",
    "- **for** k = 0, 1, 2, ..., $n$ - 1\n",
    "    - **if** $r_k = 0$ **stop**, **end**\n",
    "    - $\\alpha_{k}=\\frac{r_{k}^{T} r_{k}}{d_{k}^{T} A d_{k}}$ (compute $d_k$ component of error)\n",
    "    - $x_{k+1}=x_{k}+\\alpha_{k} d_{k}$ (subtract it out)\n",
    "    - $r_{k+1}=r_{k}-\\alpha_{k} A d_{k}$ (compute the new residual)\n",
    "    - $\\beta_{k}=\\frac{r_{k+1}^{T} r_{k+1}}{r_{k}^{T} r_{k}}$ (compute $d_k$ component of the residual)\n",
    "    - $d_{k+1}=r_{k+1}+\\beta_{k} d_{k}$ (conduct Gram-Schmidt with respect to $A$-inner product)\n",
    "**end**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**History**\n",
    "\n",
    "1. First proposed by Schmidt (1908) [^1] (*the* Schmidt in Gram-Schmidt)\n",
    "1. Independently re-invented by Fox, Huskey, and Wilkinson (1948) [^2]\n",
    "1. Hestenes and Stiefel (1952) made this idea explicit and practical. [^3]\n",
    "1. CG does not reach the solution in $n$ steps in practice due to round off errors. It became popular only after Reid (1971) showed its value as an iterative method for large, sparse matrices. [^4] \n",
    "\n",
    "[^1]: Schmidt (1908) Uber die Auflosung linearer Gleichungen mit Unendlich vielen unbekannten (accent removed)\n",
    "\n",
    "[^2]: Fox, Huskey, and Wilkinson (1948) Notes on the solution of algebraic linear simultaneous equations\n",
    "\n",
    "[^3]: Hestenes and Stiefel (1952) Methods of conjugate gradients for solving linear systems \n",
    "\n",
    "[^4]: Reid (1971) On the method of conjugate gradients for the solution of large\n",
    "\tsparse systems of linear equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation/Settings\n",
    "\n",
    "| expression | meaning |\n",
    "|---|---|\n",
    "| $x$ | true solution ($Ax=b$) |\n",
    "| $x_k$ | $k$-th approximate solution by the conjugate gradient method ($k=0,1,2,\\cdots$)|\n",
    "| $e_k$ | $=x - x_k$ error caused by $x_k$ |\n",
    "| $r_k$ | $=b-Ax_k$ residual caused by $x_k$ |\n",
    "| $d_k$ | conjugate directions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea behind the conjugate gradient method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Warm up (Gaussian elimination)\n",
    "\n",
    "1. Change the perspective\n",
    "\n",
    "View the process of finding the solution as removing components of error one by one. Unless we are extremely lucky, our error caused by the initial guess will be full of possible components. \n",
    "\n",
    "For the moment, to avoid introducing too many symbols, let us override the notations and let $d_k$'s be canonical basis of $\\mathbb{R}^{n}$. \n",
    "\n",
    "We can expand the initial error in the canonical basis.\n",
    "\n",
    "$$\n",
    "e_{0}=\\sum_{k=0}^{n-1} \\eta_k b_k\n",
    "$$\n",
    "\n",
    "2. Remove component one by one\n",
    "\n",
    "Then the back substitution step can be seen as removing $d_{n-2}$ component from the error, then $d_{n-3}$ component, all the way to $d_{0}$. ($d_{n-1}$ component is already absent since the last component of $x$ is precise.)\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1&-\\frac{1}{2}&\\frac 3 4 & \\frac 7 4 \\\\\n",
    "0&1& \\frac 3 5 & \\frac {26} 5\\\\\n",
    "0&0&1& \\underbrace{2}_{x_1} \n",
    "\\end{bmatrix}\n",
    "\\longrightarrow\t\n",
    "\\begin{bmatrix}\n",
    "1&-\\frac{1}{2}&0 & -1 \\\\\n",
    "0&1& 0& 4\\\\\n",
    "0&0&1& \\underbrace{2}_{x_2}  \n",
    "\\end{bmatrix}\n",
    "\\longrightarrow\t\n",
    "\\begin{bmatrix}\n",
    "1&0&0 & 1 \\\\\n",
    "0&1& 0& 4\\\\\n",
    "0&0&1& \\underbrace{2}_{x_3}  \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "![Gaussian elimination from error improvement point of view](../images/CG01.png)\n",
    "\n",
    "**Remark**\n",
    "\n",
    "3. Algorithm-friendly summary\n",
    "\n",
    "- Removing $d_k$ from the approximate solution $x_k$ and removing it from the error $e_k$ is equivalent because they differ only by a fixed vector, the true solution $x$, that is, $e_k = x - x_k$. \n",
    "\n",
    "Given $x_k$, take exact step to remove $d_k$ component each time\n",
    "\n",
    "$$\n",
    "x_{k+1}=x_k \\blue{- \\eta_k} d_k,\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "e_{k+1}=e_k \\blue{- \\eta_k} d_k\n",
    "$$\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrch",
   "language": "python",
   "name": "ptrch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
