{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjugate gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem of interest**\n",
    "\n",
    "Given an $n$-by-$n$ symmetric positive definite (SPD) matrix $A$ and a $\\mathbb{R}^n$-vector $b$, find $\\mathbb{R}^n$-vector $x$ such that\n",
    "\n",
    "$$ Ax = b. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method: Conjugate gradient method\n",
    "\n",
    "**Data**\n",
    "\n",
    "- $x_0 \\in \\mathbb{R}^n$: initial guess\n",
    "\n",
    "**Initialize**\n",
    "\n",
    "- $d_0=r_0=b-Ax_0$\n",
    "\n",
    "**Main computation**\n",
    "\n",
    "- **for** k = 0, 1, 2, ..., $n$ - 1\n",
    "    - **if** $r_k = 0$ **stop**, **end**\n",
    "    - $\\alpha_{k}=\\frac{r_{k}^{T} r_{k}}{d_{k}^{T} A d_{k}}$ (compute $d_k$ component of error)\n",
    "    - $x_{k+1}=x_{k}+\\alpha_{k} d_{k}$ (subtract it out)\n",
    "    - $r_{k+1}=r_{k}-\\alpha_{k} A d_{k}$ (compute the new residual)\n",
    "    - $\\beta_{k}=\\frac{r_{k+1}^{T} r_{k+1}}{r_{k}^{T} r_{k}}$ (compute $d_k$ component of the residual)\n",
    "    - $d_{k+1}=r_{k+1}+\\beta_{k} d_{k}$ (conduct Gram-Schmidt with respect to $A$-inner product)\n",
    "**end**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**History**\n",
    "\n",
    "1. First proposed by Schmidt (1908) [^1] (*the* Schmidt in Gram-Schmidt)\n",
    "1. Independently re-invented by Fox, Huskey, and Wilkinson (1948) [^2]\n",
    "1. Hestenes and Stiefel (1952) made this idea explicit and practical. [^3]\n",
    "1. CG does not reach the solution in $n$ steps in practice due to round off errors. It became popular only after Reid (1971) showed its value as an iterative method for large, sparse matrices. [^4] \n",
    "\n",
    "[^1]: Schmidt (1908) Uber die Auflosung linearer Gleichungen mit Unendlich vielen unbekannten (accent removed)\n",
    "\n",
    "[^2]: Fox, Huskey, and Wilkinson (1948) Notes on the solution of algebraic linear simultaneous equations\n",
    "\n",
    "[^3]: Hestenes and Stiefel (1952) Methods of conjugate gradients for solving linear systems \n",
    "\n",
    "[^4]: Reid (1971) On the method of conjugate gradients for the solution of large\n",
    "\tsparse systems of linear equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def CG(A, b, x0, tol=1e-9, max_iter=None):\n",
    "    \"\"\"\n",
    "    Conjugate Gradient method for solving linear systems of equations.\n",
    "    \n",
    "    Parameters:\n",
    "        A (ndarray): The coefficient matrix of the linear system.\n",
    "        b (ndarray): The right-hand side vector of the linear system.\n",
    "        x0 (ndarray): The initial guess for the solution.\n",
    "        max_iter (int): The maximum number of iterations (default: 100).\n",
    "        tol (float): The tolerance for convergence (default: 1e-9).\n",
    "    \n",
    "    Returns:\n",
    "        x (ndarray): The approximate solution to the linear system.\n",
    "        num_iter (int): The number of iterations performed.\n",
    "    \"\"\"\n",
    "    # default number of iterations is the dimension of the matrix\n",
    "    if max_iter is None:\n",
    "        n = A.shape[0]\n",
    "        max_iter = n\n",
    "\n",
    "    # initialize\n",
    "    x = x0\n",
    "    r = b - A @ x\n",
    "    d = r\n",
    "\n",
    "    r_nrm2 = np.dot(r, r)\n",
    "\n",
    "    for i in range(max_iter + 1): \n",
    "        # stopping criterion\n",
    "        if np.sqrt(r_nrm2) < tol:\n",
    "            break\n",
    "\n",
    "        # intermediate computations part 1 \n",
    "        Ad = A @ d\n",
    "\n",
    "        # main conjugate gradient iteration part 1\n",
    "        alpha = r_nrm2 / np.dot(d, Ad)\n",
    "        x = x + alpha * d\n",
    "        r_new = r - alpha * Ad\n",
    "        \n",
    "        # intermediate computations part 2\n",
    "        r_new_nrm2 = np.dot(r_new, r_new)\n",
    "        \n",
    "        # main conjugate gradient iteration part 2\n",
    "        beta = r_new_nrm2 / r_nrm2\n",
    "        d = r_new + beta * d\n",
    "        \n",
    "        # updata\n",
    "        r = r_new\n",
    "        r_nrm2 = r_new_nrm2\n",
    "    \n",
    "    return x, i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (CG) =  [-3.75 -2.5  -7.25] in 3 iterations\n",
      "x_true =  [-3.75 -2.5  -7.25]\n",
      "error =  8.042797661766892e-15\n"
     ]
    }
   ],
   "source": [
    "# A is of full rank\n",
    "A = np.array([  [1, 2 , -1],\n",
    "                [2, 1 , -2],\n",
    "                [-3, 1,  1]])\n",
    "\n",
    "# take a SPD matrix\n",
    "A = A.T @ A\n",
    "\n",
    "b = np.array([3, 3, -6])\n",
    "\n",
    "x0 = np.array([0, 0, 0])\n",
    "\n",
    "x, iter = CG(A, b, x0)\n",
    "x_true = np.linalg.solve(A, b)\n",
    "\n",
    "print(\"x (CG) = \", x, \"in\", iter, \"iterations\")\n",
    "print(\"x_true = \", x_true)\n",
    "print(\"error = \", np.linalg.norm(x - x_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notation/Settings\n",
    "\n",
    "| expression | meaning |\n",
    "|---|---|\n",
    "| $x$ | true solution ($Ax=b$) |\n",
    "| $x_k$ | $k$-th approximate solution by the conjugate gradient method ($k=0,1,2,\\cdots$)|\n",
    "| $e_k$ | $=x - x_k$ (error caused by $x_k$) |\n",
    "| $r_k$ | $=b-Ax_k$ (residual caused by $x_k$) |\n",
    "| $d_k$ | conjugate directions |\n",
    "| $(u,v)$ | $=v^T u$ (standard inner product) |\n",
    "| $(u,v)_A$ | $=v^T A u$ (A-inner product, where $A$ is a SPD matrix) |\n",
    "| $v \\perp_A u$ | $(u,v)_A =0$ (A-orthogonality) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea behind the conjugate gradient method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "A minimization approach to conjugate gradient method is also helpful, where we use the equivalence\n",
    "\n",
    "$$\n",
    "Ax = b \\quad \\leftrightarrow \\quad \\mathrm{minimize} ~ \\frac{1}{2} x^T A x - b^T x, \\quad \\text{subject to} ~ x \\in \\mathbb{R}^n.\n",
    "$$\n",
    "\n",
    "However, this involves more discussions from different angles. Between the trade-off of richness and simplicity in presentation, these notes have chosen the simplicity, and do not discuss minimization approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Warm up (Gaussian elimination)\n",
    "\n",
    "1. Change the perspective\n",
    "\n",
    "View the process of finding the solution as removing components of error one by one. Unless we are extremely lucky, our error caused by the initial guess will be full of possible components. \n",
    "\n",
    "For the moment, to avoid introducing too many symbols, let us override the notations and let $d_k$'s be canonical basis of $\\mathbb{R}^{n}$. \n",
    "\n",
    "We can expand the initial error in the canonical basis.\n",
    "\n",
    "$$\n",
    "e_{0}=\\sum_{k=0}^{n-1} \\eta_k d_k\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Remove component one by one\n",
    "\n",
    "Then the back substitution step can be seen as removing $d_{n-2}$ component from the error, then $d_{n-3}$ component, all the way to $d_{0}$. ($d_{n-1}$ component is already absent since the last component of $x$ is precise.)\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1&-\\frac{1}{2}&\\frac 3 4 & \\frac 7 4 \\\\\n",
    "0&1& \\frac 3 5 & \\frac {26} 5\\\\\n",
    "0&0&1& \\underbrace{2}_{x_1} \n",
    "\\end{bmatrix}\n",
    "\\longrightarrow\t\n",
    "\\begin{bmatrix}\n",
    "1&-\\frac{1}{2}&0 & -1 \\\\\n",
    "0&1& 0& 4\\\\\n",
    "0&0&1& \\underbrace{2}_{x_2}  \n",
    "\\end{bmatrix}\n",
    "\\longrightarrow\t\n",
    "\\begin{bmatrix}\n",
    "1&0&0 & 1 \\\\\n",
    "0&1& 0& 4\\\\\n",
    "0&0&1& \\underbrace{2}_{x_3}  \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "![Gaussian elimination from error improvement point of view](../images/CG01.png)\n",
    "\n",
    "**Remark**\n",
    "\n",
    "\n",
    "- Removing $d_k$ from the approximate solution $x_k$ and removing it from the error $e_k$ are equivalent because they differ only by a fixed vector, the true solution $x$, that is, $e_k = x - x_k$. But the signs of $e_k$ and $x_k$ are opposite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Algorithm-friendly summary\n",
    "\n",
    "\n",
    "Given $x_k$, hence $e_k = x - x_k$, take exact step to remove $d_k$ component each time\n",
    "\n",
    "$$\n",
    "e_{k+1}=e_k - \\eta_k d_k.\n",
    "$$\n",
    "\n",
    "or equivalently, \n",
    "\n",
    "$$\n",
    "x_{k+1}=x_k + \\eta_k d_k,\n",
    "$$\n",
    "\n",
    "The exact component $\\eta_k$ in $d_k$ direction can be computed by the condition $e_{k+1} \\perp d_k$ since we just subtracted out $d_k$:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "e_{1}&=\\sum_{j=1}^{n-1} \\eta_k d_j\n",
    "\\\\\n",
    "e_{2}&=\\sum_{j=2}^{n-1} \\eta_k d_j\n",
    "\\\\\n",
    "&\\vdots\n",
    "\\\\\n",
    "e_{k+1}&=\\sum_{j=k+1}^{n-1} \\eta_k d_j.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "By taking dot product with $d_k$ on both sides of the error update rule $e_{k+1}=e_k - \\eta_k d_k$, we can find\n",
    "\n",
    "$$\n",
    "\\eta_k = \\frac{d_k^T e_{k}}{d_k^T d_k}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "- $e_k$ is not computable since we do not know the true solution $x$.\n",
    "- The residual $r_k:=b - Ax_k$ is computable. \n",
    "- Since $r_k = b - A x_k = A x - A x_k = A(x - x_k) = Ae_k$, the residual and the error are related:\n",
    "\n",
    "$$\n",
    "r_k = Ae_k\n",
    "$$\n",
    "\n",
    "- Though not at all trivial, these observations suggest a possibility to play between A-orthogonality and orthogonality for our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Devising conjugate gradient method: Part 1 (main idea)\n",
    "\n",
    "1. Suppose we have an A-orthogonal basis $\\{d_k\\}_{k=0}^{n-1}$ for $\\mathbb{R}^n$. (We will discuss how to obtain it. For now, assume it is possible.) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. View finding the solution as removing $d_k$ component at a time. We can expand the initial error in this basis. \n",
    "\n",
    "$$\n",
    "e_{0}=\\sum_{k=0}^{n-1} \\alpha_k d_k \\quad \\text{(error update)}\n",
    "$$\n",
    "\n",
    "It is likely that the initial guess contains all components of $d$'s.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Suppose we can remove $d_k$ component from the previous error each time (this can be achieved by finding $\\alpha_k$ momentarily):\n",
    "\n",
    "$$\n",
    "e_{k+1}=e_k - \\alpha_k d_k.\n",
    "$$\n",
    "\n",
    "Equivalently, we can write this in terms of $x_k$'s\n",
    "\n",
    "$$\n",
    "x_{k+1}=x_k + \\alpha_k d_k.\n",
    "$$\n",
    "\n",
    "Then, we are left with less and less $d_k$ components in the error:\n",
    "\n",
    "$$\n",
    "e_{k+1}=\\sum_{j=k+1}^{n-1} \\alpha_k d_j\n",
    "$$\n",
    "\n",
    "since\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "e_{1}&=\\sum_{j=1}^{n-1} \\alpha_k d_j\n",
    "\\\\\n",
    "e_{2}&=\\sum_{j=2}^{n-1} \\alpha_k d_j\n",
    "\\\\\n",
    "&\\vdots\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Thus, since $e_{k+1}$ has no $d_k$ component (it has just been removed), we must have $e_{k+1} \\perp_{A} d_k$. Use this fact to the error update rule, then we can compute $\\alpha_k$:\n",
    "\n",
    "$$\n",
    "\\alpha_k = \\frac{d_k^T A e_k}{d_k^T A d_k}=\\frac{d_k^T r_k}{d_k^T A d_k} \\quad \\text{(computable)}.\n",
    "$$\n",
    "\n",
    "**Remark**\n",
    "\n",
    "- This is where $A$-orthogonality is used in a nice way.\n",
    "- In the algorithm, we use $\\alpha_k$ formula to update $x_k$'s since $e_k$ is not computable during implementation.\n",
    "- Thus, in theory, the conjugate gradient will eventually exhaust all directions $d_k$'s at most $n$ steps and give the solution. From this point of view, it can be seen as a direct method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Devising conjugate gradient method: Part 2(conjugate directions)\n",
    "\n",
    "**Question**: How to find the $A$-orthogonal basis $\\{d_k\\}$?\n",
    "\n",
    "**Idea**: Gram-Schmidt process with respect to $A$-inner product. We will construct $d_k$'s dynamically as the algorithm proceeds.\n",
    "\n",
    "**Observation 1**\n",
    "\n",
    "Computation dependency (incomplete):\n",
    "\n",
    "$$\n",
    "x_k, d_k, r_k \\longrightarrow \\alpha_k \\longrightarrow x_{k+1}, r_{k+1} \\longrightarrow  d_{k+1} ~ (?)\n",
    "$$\n",
    "\n",
    "**Observation 2-(a)**\n",
    "\n",
    "New residual $r_{k+1}$ guarantees a new direction outside of $\\mathrm{span}\\{d_0, d_1, \\cdots, d_k \\}$. In fact, if everything works out as we planned, it is orthogonal to all the past directions:\n",
    "\n",
    "$$ \n",
    "r_{k+1} \\perp \\mathrm{span} \\{ d_j \\ : \\  0 \\le j \\le k \\}\n",
    "$$\n",
    "\n",
    "since, by mutual A-orthogonality between $d_k$'s, \n",
    "\n",
    "$$\n",
    "d_j^T r_{k+1} = d_j^T A e_{k+1} = \\sum_{i=k+1}^{n-1} \\eta_k d_j^T A  d_i = 0.\n",
    "$$\n",
    "\n",
    "**Observation 2-(b)**\n",
    "\n",
    "![Gram-Schmidt](https://upload.wikimedia.org/wikipedia/commons/7/7b/Linalg_orth_proj_R3.png)\n",
    "\n",
    "We can extract $d_{k+1}$ from $r_{k+1}$ by filtering out $\\{d_0, d_1, \\cdots, d_k \\}$: Gram-Schmidt.\n",
    "\n",
    "$$\n",
    "d_{k+1} = r_{k+1} +\\sum_{j=0}^{k}\\beta_{j} d_j,\n",
    "$$\n",
    "\n",
    "where by taking $A$-inner product, we know, for $j=0,1,\\cdots,k$\n",
    "\n",
    "$$\n",
    "\\beta_j = - \\frac{d_j^T A r_{k+1}}{d_j^T A d_j}.\n",
    "$$\n",
    "\n",
    "**Remark**\n",
    "\n",
    "- This turns out not to be practical. We need to store $d_j$'s, but this runs out of memory quickly for large problems. However, surprisingly, we only need to store the last one. (See below)\n",
    "\n",
    "**Observation 3**\n",
    "\n",
    "A close look reveals $\\beta_j=0$ for $j=0,1,\\cdots,k-1$, meaning, we need to store only $d_k$, and the Gram-Schmidt process to extract $d_{k+1}$ reduces to \n",
    "\n",
    "$$\n",
    "d_{k+1} = r_{k+1} +\\beta_{k} d_k,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\beta_k = - \\frac{r_{k+1}^T A d_k^T}{d_{k}^T A d_k^T}.\n",
    "$$\n",
    "\n",
    "**Remark**\n",
    "\n",
    "- This observation involves a fair amount of details. Hence, we skip it for now in favor of a better bigger picture.\n",
    "\n",
    "**Complete computation dependency**\n",
    "\n",
    "Now that we know how to compute $d_{k+1}$, we have completed a working loop:\n",
    "\n",
    "$$\n",
    "x_k, d_k, r_k \\longrightarrow \\alpha_k \\longrightarrow x_{k+1}, r_{k+1} \\longrightarrow \\beta_k \\longrightarrow d_{k+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some computational optimizations\n",
    "\n",
    "The formulas for $\\alpha_k$, $\\beta_k$, and $r_{k+1}$ from the discussion of \"devising conjugate gradient method\" convey the idea behind it. However, there are equivalent expressions that are computationally more efficient.\n",
    "\n",
    "| formula from idea | formula for computation | Advantage |\n",
    "|---|---|---|\n",
    "| $$\\alpha_k=\\frac{d_k^T r_k}{d_k^T A d_k}$$ | $$\\alpha_k=\\frac{r_k^T r_k}{d_k^T A d_k}$$ | recycle $r_k^T r_k$ (this is the same as $r_{k+1}^T r_{k+1}$ from the previous iteration) |\n",
    "| $$x_{k+1}=x_k+\\alpha_k d_k$$ | Same |\n",
    "| $$r_{k+1}=b - Ax_{k+1}$$ | $$r_{k+1}=r_k-\\alpha_k A d_k$$ | avoid $Ax_{k+1}$ and recycle $A d_k$ |\n",
    "| $$\\beta_k = - \\frac{r_{k+1}^T A d_k^T}{d_{k}^T A d_k^T}$$| $$\\beta_k=\\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}$$ | recycle $r_k^T r_k$ and avoid $r_{k+1}^T (A d_k^T)$ |\n",
    "| $$d_{k+1}=r_{k+1}+\\beta_k d_k$$ | Same |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrch",
   "language": "python",
   "name": "ptrch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
