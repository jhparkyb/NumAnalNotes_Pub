{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take-aways\n",
    "\n",
    "After studying this chapter, we will be able to\n",
    "\n",
    "- say what is the main problem of interest,\n",
    "- explain some standard root finding methods, \n",
    "  - write the methods (pseudo-algorithm): bisection, Newton's method, secant method, and fixed point iteration\n",
    "  - explain their mathematical and computational pros and cons, \n",
    "- explain why they work or related facts at an intuitive level,\n",
    "  - intuition behind the four methods,\n",
    "- give theoretical arguments about important facts,\n",
    "  - derivation of Newton's method,\n",
    "  - contraction mapping theorem,\n",
    "  - convergence of fixed point iteration,\n",
    "- give precise results on the four methods and related facts with the help of reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "\n",
    "##### Problem of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Problem of interest***\n",
    ">\n",
    "> Given a function $f:\\mathbb{R} \\to \\mathbb{R}$, find $\\xi\\in\\mathbb{R}$ such that\n",
    "> $$f(\\xi)=0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Methods\n",
    "\n",
    "1. Bisection method\n",
    "1. Newton's method\n",
    "1. Secant method\n",
    "1. Fixed point iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bisection method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method\n",
    "\n",
    "![Bisection illustration](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Bisection_method.svg/1024px-Bisection_method.svg.png)\n",
    "\n",
    "Figure: Wikipedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Algorithm** (Bisection method)\n",
    "> \n",
    "> - Get an initial interval $[a, b]$ with a sign-change: $f(a) f(b) < 0$.\n",
    "> \n",
    "> - Choose $N$, maximum number of iterations.\n",
    "> \n",
    "> - for i from 1 to N:\n",
    "> <br>$\\quad$ $\\displaystyle c \\leftarrow \\frac{a + b}{2}$\n",
    "> <br>$\\quad$ if $f(a) f(c) \\le 0$ then:\n",
    "> <br>$\\quad$ $\\quad$ $b \\leftarrow c$\n",
    "> <br>$\\quad$ else:\n",
    "> <br>$\\quad$ $\\quad$ $a \\leftarrow c$\n",
    "> <br>$\\quad$ end if\n",
    "> <br>end for\n",
    "> \n",
    "> - The approximate root is the final value of $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "- If convergent, the bisection method converges to the solution linearly.\n",
    "> **Definition** (Linear convergence) \n",
    "> \n",
    "> Let $\\{x_n\\}_{n\\in\\mathbb{N}_0}$ be a sequence that converges to $\\xi$. We say that it converges *linearly* if there exists $\\lambda\\in(0,1)$ such that\n",
    "> $$ e_{n+1} = \\lambda e_n, $$\n",
    "> where $e_n:=|x_n - \\xi|$ for $n=0, 1, 2,\\cdots$. In words, it means *errors get shrunken by a factor of a fraction*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Remark** (flexibility in definitions of asymptotic behaviors)\n",
    ">\n",
    "> The above definition is too strong (i.e., hard to satisfy) to be useful. \n",
    "> \n",
    "> 1. For example, if the following is the case,\n",
    "> \n",
    "> $$\n",
    "> e_n= 1, 1/2, \\mathbf{1/3}, 1/4, 1/8, 1/16, \\cdots,\n",
    "> $$\n",
    "> \n",
    "> then a single number $1/3$ mess up the definition. But it is more reasonable that we still consider this error decays linearly.\n",
    "> Therefore, we usually require the condition $e_{n+1}=\\lambda e_n$ except possibly finite number of exceptions: \"there exists $N\\in \\mathbb{N}$ such that, for all $n\\ge N$, we have $e_{n+1}=\\lambda e_n$.\"\n",
    "> \n",
    "> 2. Also, if \n",
    "> \n",
    "> $$\n",
    "> e_n= 1, 1, 1/2, 1/3, 1/4, 1/9, 1/8, 1/27, \\cdots,\n",
    "> $$\n",
    "> \n",
    "> the behaviors are similar to the linear convergence, but not quite: the pattern differs depending on odd- or even-numbered terms. Thus, in a more rigorous context, where word-by-word translations are fundamental to communications between people from broad backgrounds, we usually prefer to use inequalities: \"there exists $N\\in \\mathbb{N}$ such that, for all $n\\ge N$, we have $e_{n+2}\\le \\lambda e_n$.\" In the current example, if we set $\\lambda=1/2$ the statement is true.\n",
    "> \n",
    "> However, in this course, we will not pursue that level of rigor for convergence rates. We will use a bit flexible description unless there is an appropriate reason.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis\n",
    "\n",
    "![Convergence of bisection](https://jhparkyb.github.io/resources/notes/na/104ASlides_RootFinding011.png)\n",
    "\n",
    "Proof: See Kincaid and Cheney (2002) p. 79."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method\n",
    "\n",
    "**Terminology**\n",
    "\n",
    "It is also called *Newton-Raphson* method.\n",
    "\n",
    "\n",
    "**Geometric intuition**\n",
    "\n",
    "[Newton's method: Geogebra interactive module](https://www.geogebra.org/m/n6KXp4hE)\n",
    "\n",
    "Creator: Lenore Horner\n",
    "\n",
    "[Newton's method: Still illustration](https://math24.net/images/newtons-method1.svg)\n",
    "\n",
    "Figure: https://math24.net/newtons-method.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Algorithm** (Newton's method)\n",
    ">\n",
    "> Given a differentiable function $f:\\mathbb{R}\\to\\mathbb{R}$ and an initial guess $x_0\\in\\mathbb{R}$, compute, for $n\\ge 0$,\n",
    ">\n",
    "> $$ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivation of Newton's method using tangent line intuition**\n",
    "\n",
    "See Board work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Remark***\n",
    ">\n",
    "> There are different styles of algorithm or pseudo-algorithm.\n",
    ">\n",
    "| Mathematics- or idea-oriented pseudo-algorithm | Coding-oriented pseudo-algorithm |\n",
    "|---|---|\n",
    "|Given an initial guess $x_0$, <br> compute <br> $ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$ for $n\\ge 0$. | Input (or Data): $x_0$, $f$, $f'$ <br> Set: $Tol>0$, $x \\gets x_0$ <br> While $\\|x - x_{pre}\\| > Tol$: <br> $ \\quad \\quad x_{pre} \\gets x $ <br> $\\quad \\quad x \\gets x - \\frac{f(x)}{f'(x)}$ |\n",
    "| Focus on the essence | Also consider some details in implementation. In particular, this usually includes *stopping criteria*. |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "> **Theorem** (Local, quadratic convergence of Newton's method)\n",
    "> \n",
    "> Let $f$ be twice continuously differentiable and $f(\\xi)=0$. If $f'(\\xi) \\neq 0$, then Newton's method is locally and quadratically convergent to $x$. That is, the method converges to the zero $\\xi$ if the initial guess $x_0$ is sufficiently close to $\\xi$. \n",
    "\n",
    "> **Definition** (Quadradic convergence) \n",
    "> \n",
    "> Let $\\{x_n\\}_{n\\in\\mathbb{N}}$ be a sequence that converges to $\\xi$. We say that it converges quadratically fast if there exists $C>0$ such that\n",
    "> $$ \\lim_{n\\to\\infty} \\frac{e_{n+1}}{e_n^2} = C, $$\n",
    "> where $e_n:=|x_n - \\xi|$ for $n=0, 1, 2,\\cdots$. In words, it means *errors get shrunken by a square of the previous error*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question** (How fast is a quadratic convergence?)\n",
    ">\n",
    "> Suppose Newton's method starts to manifest quadratic converge from 5th iteration with $e_5 = 0.01$. What will be the error after four more iterations? For simplicity, assume $C=1$.\n",
    ">\n",
    "> (Reminder) This is **about atmosphere**, not getting it right.\n",
    "> \n",
    "> 1. Think for a short time.\n",
    "> 2. Share your guess with your pair.\n",
    "> 3. Type your answer in clicker.\n",
    "> 4. Feel free to say out loud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis\n",
    "\n",
    "**Derivation of Newton's method using Taylor theorem**\n",
    "\n",
    "![Derivation of Newton's method](https://jhparkyb.github.io/resources/notes/na/104ASlides_RootFinding014.png)\n",
    "\n",
    "[Derivation of Newton's method](https://jhparkyb.github.io/resources/notes/na/der_NewtonMethodTaylor_lp2000.png)\n",
    "\n",
    "\n",
    "\n",
    "In favor of more computational activities, we skip the proof the quadratic convergence of Newton's method. But since Newton's method is highly relevant even these days and since it inspires many other methods, we include some history about it.\n",
    "\n",
    "> **Historical note**\n",
    ">\n",
    "> 1. Babylonians (1894 BC - 539 BC) used the method to approximate square roots: $\\sqrt{2}$ accurately up to seven places. (Ref: [2, 3])\n",
    ">       ![Babylonian clay tablet](https://projectlovelace.net/static_prod/img/YBC7289.jpg)\n",
    ">\n",
    ">       Figure: Project Lovelace\n",
    "> 1. In 1669, the method was employed by Newton for the cubic equation $3x^3 -2x-5 = 0$. (Ref: [1])\n",
    "> 1. In 1690, Raphson described the method for a general cubic equation $x^3 â€” bx = c$. (Ref: [1])\n",
    "> 1. In 1818, Fourier proved the quadratic convergence of the method. (Ref: [1])\n",
    "> 1. In 1829, Cauchy proved a convergence theorem which does not assume the existence of a solution. (existence of a solution is a consequence; but it assumes some other conditions on the iterates) (Ref: [1])\n",
    "> 1. In 1939, Kantorovich proved a convergence theorem in a very general setting. (Ref: [1])\n",
    "> 1. In 1948, Kantorovich proved an improved version, which is now called Kantorovich's theorem or the Newton-Kantorovich theorem: existence of a solution is not assumed and the convergence is quadratic in a very general setting. (Ref: [1])\n",
    "> \n",
    "> Reference\n",
    "> \n",
    "> [1] Brezinski (2001) Numerical Analysis: Historical Developments in the 20th Century. p. 242\n",
    "> \n",
    "> [2] Sauer (2017) Numerical Analysis p. 41\n",
    "> \n",
    "> [3] Wikipedia (Babylonia) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Warning (newton): maximum number of iteration reached.\n",
      "     --> The output may not be close enough to the zero.\n",
      "Newton's method :  1.4142135623730954    (7 iterations taken)\n",
      "True solution   :  1.4142135623730951\n",
      "Error           :  2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def newton(f, fp, ini, tol=1e-8, max_iter=20):\n",
    "    \"\"\"\n",
    "    Return an approximate root of a function using Newton's method.\n",
    "\n",
    "    INPUT\n",
    "        f: function whose zero is sought.\n",
    "        fp: derivative of f (name from 'f prime')\n",
    "        ini: initial guess\n",
    "        tol: tolerance for stopping criterion. If consecutive iterates differ by less than this, it is considered convergenct.\n",
    "        max_iter: maximum number of iterations\n",
    "    OUTPU\n",
    "        approximated zero and the number of iterations. When the maximum number of iterations is reached, the last iterate with a warning message.\n",
    "    \"\"\"\n",
    "    x = ini\n",
    "    for i in range(max_iter):\n",
    "        x_pre = x\n",
    "        x = x - f(x)/fp(x)\n",
    "\n",
    "        if np.abs(x - x_pre) < tol: \n",
    "            break\n",
    "    \n",
    "    if i == max_iter - 1:\n",
    "        print(\"   Warning (newton): maximum number of iteration reached.\\n     --> The output may not be close enough to the zero.\")\n",
    "    return x, i + 1\n",
    "\n",
    "# find the square root\n",
    "f = lambda x: x*x - 2.\n",
    "fp = lambda x: 2*x\n",
    "\n",
    "x0 = 10.\n",
    "\n",
    "appr, iter = newton(f, fp, x0, max_iter=7)\n",
    "sol = np.sqrt(2.)\n",
    "\n",
    "print(\"Newton's method : \", appr, f\"   ({iter} iterations taken)\")\n",
    "print(\"True solution   : \", sol)\n",
    "print(\"Error           : \", appr - sol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secant method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method\n",
    "\n",
    "**Idea**: Replace $f'(x_n)$ with something similar in Newton's method.\n",
    "\n",
    "![Secant method](https://mathworld.wolfram.com/images/eps-svg/SecantMethod_800.svg)\n",
    "\n",
    "Figure: Wolfram MathWorld.\n",
    "\n",
    "> ***Algorithm*** (Secant method)\n",
    ">\n",
    "> Given $x_0, x_1\\in\\mathbb{R}$, compute, for $n\\ge 1$,\n",
    ">\n",
    "> $$ x_{n+1}=x_{n}-f\\left(x_{n}\\right)\\frac{\\left(x_{n}-x_{n-1}\\right)}{f\\left(x_{n}\\right)-f\\left(x_{n-1}\\right)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "- If the secant method converges, its rate of convergence is the *golden ratio* ($\\approx 1.618$).\n",
    "- User must feed **two initial guesses**.\n",
    "- It requires **only the function evaluation**, but not the derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis\n",
    "\n",
    "In favor of more computational activities, we skip the proof the *superlinear* convergence (i.e., a convergence rate that is faster the linear: $e_{k+1} \\approx C e_k^\\alpha$ with $\\alpha>1$) of the secant method.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed point iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method\n",
    "\n",
    "**Terminology**\n",
    "\n",
    "It is also called *Picard iteration* or *functional iteration*.\n",
    "\n",
    "**Geometric interpretation**\n",
    "\n",
    "\"A picture paints a thousand words.\" \n",
    "\n",
    "![Fixed point iteration](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Cosine_fixed_point.svg/1920px-Cosine_fixed_point.svg.png)\n",
    "\n",
    "Figure: Wikipedia\n",
    "\n",
    "[Fixed point iteration](https://www.geogebra.org/m/qUbg7Z6W) (Geogebra construction due to stuart.cork)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Algorithm** (Fixed point iteration - general)\n",
    ">\n",
    "> Given a function $f:\\mathbb{R}\\to\\mathbb{R}$ and an initial guess $x_0\\in\\mathbb{R}$, compute, for $n\\ge 0$,\n",
    ">\n",
    "> $$ x_{n+1} = f(x_n). $$\n",
    "\n",
    "> **Algorithm** (Fixed point iteration - root finding for $f$)\n",
    ">\n",
    "> Given a function $f:\\mathbb{R}\\to\\mathbb{R}$ and an initial guess $x_0\\in\\mathbb{R}$, set $g(x)=x+f(x)$, compute, for $n\\ge 0$, \n",
    ">\n",
    "> $$ x_{n+1} = g(x_n). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "- If repeated applications of a function $g$ converges to $\\xi$, then it solves $x=g(x)$. (Some condition on $f$ is needed: see Analysis below.)\n",
    "- If converges, the fixed point iteration method converges *linearly*: there exists $C>0$ such that $e_{k+1}\\approx \\lambda e_k$ with $0<\\lambda<1$. \n",
    "- If you want to solve the equation $f(x)=0$, set $g(x):=x+f(x)$ and apply the fixed point iteration to $g$. Then, the fixed point $\\xi$ satisfies \n",
    "    $$\\xi = g(\\xi)=\\xi+f(\\xi) \\quad \\text{implies} \\quad f(\\xi)=0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition** (Fixed point)\n",
    "> $x$ is called a *fixed point* of the function $g$ if $g(x)=x$.\n",
    "\n",
    "\n",
    "> **Definition** (Contractive/Contraction mapping)\n",
    "> A function $g:D \\to \\mathbb{R}$ is called *contractive* or a *contractive mapping/contraction* if there is $\\lambda\\in[0,1)$ such that $|g(x)-g(y)|\\le \\lambda|x-y|$ for all $x,y\\in D$.\n",
    "\n",
    "> **Theorem** (Contraction mapping is continuous)\n",
    "> If $g:D\\to \\mathbb{R}$ is contractive, it is continuous.\n",
    "\n",
    "> **Theorem** (Absolute convergence implies convergence)\n",
    "> If $\\sum_{n=1}^\\infty x_n$ is absolutely convergent, i.e., $\\sum_{n=1}^\\infty |x_n| < \\infty$, then $\\sum_{n=1}^\\infty x_n$ also converges.\n",
    "\n",
    "> **Theorem** (Contraction Mapping Theorem)\n",
    "> Let $D$ be a closed subset of $\\mathbb{R}$. If $g:D \\to D$ is a contraction, then it has a unique fixed point. Moreover, this fixed point is the limit of the functional iteration starting with any initial guess.\n",
    "\n",
    "[Proof of contraction mapping theorem 1](https://jhparkyb.github.io/resources/notes/na/pf_ContractionMappingThm1_lp3000.png)\n",
    "\n",
    "[Proof of contraction mapping theorem 2](https://jhparkyb.github.io/resources/notes/na/pf_ContractionMappingThm2_lp3001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof outline\n",
    "\n",
    "1. $x_n= (x_n - x_{n-1}) + (x_{n-1} - x_{n-2}) + \\cdots + (x_{1} - x_{0}) + x_0$ absolutely converges, hence converges.\n",
    "   - $|x_n - x_{n-1}| \\le \\lambda^{n-1} |x_1 - x_0|$\n",
    "2. Pass $x_{n+1}=g(x_n)$ to the limit $n\\to \\infty$.\n",
    "3. Uniqueness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Question** \n",
    ">\n",
    "> The above proof outline did not use one condition and used another condition implicitly. What are they? \n",
    ">\n",
    "> (Reminder) This is **about atmosphere**, not getting it right.\n",
    "> \n",
    "> 1. Think for a short time.\n",
    "> 2. Share your guess with your pair.\n",
    "> 3. Type your answer in clicker.\n",
    "> 4. Feel free to say out loud.\n",
    "\n",
    "(Homework questions will ask you what happens if you ignore them.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparisons of root-finding methods\n",
    "\n",
    "| | Bisection | Newton | Secant | Fixed point |\n",
    "|---|---|---|---|---|\n",
    "| need $f(x)$ | O | O | O | O |\n",
    "| need $f'(x)$ | - | O | - | - |\n",
    "| rate of convergence | 1 | 2 | 1.618 | 1 |\n",
    "| rate of convergence <br> per two function eval's | 1 <br> (with smaller contraction constant) | 2 | $1.618^2\\approx 2.618$ | 1 <br> (with smaller contraction constant) |\n",
    "| global convergence | yes <br> if $f(a)f(b)<0$ | no | no | practially no |\n",
    "| solution boxed | yes | no | no | generally, no |  \n",
    "| generalization <br> to high dimensions <br> (intellectual effort) | awkward | yes, <br> but gradient may be unavailable  | yes, <br> but not very trival <br> (called quasi-Newton methods)| yes |\n",
    "| generalization <br> to high dimensions <br> (numerical aspects) | N/A | demanding | depends | depends |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "This work is licensed under [Creative Commons Attribution-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-sa/4.0/)\n",
    "Part of the content of this notebook is borrowed from [Elementary Numerical Analysis (with Python)](https://lemesurierb.people.cofc.edu/elementary-numerical-analysis-python/preface.html) written by Brenton LeMesurier, College of Charleston and University of Northern Colorado. Thanks to Dr. LeMesurier for sharing excellent notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "[proof of contraction mapping theorem 1](https://jhparkyb.github.io/resources/notes/na/104ABoardWork_RootFinding015.png)\n",
    "\n",
    "[proof of contraction mapping theorem 2](https://jhparkyb.github.io/resources/notes/na/104ABoardWork_RootFinding016.png)\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrch",
   "language": "python",
   "name": "ptrch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
