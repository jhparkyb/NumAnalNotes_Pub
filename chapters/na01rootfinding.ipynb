{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take-aways (Chapter)\n",
    "\n",
    "After studying this chapter, we will be able to\n",
    "\n",
    "- say what is the main problem of interest,\n",
    "- explain some standard root finding methods, \n",
    "  - write the methods (pseudo-algorithm): bisection, Newton's method, secant method, and fixed point iteration\n",
    "  - explain their mathematical and computational pros and cons, \n",
    "- explain why they work or related facts at an intuitive level,\n",
    "  - intuition behind the four methods,\n",
    "- give theoretical arguments about important facts,\n",
    "  - derivation of Newton's method,\n",
    "  - contraction mapping theorem,\n",
    "  - convergence of fixed point iteration,\n",
    "- give precise results on the four methods and related facts with the help of reference\n",
    "- write a program that solve an equation,\n",
    "  - write a code that implements at least two of the main root finding methods,\n",
    "  - report computational results that highlight some important aspects of the methods or problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Problem of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Problem of interest***\n",
    ">\n",
    "> Given a function $f:\\mathbb{R} \\to \\mathbb{R}$, find $\\xi\\in\\mathbb{R}$ such that\n",
    "> $$f(\\xi)=0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Methods\n",
    "\n",
    "1. Bisection method\n",
    "1. Newton's method\n",
    "1. Secant method\n",
    "1. Fixed point iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why do we care about root finding?\n",
    "\n",
    "1. mathematical problem \n",
    "   1. Polynomials of degree 5 or higher do not have solution formula. (Galois and Abel)\n",
    "   2. We often need only approximate zeros to even polynomials of degree 3 or 4. And their formula are complicated.\n",
    "   3. Transcendental equations.\n",
    "2. Many other applications end up resulting in equations to solve.\n",
    "   1. $x-\\tan(x)=0$ (diffraction of light)\n",
    "   2. $ x -a \\sin(x) = b$, where $a,b$ take various values (planetary orbits)\n",
    "   3. Finding solution to differential equations (ODE and PDE) result in a system of algebraic equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bisection method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method\n",
    "\n",
    "![Bisection illustration](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Bisection_method.svg/1024px-Bisection_method.svg.png)\n",
    "\n",
    "Figure: Wikipedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Algorithm** (Bisection method)\n",
    "> \n",
    "> **Data**\n",
    "> - $f$: function\n",
    "> - $[a, b]$: initial interval with a sign-change: $f(a) f(b) < 0$\n",
    "> \n",
    "> **Initialize**\n",
    "> - TOL: error tolerance\n",
    ">\n",
    "> **Main computation**\n",
    "> \n",
    "> - **while** $(b-a)/2 > $ TOL \n",
    ">   - $c \\leftarrow \\frac{a + b}{2}$\n",
    ">   - if $f(c) = 0$, **stop**, **end**\n",
    ">   - if $f(a) f(c) < 0$ then:\n",
    ">       - $b \\leftarrow c$\n",
    ">   - else:\n",
    ">       - $a \\leftarrow c$\n",
    ">\n",
    "> **Result**\n",
    ">\n",
    "> - The final interval $[a,b]$ contains a root.\n",
    "> - The approximate root is the final value of $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "- The bisection method converges to the solution linearly.\n",
    "\n",
    "> **Definition** (Linear convergence) \n",
    "> \n",
    "> Let $\\{x_n\\}_{n\\in\\mathbb{N}_0}$ be a sequence that converges to $\\xi$. We say that it converges *linearly* if there exists $\\lambda\\in(0,1)$ such that\n",
    "> $$ e_{n+1} = \\lambda e_n, $$\n",
    "> where $e_n:=|x_n - \\xi|$ for $n=0, 1, 2,\\cdots$. In words, it means *errors get shrunken by a factor of a fraction*.\n",
    "\n",
    "> **Theorem** (Linear convergence of bisection method)\n",
    ">\n",
    "> Suppose the bisection method is applied to solve an equation $f(x)=0$, where $f:[a,b]\\to{\\mathbb{R} }$ is a continuous function and satisfies $f(a)f(b) < 0$. Let $[a_0, b_0]=[a,b], [a_1, b_1], [a_2, b_2], \\cdots$ be the intervals generated by the method and let $c_n=(a_n+b_n)/2$ be the midpoint of $[a_n,b_n]$. Then $\\lim_{n\\to\\infty} a_n=\\lim_{n\\to\\infty} b_n = \\lim_{n\\to\\infty} c_n=\\xi$, where $\\xi\\in[a,b]$ satisfies $f(\\xi)=0$. Furthermore, the error satisfies\n",
    "> \n",
    "> $$\n",
    "> |c_n - \\xi| \\le 2^{-(n+1)}(b-a)\n",
    "> $$\n",
    ">\n",
    "> Proof: See Kincaid and Cheney (2002) p. 79."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Remark** (flexibility in definitions of convergence rate)\n",
    ">\n",
    "> The above definition is too strong (i.e., hard to satisfy) to be useful. \n",
    "> \n",
    "> 1. For example, if the following is the case,\n",
    "> \n",
    "> $$\n",
    "> e_n= 1, 1/2, \\mathbf{1/3}, 1/4, 1/8, 1/16, \\cdots,\n",
    "> $$\n",
    "> \n",
    "> then a single number $1/3$ messes up the definition. But it is more reasonable that we still consider this error decays linearly.\n",
    "> Therefore, we usually require the condition $e_{n+1}=\\lambda e_n$ except possibly finite number of exceptions. This is why we often see the following state often in a more rigorous context: \"there exists $N\\in \\mathbb{N}$ such that, for all $n\\ge N$, we have $e_{n+1}=\\lambda e_n$.\"\n",
    "> \n",
    "> 2. Also, if \n",
    "> \n",
    "> $$\n",
    "> e_n= 1, 1, 1/2, 1/3, 1/4, 1/9, 1/8, 1/27, \\cdots,\n",
    "> $$\n",
    "> \n",
    "> the behaviors are similar to the linear convergence, but not quite: the pattern differs depending on odd- or even-numbered terms. Thus, in a more rigorous context, where word-by-word translations are fundamental to communications between people from broad backgrounds, we usually prefer to use inequalities: \"there exists $N\\in \\mathbb{N}$ such that, for all $n\\ge N$, we have $e_{n+2}\\le \\lambda e_n$.\" In the current example, if we set $\\lambda=1/2$ the statement is true.\n",
    "> \n",
    "> In this course, we will pay more attention to the idea rather than trying to be very accurate about the statements of convergence rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis\n",
    "\n",
    "- We skip the proof of the convergence of the bisection method (a) for it is evident from our intuition, and (b) to include more hands-on computations. \n",
    "- However, the proof is a great exercise involving what we have learned from real analysis. I encourage you trying it and welcome any questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Take-aways (lecture)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After studying this chapter, we will be able to\n",
    "\n",
    "- start by clarifying the problem of interest,\n",
    "- explain some standard root finding methods, \n",
    "  - write the methods (pseudo-algorithm): Newton's method (and maybe secant method)\n",
    "  - explain their mathematical and computational pros and cons, \n",
    "- explain why they work or related facts at an intuitive level,\n",
    "  - intuition behind Newton's method,\n",
    "    - geometric\n",
    "    - Taylor theorem\n",
    "- give theoretical arguments about important facts,\n",
    "  - derivation of Newton's method,\n",
    "    - geometric\n",
    "    - Taylor theorem\n",
    "  - describe quadratic convergence with concrete numbers,\n",
    "- give precise convergence results on the Newton's methods and related facts with the help of reference,\n",
    "- write a program that solve an equation,\n",
    "  - write a code that implements the Newton's methods,\n",
    "  - report computational results that highlight some important aspects of the methods or problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ***Problem of interest***\n",
    ">\n",
    "> Given a function $f:\\mathbb{R} \\to \\mathbb{R}$, find $\\xi\\in\\mathbb{R}$ such that\n",
    "> $$f(\\xi)=0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Terminology**\n",
    "\n",
    "It is also called *Newton-Raphson* method.\n",
    "\n",
    "\n",
    "**Geometric intuition**\n",
    "\n",
    "[Newton's method: Geogebra interactive module](https://www.geogebra.org/m/n6KXp4hE)\n",
    "\n",
    "Creator: Lenore Horner\n",
    "\n",
    "[Newton's method: Still illustration](https://math24.net/images/newtons-method1.svg)\n",
    "\n",
    "Figure: https://math24.net/newtons-method.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Derivation of Newton's method using tangent line intuition**\n",
    "\n",
    "(Tell the teacher what you want to try.)\n",
    "\n",
    "See Board work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Algorithm** (Newton's method)\n",
    ">\n",
    "> Given a differentiable function $f:\\mathbb{R}\\to\\mathbb{R}$ and an initial guess $x_0\\in\\mathbb{R}$, compute, for $n\\ge 0$,\n",
    ">\n",
    "> $$ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question** (The Newton's method)\n",
    ">\n",
    "> Write down the Newton's method on a piece of paper.\n",
    ">  \n",
    "> - Repeat it until you get the method precisely. \n",
    "> - Consult notes/books only after finishing a trial.\n",
    "> - Feel free to a conversation with your peers. \n",
    "> - Share the clues/tricks/mnemonic device, etc.\n",
    ">\n",
    "> (Reminder) This is **about atmosphere and process**, not getting it right at once.\n",
    "> \n",
    "> 1. Think for a short time.\n",
    "> 2. Share your guess with your pair.\n",
    "> 3. Feel free to say out loud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Remark***\n",
    ">\n",
    "> There are different styles of algorithm or pseudo-algorithm.\n",
    ">\n",
    "| Mathematics- or idea-oriented pseudo-algorithm | Coding-oriented pseudo-algorithm |\n",
    "|---|---|\n",
    "|Given an initial guess $x_0$, <br> compute <br> $ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$ for $n\\ge 0$. | Input (or Data): $x_0$, $f$, $f'$ <br> Set: $Tol>0$, $x \\gets x_0$ <br> While $\\|x - x_{pre}\\| > Tol$: <br> $ \\quad \\quad x_{pre} \\gets x $ <br> $\\quad \\quad x \\gets x - \\frac{f(x)}{f'(x)}$ |\n",
    "| Focus on the essence | Also consider some details in implementation. In particular, this usually includes *stopping criteria*. |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Convergence of the Newton's method is not guaranteed.\n",
    "- If convergent, it converges to quadratically fast.\n",
    "\n",
    "> **Theorem** (Local, quadratic convergence of Newton's method)\n",
    "> \n",
    "> Let $f$ be twice continuously differentiable and $f(\\xi)=0$. If $f'(\\xi) \\neq 0$, then Newton's method is locally and quadratically convergent to $x$. That is, (local convergence) the method converges to the zero $\\xi$ if the initial guess $x_0$ is sufficiently close to $\\xi$, and (quadratic convergence) there exists $C>0$ such that\n",
    "> $$ \\lim_{n\\to\\infty} \\frac{e_{n+1}}{e_n^2} = C, $$\n",
    "> where $e_n:=|x_n - \\xi|$ and $x_n$'s ($n=0,1,2,\\cdots$) are the sequence generated by the Newton's method.\n",
    "\n",
    "> **Definition** (Quadradic convergence) \n",
    "> \n",
    "> Let $\\{x_n\\}_{n\\in\\mathbb{N}}$ be a sequence that converges to $\\xi$. We say that it converges quadratically fast if there exists $C>0$ such that\n",
    "> $$ \\lim_{n\\to\\infty} \\frac{e_{n+1}}{e_n^2} = C, $$\n",
    "> where $e_n:=|x_n - \\xi|$ for $n=0, 1, 2,\\cdots$. In words, it means *errors get shrunken by a square of the previous error*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question** (How fast is a quadratic convergence?)\n",
    ">\n",
    "> Suppose Newton's method starts to manifest quadratic converge from 5th iteration with $e_5 = 0.01$. Guess what will be the error after four more iterations? For simplicity, assume $C=1$.\n",
    ">\n",
    "> (Reminder) This is **about atmosphere**, not getting it right.\n",
    "> \n",
    "> 1. Think for a short time.\n",
    "> 2. Share your guess with your pair.\n",
    "> 3. Type your answer in clicker.\n",
    "> 4. Feel free to say out loud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Divergence of Newton's method)\n",
    "\n",
    "- Newton's method may diverge while it converges fast if it does.\n",
    "- If it happens to be $f'(x_n)=0$, the method breaks down.\n",
    "\n",
    "![Divergence of Newton's method](https://amsi.org.au/ESA_Senior_Years/imageSenior/2a_numerical_methods_graph_7.png)\n",
    "\n",
    "Figure: https://amsi.org.au/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!-- \n",
    "- In favor of more computational activities, we skip the proof the quadratic convergence of Newton's method. \n",
    "- Instead, we derive the Newton's method from calculus point of view. It involves a great application of Taylor's theorem. \n",
    "- Also, we include some history of the Newton's method in place of its convergence proof. \n",
    "- Newton's method is highly relevant even these days and since it inspires many other methods. \n",
    "- -->\n",
    "\n",
    "**Derivation of Newton's method using Taylor theorem**\n",
    "\n",
    "1. Let $\\xi$ is a root, i.e., $f(\\xi)=0$. \n",
    "2. Expand $f(\\xi)$ around the current position, say, $x_n$. \n",
    "3. Take the linear approximation, namely, ignore the second order term or higher.\n",
    "4. Solve for $\\xi$, and call it $x_{n+1}$. \n",
    "\n",
    "**Remark**\n",
    "\n",
    "- (In step 1) Pretending to know the solution is often start of a magic.\n",
    "- (In step 2) What about the other way around?\n",
    "- (In step 3) What did we lose and what did we obtain? \n",
    "\n",
    "<!-- ![Derivation of Newton's method](https://jhparkyb.github.io/resources/notes/na/104ASlides_RootFinding014.png) -->\n",
    "\n",
    "[Derivation of Newton's method](https://jhparkyb.github.io/resources/notes/na/der_NewtonMethodTaylor_lp2000.png)\n",
    "\n",
    "\n",
    "In favor of more computational activities, we skip the proof the quadratic convergence of Newton's method. But since Newton's method is highly relevant even these days and since it inspires many other methods, we include some history about it.\n",
    "\n",
    "> **Historical note**\n",
    ">\n",
    "> 1. Babylonians (1894 BC - 539 BC) used the method to approximate square roots: $\\sqrt{2}$ accurately up to seven places. (Ref: [2, 3])\n",
    ">       ![Babylonian clay tablet](https://projectlovelace.net/static_prod/img/YBC7289.jpg)\n",
    ">\n",
    ">       Figure: Project Lovelace\n",
    "> 1. In 1669, the method was employed by Newton for the cubic equation $3x^3 -2x-5 = 0$. (Ref: [1])\n",
    "> 1. In 1690, Raphson described the method for a general cubic equation $x^3 — bx = c$. (Ref: [1])\n",
    "> 1. In 1818, Fourier proved the quadratic convergence of the method. (Ref: [1])\n",
    "> 1. In 1829, Cauchy proved a convergence theorem which does not assume the existence of a solution. (existence of a solution is a consequence; but it assumes some other conditions on the iterates) (Ref: [1])\n",
    "> 1. In 1939, Kantorovich proved a convergence theorem in a very general setting. (Ref: [1])\n",
    "> 1. In 1948, Kantorovich proved an improved version, which is now called Kantorovich's theorem or the Newton-Kantorovich theorem: existence of a solution is not assumed and the convergence is quadratic in a very general setting. (Ref: [1])\n",
    "> \n",
    "> Reference\n",
    "> \n",
    "> [1] Brezinski (2001) Numerical Analysis: Historical Developments in the 20th Century. p. 242\n",
    "> \n",
    "> [2] Sauer (2017) Numerical Analysis p. 41\n",
    "> \n",
    "> [3] Wikipedia (Babylonia) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computational example (Babylonians)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Problem** (Computing $\\sqrt{2}$)\n",
    ">\n",
    "> Write a code that computes approximate value of $\\sqrt{2}$ using Newton's method.\n",
    "\n",
    "(Step 1) Cast the problem as a root finding problem and summarize it. (Intellectual work needed for $f$ and $f'$)\n",
    "\n",
    "(Step 2) Write a (programming) function that implements Newton's method.\n",
    "\n",
    "(Step 3) Set up the computation (function, initial guess, etc.) and implement it.\n",
    "\n",
    "(Step 4) Reorganize the result for specific purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- True solution can be obtained from [Wolfram alpha: N[sqrt[2], 20]](https://www.wolframalpha.com/input?i=N%5Bsqrt%5B2%5D%2C+20%5D).\n",
    "  - `N[sqrt[2], 20]` (Numerical value of $\\sqrt{2}$ up to 20 decimal digit) gives us 1.4142135623730950488.\n",
    "- 20 decimal digits are enough because computers can distinguish only up to around $2^{−52} \\approx 2.22\\times 10^{-16}$ when they use floating point arithmetic.\n",
    "  - This number is called *machine epsilon*.\n",
    "  - Machine epsilon depends on data type. (See [Wikipedia](https://en.wikipedia.org/wiki/Machine_epsilon) page for details)\n",
    "  - Wolfram alpha can handle higher precision by using more computing resources than floating point arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton's method :  1.444238094866232    (4 iterations taken)\n",
      "True solution   :  1.4142135623730951\n",
      "Error           :  0.030024532493136746\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def newton(f, fp, ini, tol=1e-8, max_iter=20):\n",
    "    \"\"\"\n",
    "    Return an approximate root of a function using Newton's method.\n",
    "\n",
    "    INPUT\n",
    "        f: function whose zero is sought.\n",
    "        fp: derivative of f (name from 'f prime')\n",
    "        ini: initial guess\n",
    "        tol: tolerance for stopping criterion. If consecutive iterates differ by less than this, it is considered convergenct.\n",
    "        max_iter: maximum number of iterations\n",
    "    OUTPU\n",
    "        approximated zero and the number of iterations. When the maximum number of iterations is reached, the last iterate with a warning message.\n",
    "    \"\"\"\n",
    "    x = ini\n",
    "    for i in range(max_iter):\n",
    "        x_pre = x\n",
    "        x = x - f(x)/fp(x)\n",
    "\n",
    "        if np.abs(x - x_pre) < tol: \n",
    "            break\n",
    "    \"\"\"\n",
    "    if i == max_iter - 1:\n",
    "        print(\"   Warning (newton): maximum number of iteration reached.\\n     --> The output may not be close enough to the zero.\")\n",
    "    \"\"\"\n",
    "    return x, i + 1\n",
    "\n",
    "# find the square root\n",
    "f = lambda x: x*x - 2.\n",
    "fp = lambda x: 2.*x\n",
    "\n",
    "x0 = 10.\n",
    "max_iter = 4\n",
    "\n",
    "appr, iter = newton(f, fp, x0, max_iter=max_iter)\n",
    "sol = 1.4142135623730950488 # obtained from Wolfram Alpha\n",
    "err = np.abs(appr - sol)\n",
    "\n",
    "print(\"Newton's method : \", appr, f\"   ({iter} iterations taken)\")\n",
    "print(\"True solution   : \", sol)\n",
    "print(\"Error           : \", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\\# iterations</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.685786e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.331865e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.229813e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.002453e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.120928e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.442917e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    \\# iterations         error\n",
       "1             1.0  3.685786e+00\n",
       "2             2.0  1.331865e+00\n",
       "3             3.0  3.229813e-01\n",
       "4             4.0  3.002453e-02\n",
       "5             5.0  3.120928e-04\n",
       "6             6.0  3.442917e-08\n",
       "7             7.0  2.220446e-16\n",
       "8             8.0  0.000000e+00\n",
       "9             8.0  0.000000e+00\n",
       "10            8.0  0.000000e+00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "N = 10\n",
    "df = pd.DataFrame(columns=['\\# iterations', 'error'])\n",
    "\n",
    "for i in range(1, N+1):\n",
    "    appr, iter = newton(f, fp, x0, max_iter=i)\n",
    "    err = np.abs(appr - sol)\n",
    "    df.loc[i] = [iter, appr - sol]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secant method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n",
    "Newton's method is great. But it requires $f'(x)$ as well as $f(x)$.\n",
    "\n",
    "How can we overcome this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method\n",
    "\n",
    "**Idea**: Replace $f'(x_n)$ in Newton's method with something similar.\n",
    "\n",
    "**Geometric intuition**\n",
    "\n",
    "![Secant method](https://mathworld.wolfram.com/images/eps-svg/SecantMethod_800.svg)\n",
    "\n",
    "Figure: Wolfram MathWorld.\n",
    "\n",
    "[Secant method: Geogebra interactive module](https://www.geogebra.org/m/vpk4geyu)\n",
    "\n",
    "Author: Marian Choy\n",
    "\n",
    "\n",
    "> ***Algorithm*** (Secant method)\n",
    ">\n",
    "> Given $x_0, x_1\\in\\mathbb{R}$, compute, for $n\\ge 1$,\n",
    ">\n",
    "> $$ x_{n+1}=x_{n}-f\\left(x_{n}\\right)\\frac{\\left(x_{n}-x_{n-1}\\right)}{f\\left(x_{n}\\right)-f\\left(x_{n-1}\\right)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "- If the secant method converges, its rate of convergence is the *golden ratio* ($\\approx 1.618$).\n",
    "- User must feed **two initial guesses**.\n",
    "- It requires **only the function evaluation**, but not the derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis\n",
    "\n",
    "In favor of more computational activities, we skip the proof the *superlinear* convergence (i.e., a convergence rate that is faster the linear: $e_{k+1} \\approx C e_k^\\alpha$ with $\\alpha>1$) of the secant method.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed point iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method\n",
    "\n",
    "**Terminology**\n",
    "\n",
    "It is also called *Picard iteration* or *functional iteration*.\n",
    "\n",
    "**Geometric interpretation**\n",
    "\n",
    "\"A picture paints a thousand words.\" \n",
    "\n",
    "![Fixed point iteration](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Cosine_fixed_point.svg/1920px-Cosine_fixed_point.svg.png)\n",
    "\n",
    "Figure: Wikipedia\n",
    "\n",
    "[Fixed point iteration: Geogebra interactive module](https://www.geogebra.org/m/qUbg7Z6W) \n",
    "\n",
    "Author: stuart.cork\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Algorithm** (Fixed point iteration - general)\n",
    ">\n",
    "> Given a function $f:\\mathbb{R}\\to\\mathbb{R}$ and an initial guess $x_0\\in\\mathbb{R}$, compute, for $n\\ge 0$,\n",
    ">\n",
    "> $$ x_{n+1} = f(x_n). $$\n",
    "\n",
    "> **Algorithm** (Fixed point iteration - root finding for $f$)\n",
    ">\n",
    "> Given a function $f:\\mathbb{R}\\to\\mathbb{R}$ and an initial guess $x_0\\in\\mathbb{R}$, set $g(x)=x+f(x)$, compute, for $n\\ge 0$, \n",
    ">\n",
    "> $$ x_{n+1} = g(x_n). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "- If repeated applications of a function $g$ converges to $\\xi$, then it solves $x=g(x)$. (Some condition on $f$ is needed: see Analysis below.)\n",
    "- If converges, the fixed point iteration method converges *linearly*: there exists $C>0$ such that $e_{k+1}\\approx \\lambda e_k$ with $0<\\lambda<1$. \n",
    "- If you want to solve the equation $f(x)=0$, set $g(x):=x+f(x)$ and apply the fixed point iteration to $g$. Then, the fixed point $\\xi$ satisfies \n",
    "    $$\\xi = g(\\xi)=\\xi+f(\\xi) \\quad \\text{implies} \\quad f(\\xi)=0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition** (Fixed point)\n",
    "> $x$ is called a *fixed point* of the function $g$ if $g(x)=x$.\n",
    "\n",
    "\n",
    "> **Definition** (Contractive/Contraction mapping)\n",
    "> A function $g:D \\to \\mathbb{R}$ is called *contractive* or a *contractive mapping/contraction* if there is $\\lambda\\in[0,1)$ such that $|g(x)-g(y)|\\le \\lambda|x-y|$ for all $x,y\\in D$.\n",
    "\n",
    "> **Theorem** (Contraction mapping is continuous)\n",
    "> If $g:D\\to \\mathbb{R}$ is contractive, it is continuous.\n",
    "\n",
    "> **Theorem** (Absolute convergence implies convergence)\n",
    "> If $\\sum_{n=1}^\\infty x_n$ is absolutely convergent, i.e., $\\sum_{n=1}^\\infty |x_n| < \\infty$, then $\\sum_{n=1}^\\infty x_n$ also converges.\n",
    "\n",
    "> **Theorem** (Contraction Mapping Theorem)\n",
    "> Let $D$ be a closed subset of $\\mathbb{R}$. If $g:D \\to D$ is a contraction, then it has a unique fixed point. Moreover, this fixed point is the limit of the functional iteration starting with any initial guess.\n",
    "\n",
    "[Proof of contraction mapping theorem 1](https://jhparkyb.github.io/resources/notes/na/pf_ContractionMappingThm1_lp3000.png)\n",
    "\n",
    "[Proof of contraction mapping theorem 2](https://jhparkyb.github.io/resources/notes/na/pf_ContractionMappingThm2_lp3001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof outline\n",
    "\n",
    "1. $x_n= (x_n - x_{n-1}) + (x_{n-1} - x_{n-2}) + \\cdots + (x_{1} - x_{0}) + x_0$ absolutely converges, hence converges.\n",
    "   - $|x_n - x_{n-1}| \\le \\lambda^{n-1} |x_1 - x_0|$\n",
    "2. Pass $x_{n+1}=g(x_n)$ to the limit $n\\to \\infty$.\n",
    "3. Uniqueness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Question** \n",
    ">\n",
    "> The above proof outline did not use one condition and used another condition implicitly. What are they? \n",
    ">\n",
    "> (Reminder) This is **about atmosphere**, not getting it right.\n",
    "> \n",
    "> 1. Think for a short time.\n",
    "> 2. Share your guess with your pair.\n",
    "> 3. Type your answer in clicker.\n",
    "> 4. Feel free to say out loud.\n",
    "\n",
    "(Homework questions will ask you what happens if you ignore them.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparisons of root-finding methods\n",
    "\n",
    "| | Bisection | Newton | Secant | Fixed point |\n",
    "|---|---|---|---|---|\n",
    "| need $f(x)$ | O | O | O | O |\n",
    "| need $f'(x)$ | - | O | - | - |\n",
    "| rate of convergence | 1 | 2 | 1.618 | 1 |\n",
    "| rate of convergence <br> per two function eval's | 1 <br> (with smaller contraction constant) | 2 | $1.618^2\\approx 2.618$ | 1 <br> (with smaller contraction constant) |\n",
    "| global convergence | yes <br> if $f(a)f(b)<0$ | no | no | practially no |\n",
    "| solution boxed | yes | no | no | generally, no |  \n",
    "| generalization <br> to high dimensions <br> (intellectual effort) | awkward | yes, <br> but gradient may be unavailable  | yes, <br> but not very trival <br> (called quasi-Newton methods)| yes |\n",
    "| generalization <br> to high dimensions <br> (numerical aspects) | N/A | demanding | depends | depends |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "This work is licensed under [Creative Commons Attribution-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-sa/4.0/)\n",
    "Part of the content of this notebook is borrowed from [Elementary Numerical Analysis (with Python)](https://lemesurierb.people.cofc.edu/elementary-numerical-analysis-python/preface.html) written by Brenton LeMesurier, College of Charleston and University of Northern Colorado. Thanks to Dr. LeMesurier for sharing excellent notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "[proof of contraction mapping theorem 1](https://jhparkyb.github.io/resources/notes/na/104ABoardWork_RootFinding015.png)\n",
    "\n",
    "[proof of contraction mapping theorem 2](https://jhparkyb.github.io/resources/notes/na/104ABoardWork_RootFinding016.png)\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrch",
   "language": "python",
   "name": "ptrch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
