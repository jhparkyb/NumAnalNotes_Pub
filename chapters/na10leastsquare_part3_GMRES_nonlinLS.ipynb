{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is GMRES?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GMRES is an iterative solver for a system of linear equations: $Ax=b$.\n",
    "- GMRES generalizes the Conjugate gradient method to *asymmetric* matrix $A$.\n",
    "- The name is short for *generalized minimum residual* method.\n",
    "- Why do we discuss that in a chapter of least square?\n",
    "  - Part of its algorithm involves solving least square problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we care?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GMRES can deal with asymmetric matrix, which the conjugate gradient method fails with.\n",
    "- GMRES is a good choice for the solution of large, sparse, asymmetric (square) linear system $Ax=b$. [Sauer (2017) p. 235]\n",
    "- GMRES deals with ill-conditioning using orthogonality. [Sauer (2017) p. 235]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $K_j:=\\{r, Ar, A^2 r, \\cdots, A^j r\\}$ is called *Krylov* space. \n",
    "- The approximate solution $x_k$ at $k$-th iteration is the best approximation of the true solution $x$ in $K_{k}$. \n",
    "  - Conjugate gradient method uses the similar idea. And they both belong to *Krylov methods*.\n",
    "- As $k$ increases, $K_k$ expands and the approximation gets better and better.\n",
    "  - In theory, GMRES is a direct method: It terminates at $n$-th iteration with the exact solution if $A$ is nonsingular. [Sauer (2017) p. 237]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm** (GMRES; Sauer (2017) p. 235)\n",
    "\n",
    "- Given\n",
    "  - $A$: $n$-by-$n$ matrix\n",
    "  - $b$: vector of length $n$\n",
    "- Initialize\n",
    "  - $x_0$: initial guess\n",
    "  - $r=b-A x_0$: initial residual \n",
    "  - $q_1=r /\\|r\\|_2$\n",
    "- Compute\n",
    "  - **for** $k=1,2, \\ldots, m$\n",
    "    - $y=A q_k$\n",
    "    - **for** $j=1,2, \\ldots, k$\n",
    "      - $h_{j k}=q_j^T y $\n",
    "      - $y=y-h_{j k} q$\n",
    "    - $h_{k+1, k}=\\|y\\|_2$ (If $h_{k+1, k}=0$, skip next line and terminate at bottom.)\n",
    "    - $q_{k+1}=y / h_{k+1, k}$\n",
    "    - Minimize $\\left\\|H_k c_k- [\\|r\\|_2, 0, 0, \\ldots, 0]^T \\right\\|_2$ for $c_k$\n",
    "    - $x_k=Q_k c_k+x_0$\n",
    "\n",
    "At $k$-th step, \n",
    "- $[\\|r\\|_2, 0, 0, \\ldots, 0]$ is length $k+1$.\n",
    "- $c_k$ is of length $k$.\n",
    "- $H_k$ is of size $(k+1)\\times k$ and given by \n",
    " \n",
    "$$\n",
    "H = \\left[\\begin{array}{cccc}\n",
    "h_{11} & h_{12} & \\cdots & h_{1 k} \\\\\n",
    "h_{21} & h_{22} & \\cdots & h_{2 k} \\\\\n",
    "& h_{32} & \\cdots & h_{3 k} \\\\\n",
    "& \\ddots & \\vdots \\\\\n",
    "& & & h_{k+1, k}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "  - $Q_k$ is of size $n\\times k$ and given by\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{c:c:c} \n",
    "& & \\\\\n",
    "& & \\\\\n",
    "q_1 & \\cdots & q_k \\\\\n",
    "& & \\\\\n",
    "& &\n",
    "\\end{array}\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detail 1**\n",
    "\n",
    "- At step $k$ of the method, we enlarge the Krylov space by adding $A^k r$, \n",
    "- reorthogonalize the basis (i.e., inner loop for modified Gram-Schimidt), \n",
    "- and then use least squares to find the best approximation in $K_k$.\n",
    "  - This is done by finding $x_{add}$ ($Q_k c_k$ in the algorith) and add it to $x_0$.\n",
    "  - This step involves its own details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detail 2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It holds that $AQ_k = Q_{k+1} H_k$ for each $k$\n",
    "  - This is a consequence of Gram-Schmidt (inner loop): with $y=A q_j$, we have orthogonal decomposition $y = \\underbrace{(q_1^T y)}_{h_{1,j}} q_1 + \\underbrace{(q_2^T y)}_{h_{2,j}} q_2 + \\cdots + \\underbrace{(q_{j+1}^T y)}_{h_{j+1,j}}q_{j+1}$ for $j=1,2,\\cdots,k$.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "AQ_k &= A\\left[\\begin{array}{c:c:c} \n",
    "  & & \\\\\n",
    "  & & \\\\\n",
    "  q_1 & \\cdots & q_k \\\\\n",
    "  & & \\\\\n",
    "  & &\n",
    "  \\end{array}\\right] \\\\\n",
    "&=\\left[\\begin{array}{c:c:c} \n",
    "  & & \\\\\n",
    "  & & \\\\\n",
    "  Aq_1 & \\cdots & Aq_k \\\\\n",
    "  & & \\\\\n",
    "  & &\n",
    "  \\end{array}\\right]\n",
    "\\\\\n",
    "&=\n",
    "\\left[\n",
    "\t\\begin{array}{c:c:c:c}\n",
    "\t\tQ_{k+1} \n",
    "\t\t\\begin{pmatrix}\n",
    "  \t\t\th_{11}\\\\\n",
    "  \t\t\th_{21}\\\\\n",
    "  \t\t\t\\\\\n",
    "  \t\t\t\\\\\n",
    "  \t\t\\end{pmatrix} \n",
    "  \t\t&\n",
    "  \t\tQ_{k+1} \n",
    "\t\t\\begin{pmatrix}\n",
    "  \t\t\th_{12}\\\\\n",
    "  \t\t\th_{22}\\\\\n",
    "  \t\t\th_{32}\\\\\n",
    "  \t\t\t\\\\\n",
    "  \t\t\\end{pmatrix}\n",
    "\t  \t&\n",
    " \t\t\\cdots \n",
    "\t\t&\n",
    "  \t\tQ_{k+1} \n",
    "\t\t\\begin{pmatrix}\n",
    "  \t\t\th_{12}\\\\\n",
    "  \t\t\th_{22}\\\\\n",
    "  \t\t\th_{32}\\\\\n",
    "  \t\t\t\\vdots \\\\\n",
    "  \t\t\th_{j+1, j}\n",
    "  \t\t\\end{pmatrix}\n",
    "\t\\end{array}\n",
    "  \\right]\n",
    "\\\\\n",
    "& = Q_{k+1}\n",
    "\\left[\\begin{array}{c:c:c:c}\n",
    "h_{11} & h_{12} & \\cdots & h_{1 k} \\\\\n",
    "h_{21} & h_{22} & \\cdots & h_{2 k} \\\\\n",
    "& h_{32} & \\cdots & h_{3 k} \\\\\n",
    "& \\ddots & \\vdots \\\\\n",
    "& & & h_{k+1, k}\n",
    "\\end{array}\\right]\n",
    "\\\\\n",
    "&= Q_{k+1}H_k\n",
    "% \\left[\\begin{array}{l:l:l:l} \n",
    "% & & & \\\\\n",
    "% q_1 & \\cdots & q_k & q_{k+1} \\\\\n",
    "% & & & \\\\\n",
    "% & & &\n",
    "% \\end{array}\\right]\\left[\\begin{array}{cccc}\n",
    "% h_{11} & h_{12} & \\cdots & h_{1 k} \\\\\n",
    "% h_{21} & h_{22} & \\cdots & h_{2 k} \\\\\n",
    "% & h_{32} & \\cdots & h_{3 k} \\\\\n",
    "% & \\ddots & \\vdots \\\\\n",
    "% & & & h_{k+1, k}\n",
    "% \\end{array}\\right]\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of GMRES is a topic of computational project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear least square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Vector dot product rule)\n",
    "\n",
    "Let $u\\left(x_1, \\ldots, x_n\\right)$ and $v\\left(x_1, \\ldots, x_n\\right)$ be $\\mathbb{R}^n$-vector-valued functions, and let $A\\left(x_1, \\ldots, x_n\\right)$ be an $n \\times n$ matrix function. The dot product $u^T v$ is a scalar function. Then, we have\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla\\left(u^T v\\right)=v^T D u+u^T D v,\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "D(A v)=A \\cdot D v+\\sum_{i=1}^n v_i D a_i,\n",
    "$$\n",
    "\n",
    "where $a_i$ denotes the $i$ th column of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More information on GMRES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convergence\n",
    "  - (Pessimistic) For every nonincreasing sequence $a_1, \\cdots, a_{m−1}, a_m = 0$, one can find a matrix A such that the $\\|r_n\\| = a_n$ for all $n$, where $r_n$ is the $n$-th residual. In particular, it is possible to find a matrix for which the residual stays constant for $m − 1$ iterations, and only drops to zero at the last iteration. (Reference: [Wikipedia](https://en.wikipedia.org/wiki/Generalized_minimal_residual_method#Convergence) and also the instructor heard this in a plenary talk of a very reliable conference, though details not remembered. I remember I got surprised by the fact that even solving a linear system can be inherently difficult.)\n",
    "  - (Optimistic in practice) In practice, though, GMRES often performs well. This can be proven in specific situations. \n",
    "- Relationship with MINRES\n",
    "  - MINRES is similar to Conjugate Gradient (CG) method, but it assumes the matrix to be only symmetric, allowing indefinite matrices, whereas CG assumes it to be symmetric positive definite.\n",
    "  - The GMRES method is essentially a generalization of MINRES for arbitrary matrices. (See the technical remark for more detials if interested.)\n",
    "- $H_k$ appearing in the GMRES is an upper (nonsquare) *Hessenberg matrix*. \n",
    "  - A upper Hessenberg matrix has zero entries below tridiaginal.\n",
    "  - This remark is meant to familarize with terminology.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Technical remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Technical remarks on GMRES)\n",
    "\n",
    "- (GMRES and MINRES) The GMRES method is essentially a generalization of MINRES for arbitrary matrices. Both minimize the 2-norm of the residual and do the same calculations in exact arithmetic when the matrix is symmetric. MINRES is a short-recurrence method with a constant memory requirement, whereas GMRES requires storing the whole Krylov space, so its memory requirement is roughly proportional to the number of iterations. On the other hand, GMRES tends to suffer less from loss of orthogonality. (Reference: Wikipedia, one of whose the original references is broken; treat this remark as advice, but not as truth before confirmation.)\n",
    "- (No Krylov space; further study needed) The Arnoldi iteration (computations for $q_j$'s) reduces to the Lanczos iteration for symmetric matrices. The corresponding Krylov subspace method is the minimal residual method (MinRes) of Paige and Saunders. \n",
    "    - Unlike the unsymmetric case, the MinRes method is given by a three-term recurrence relation. \n",
    "    - It can be shown that there is no Krylov subspace method for general matrices, which is given by a short recurrence relation and yet minimizes the norms of the residuals, as GMRES does. (Reference: [Wikipedia](https://en.wikipedia.org/wiki/Generalized_minimal_residual_method#Comparison_with_other_solvers))\n",
    "- (Hessenberg matrix) Any matrix is unitarily similar to Henssenberg. (Reference: [Wikipedia](https://en.wikipedia.org/wiki/Hessenberg_matrix))\n",
    "    - The validity of this statement is trivial by *Schur triangularization* or *Schur decomposition* ([Schur decomposition Wikipedia](https://en.wikipedia.org/wiki/Schur_decomposition)).\n",
    "  - When triangularization is needed, computing a Hessenberg matrix, then moving on to a triangular matrix is more efficient. (See more detailed remark on [Wikipedia](https://en.wikipedia.org/wiki/Hessenberg_matrix#Computer_programming))\n",
    "- (Convergence of GMRES) \n",
    "  - According to Greenbaum, Pták and Strakoš states that for every nonincreasing sequence $a_1, \\cdots, a_{m−1}, a_m = 0$, one can find a matrix A such that the $\\|r_n\\| = a_n$ for all $n$, where $r_n$ is the $n$-th residual. In particular, it is possible to find a matrix for which the residual stays constant for $m − 1$ iterations, and only drops to zero at the last iteration. (Reference: [Wikipedia](https://en.wikipedia.org/wiki/Generalized_minimal_residual_method#Convergence) and also the instructor heard this in a plenary talk of a very reliable conference, though details not remembered. I remember I got surprised by the fact that even solving a linear system can be inherently difficult.)\n",
    "  - In practice, though, GMRES often performs well. This can be proven in specific situations. If the symmetric part of $A$, that is $\\left(A^T+A\\right) / 2$, is positive definite, then\n",
    "$$\n",
    "\\left\\|r_n\\right\\| \\leq\\left(1-\\frac{\\lambda_{\\min }^2\\left(1 / 2\\left(A^T+A\\right)\\right)}{\\lambda_{\\max }\\left(A^T A\\right)}\\right)^{n / 2}\\left\\|r_0\\right\\|,\n",
    "$$\n",
    "where $\\lambda_{\\min }(M)$ and $\\lambda_{\\max }(M)$ denote the smallest and largest eigenvalue of the matrix $M$, respectively. \n",
    "\n",
    "If $A$ is symmetric and positive definite, then we even have\n",
    "$$\n",
    "\\left\\|r_n\\right\\| \\leq\\left(\\frac{\\kappa_2(A)^2-1}{\\kappa_2(A)^2}\\right)^{n / 2}\\left\\|r_0\\right\\| .\n",
    "$$\n",
    "where $\\kappa_2(A)$ denotes the condition number of $A$ in the Euclidean norm.\n",
    "\n",
    "In the general case, where $A$ is not positive definite, we have\n",
    "$$\n",
    "\\frac{\\left\\|r_n\\right\\|}{\\|b\\|} \\leq \\inf _{p \\in P_n}\\|p(A)\\| \\leq \\kappa_2(V) \\inf _{p \\in P_n} \\max _{\\lambda \\in \\sigma(A)}|p(\\lambda)|,\n",
    "$$\n",
    "where $P_n$ denotes the set of polynomials of degree at most $n$ with $p(0)=1, V$ is the matrix appearing in the spectral decomposition of $A$, and $\\sigma(A)$ is the spectrum of $A$. Roughly speaking, this says that fast convergence occurs when the eigenvalues of $A$ are clustered away from the origin and $A$ is not too far from normality. (Reference: [Wikipedia](https://en.wikipedia.org/wiki/Generalized_minimal_residual_method#Convergence), whose original reference is Lloyd N. Trefethen and David Bau, III, Numerical Linear Algebra, Society for Industrial and Applied Mathematics, 1997)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "This work is licensed under [Creative Commons Attribution-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrch",
   "language": "python",
   "name": "ptrch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
