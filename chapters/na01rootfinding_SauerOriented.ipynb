{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take-aways (Chapter)\n",
    "\n",
    "After studying this chapter, we will be able to\n",
    "\n",
    "- say what is the main problem of interest,\n",
    "- explain some standard root finding methods, \n",
    "  - write the methods (pseudo-algorithm): bisection, Newton's method, secant method, and fixed point iteration\n",
    "  - explain their mathematical and computational pros and cons, \n",
    "- explain why they work or related facts at an intuitive level,\n",
    "  - intuition behind the four methods,\n",
    "- give theoretical arguments about important facts,\n",
    "  - derivation of Newton's method,\n",
    "  - contraction mapping theorem,\n",
    "  - convergence of fixed point iteration,\n",
    "- give precise results on the four methods and related facts with the help of reference\n",
    "- write a program that solve an equation,\n",
    "  - write a code that implements at least two of the main root finding methods,\n",
    "  - report computational results that highlight some important aspects of the methods or problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Problem of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Problem of interest***\n",
    ">\n",
    "> Given a function $f:\\mathbb{R} \\to \\mathbb{R}$, find $x\\in\\mathbb{R}$ such that\n",
    "> $$f(x)=0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods\n",
    "\n",
    "1. Bisection method\n",
    "1. Newton's method\n",
    "1. Secant method\n",
    "1. Fixed point iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we care about root finding?\n",
    "\n",
    "1. mathematical problem \n",
    "   1. Polynomials of degree 5 or higher do not have solution formula. (Galois and Abel)\n",
    "   2. We often need only approximate zeros to even polynomials of degree 3 or 4. And their formula are complicated.\n",
    "   3. Transcendental equations.\n",
    "2. Many other applications end up resulting in equations to solve.\n",
    "   1. $x-\\tan(x)=0$ (diffraction of light)\n",
    "   2. $ x -a \\sin(x) = b$, where $a,b$ take various values (planetary orbits)\n",
    "   3. Finding solution to differential equations (ODE and PDE) result in a system of algebraic equations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Remark** (Aside: Cubic and quartic formula)\n",
    "\n",
    "- In regard to the first reason, even cubic (degree 3) or quartic (degree 4) equations are complicated \n",
    "- Also, they involve unusual numerical procedures: complex numbers appear even when there is only one real simple root.\n",
    "\n",
    "Solution for $ax^3+bx^2+cx+d=0$:\n",
    "\n",
    "![Cubic formula](https://www.curtisbright.com/quartic/cubic.png)\n",
    "\n",
    "Solution for $ax^4 + bx^3 + cx^2 + dx + e = 0$:\n",
    "\n",
    "![Quartic formula](https://www.curtisbright.com/quartic/quartic.png)\n",
    "\n",
    "Figures: https://www.curtisbright.com/quartic/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bisection method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Bisection illustration](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Bisection_method.svg/1024px-Bisection_method.svg.png)\n",
    "\n",
    "Figure: Wikipedia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm** (Bisection method)\n",
    "\n",
    "**Data**\n",
    "- $f$: function\n",
    "- $[a, b]$: initial interval with a sign-change: $f(a) f(b) < 0$\n",
    "\n",
    "**Initialize**\n",
    "- TOL: error tolerance\n",
    "**Main computation**\n",
    "\n",
    "- **while** $(b-a)/2 > $ TOL \n",
    "  - $c \\leftarrow \\frac{a + b}{2}$\n",
    "  - if $f(c) = 0$, **stop**, **end**\n",
    "  - if $f(a) f(c) < 0$ then:\n",
    "      - $b \\leftarrow c$\n",
    "  - else:\n",
    "      - $a \\leftarrow c$\n",
    "**Result**\n",
    "- The final interval $[a,b]$ contains a root.\n",
    "- The approximate root is the final value of $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergence\n",
    "\n",
    "- The bisection method converges to the solution globally and linearly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Definition** (Linear convergence) \n",
    "\n",
    "Let $e_i$ denote the error at step i of an iterative method. If\n",
    "\n",
    "$$ \n",
    "\\lim_{i\\to\\infty}\\frac{e_{i+1}}{e_i} = S < 1,\n",
    "$$\n",
    "\n",
    "the method is said to obey linear convergence with rate S.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Theorem** (Linear convergence of bisection method)\n",
    "\n",
    "Suppose the bisection method is applied to solve an equation $f(x)=0$, where $f:[a,b]\\to{\\mathbb{R} }$ is a continuous function and satisfies $f(a)f(b) < 0$. Let $[a_0, b_0]=[a,b], [a_1, b_1], [a_2, b_2], \\cdots$ be the intervals generated by the method and let $c_i=(a_i+b_i)/2$ be the midpoint of $[a_i,b_i]$. Then $\\lim_{i\\to\\infty} a_i=\\lim_{i\\to\\infty} b_i = \\lim_{i\\to\\infty} c_i=x$, where $x\\in[a,b]$ satisfies $f(x)=0$. Furthermore, the error satisfies\n",
    "\n",
    "$$\n",
    "|c_i - x| \\le 2^{-(i+1)}(b-a)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Proof: See Kincaid and Cheney (2002) p. 79."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis\n",
    "\n",
    "- We skip the proof of the convergence of the bisection method (a) for it is evident from our intuition, and (b) to include more hands-on computations. \n",
    "- However, the proof is a great exercise involving what we have learned from real analysis. I encourage you trying it and welcome any questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Part 1: What we can do\n",
    "  - Derive Newton's method using Taylor's theorem\n",
    "  - Compute $\\sqrt{2}$\n",
    "  - Quadratic convergence\n",
    "- Part 2: What to be careful of\n",
    "  - Local convergence\n",
    "  - Slow down $\\longrightarrow$ Modified Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: What we can do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ***Problem of interest***\n",
    ">\n",
    "> Given a function $f:\\mathbb{R} \\to \\mathbb{R}$, find $x\\in\\mathbb{R}$ such that\n",
    "> $$f(x)=0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why do we care?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Newton's method**\n",
    "\n",
    "- Fast convergence\n",
    "- Generalizes to more complex settings\n",
    "- Inspire many other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Root finding**\n",
    "\n",
    "1. Many applications boil down to solving an equation.\n",
    "   1. $x-\\tan(x)=0$ (diffraction of light)\n",
    "   2. $ x -a \\sin(x) = b$, where $a,b$ take various values (planetary orbits)\n",
    "   3. Finding solution to differential equations (ODE and PDE) result in a system of algebraic equations. \n",
    "2. mathematical problem \n",
    "   1. Transcendental equations.\n",
    "   2. Polynomials of degree 5 or higher do not have solution formula. (Galois and Abel)\n",
    "   3. We often need only approximate zeros to even polynomials of degree 3 or 4. And their formula are complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Remark** (Aside: Cubic and quartic formula)\n",
    "\n",
    "- In regard to the first reason, even cubic (degree 3) or quartic (degree 4) equations are complicated \n",
    "- Also, they involve unusual numerical procedures: complex numbers appear even when there is only one real simple root.\n",
    "\n",
    "Solution for $ax^3+bx^2+cx+d=0$:\n",
    "\n",
    "![Cubic formula](https://www.curtisbright.com/quartic/cubic.png)\n",
    "\n",
    "Solution for $ax^4 + bx^3 + cx^2 + dx + e = 0$:\n",
    "\n",
    "![Quartic formula](https://www.curtisbright.com/quartic/quartic.png)\n",
    "\n",
    "Figures: https://www.curtisbright.com/quartic/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Algorithm** (Newton's method or Newton-Raphson's method)\n",
    ">\n",
    "> Given a differentiable function $f:\\mathbb{R}\\to\\mathbb{R}$ and an initial guess $x_0\\in\\mathbb{R}$, compute, for $i\\ge 0$,\n",
    ">\n",
    "> $$ x_{i+1} = x_i - \\frac{f(x_i)}{f'(x_i)}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Geometric intuition\n",
    "\n",
    "![Newton's method: Still illustration](https://math24.net/images/newtons-method1.svg)\n",
    "\n",
    "Figure: https://math24.net/newtons-method.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Derivation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Refresher: Geometric understanding of Taylor series)\n",
    "\n",
    "![Power series as a function approximation](https://suzyahyah.github.io/assets/Calculus-taylor.png)\n",
    "\n",
    "[Interactive Geogebra by Guillermo Bautista: $e^x$](https://www.geogebra.org/m/u25naf28)\n",
    "\n",
    "<!-- [Interactive Geogebra by matheagle: $ln(1 + x^2) + sin(3x)$](https://www.geogebra.org/m/YtnuMjEF)\n",
    "\n",
    "Geogebra activity suggestion\n",
    "- Type in a simple function such as $e^x$, $1/(1-x)$, $\\sin(x)$, etc.\n",
    "- Move the slide bar to vary $n$.  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In words,\n",
    "\n",
    "- Partial sums behave more and more similarly to the specific function, in this case $e^x$: \n",
    "    - $1$, \n",
    "    - $1+x$, \n",
    "    - $1+x+\\frac{x^2}{2!}$, \n",
    "    - $1+x+\\frac{x^2}{2!}+\\frac{x^3}{3!}$, \n",
    "    - $\\vdots$\n",
    "- We can imagine that if we add infinitely many of them, that will behave exactly the same as $e^x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Taylor's theorem)\n",
    "\n",
    "Let $x$ and $x_0$ be real numbers, and let $f$ be $k + 1$ times continuously differentiable on the interval between $x$ and $x_0$. Then there exists a number $c$ between $x$ and $x_0$ such that\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "f(x) &= f(x_0) + f'(x_0)(x-x_0)+ \\frac{f''(x_0)}{2!}(x-x_0)^2+\\cdots\n",
    "\\\\\n",
    "&\\quad +\\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k +\\frac{f^{(k+1)}(c)}{(k+1)!}(x-x_0)^{k+1}.\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof** \n",
    "\n",
    "Notation: In the proof, $\\xi$ plays the role of $c$.\n",
    "\n",
    "[proof of Taylor's theorem with Lagrange remainder 1](https://jhparkyb.github.io/resources/notes/na/pf_TaylorThmLag1_lp3000.png)\n",
    "\n",
    "[proof of Taylor's theorem with Lagrange remainder 2](https://jhparkyb.github.io/resources/notes/na/pf_TaylorThmLag2_lp3001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corollary** (Taylor's theorem - linear approximation version; $k=1$)\n",
    "\n",
    "Let $x$ and $x_0$ be real numbers, and let $f$ be twice continuously differentiable on the interval between $x$ and $x_0$. Then there exists a number $c$ between $x$ and $x_0$ such that\n",
    "\n",
    "$$\n",
    "f(x) = f(x_0) + f'(x_0)(x-x_0)+ \\frac{f''(c)}{2!}(x-x_0)^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Derivation of Newton's method using Taylor theorem**\n",
    "\n",
    "1. Let $x$ be a root, i.e., $f(x)=0$. \n",
    "2. Expand $f(x)$ around the current position, say, $x_i$. \n",
    "3. Take the linear approximation, namely, ignore the second order term or higher.\n",
    "4. Solve for $x$, and call it $x_{i+1}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Derivation of Newton's method](https://jhparkyb.github.io/resources/notes/na/der_NewtonMethodTaylor_lp2000.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Remark**\n",
    "\n",
    "- (In step 1) Pretending to know the solution is often start of a magic.\n",
    "- (In step 2) What about the other way around?\n",
    "- (In step 3) What did we lose and what did we obtain? \n",
    "\n",
    "<!-- ![Derivation of Newton's method](https://jhparkyb.github.io/resources/notes/na/104ASlides_RootFinding014.png) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computational example (computing $\\sqrt{2}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Problem** (Computing $\\sqrt{2}$)\n",
    ">\n",
    "> Write a code that computes approximate value of $\\sqrt{2}$ using Newton's method.\n",
    "\n",
    "(Step 1) Cast the problem as a root finding problem and summarize it. (Intellectual work needed for $f$ and $f'$)\n",
    "\n",
    "(Step 2) Write a (programming) function that implements Newton's method.\n",
    "\n",
    "(Step 3) Set up the computation (function, initial guess, etc.) and implement it.\n",
    "\n",
    "(Step 4) Post-process and report the result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- True solution can be obtained from [Wolfram alpha: N[sqrt[2], 20]](https://www.wolframalpha.com/input?i=N%5Bsqrt%5B2%5D%2C+20%5D).\n",
    "  - `N[sqrt[2], 20]` (Numerical value of $\\sqrt{2}$ up to 20 decimal digit) gives us 1.4142135623730950488.\n",
    "- 20 decimal digits are enough because computers can distinguish only up to around $2^{−52} \\approx 2.22\\times 10^{-16}$ when they use floating point arithmetic.\n",
    "  - This number is called *machine epsilon*.\n",
    "  - Machine epsilon depends on data type. (See [Wikipedia](https://en.wikipedia.org/wiki/Machine_epsilon) page for details)\n",
    "  - Wolfram alpha can handle higher precision by using more computing resources than floating point arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def newton(f, fp, ini, tol=1e-8, max_iter=20):\n",
    "    \"\"\"\n",
    "    Return an approximate root of a function using Newton's method.\n",
    "\n",
    "    INPUT\n",
    "        f: function whose zero is sought.\n",
    "        fp: derivative of f (name from 'f prime')\n",
    "        ini: initial guess\n",
    "        tol: tolerance for stopping criterion. If consecutive iterates differ by less than this, it is considered convergenct.\n",
    "        max_iter: maximum number of iterations\n",
    "    OUTPU\n",
    "        approximated zero and the number of iterations. When the maximum number of iterations is reached, the last iterate with a warning message.\n",
    "    \"\"\"\n",
    "    x = ini\n",
    "    for i in range(max_iter):\n",
    "        x_pre = x\n",
    "        x = x - f(x)/fp(x)\n",
    "\n",
    "        if np.abs(x - x_pre) < tol: \n",
    "            break\n",
    "    \"\"\"\n",
    "    if i == max_iter - 1:\n",
    "        print(\"   Warning (newton): maximum number of iteration reached.\\n     --> The output may not be close enough to the zero.\")\n",
    "    \"\"\"\n",
    "    return x, i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton's method :  1.4142135623730951    (8 iterations taken)\n",
      "True solution   :  1.4142135623730951\n",
      "Error           :  0.0\n"
     ]
    }
   ],
   "source": [
    "# Problem settings\n",
    "f = lambda x: x*x - 2.\n",
    "fp = lambda x: 2.*x\n",
    "\n",
    "x0 = 10.\n",
    "\n",
    "# Numerical settings\n",
    "max_iter = 100\n",
    "\n",
    "appr, iter = newton(f, fp, x0, max_iter=max_iter)\n",
    "sol = 1.4142135623730950488 # obtained from Wolfram Alpha\n",
    "err = np.abs(appr - sol)\n",
    "\n",
    "print(\"Newton's method : \", appr, f\"   ({iter} iterations taken)\")\n",
    "print(\"True solution   : \", sol)\n",
    "print(\"Error           : \", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- Convergence of the Newton's method is not guaranteed. $\\longrightarrow$ local convergence.\n",
    "- If convergent, it is very fast. $\\longrightarrow$ quadratic convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Terminology** (local convergence)\n",
    "\n",
    "An iterative method is called locally convergent to $r$ if the method converges to $r$ for initial guesses sufficiently close to $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Definition** (Quadradic convergence) \n",
    "\n",
    "Let $e_i$ denote the error after step $i$ of an iterative method. The iteration is quadratically convergent if\n",
    "\n",
    "$$\n",
    "M=\\lim _{i \\rightarrow \\infty} \\frac{e_{i+1}}{e_i^2}<\\infty.\n",
    "$$\n",
    "\n",
    "In words, it means *errors get shrunken by a square of the previous error*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Theorem** (Local, quadratic convergence of Newton's method)\n",
    "\n",
    "Let $f$ be twice continuously differentiable and $f (r) = 0$. If $f'(r) \\neq 0$, then Newton’s Method is locally and quadratically convergent to $r$. The error $e_i$ at step $i$ satisfies\n",
    "\n",
    "$$\n",
    "M=\\lim _{i \\rightarrow \\infty} \\frac{e_{i+1}}{e_i^2}<\\infty,\n",
    "$$\n",
    "\n",
    "where $M= f''(r)/(2 f'(r))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question** (How fast is a quadratic convergence?)\n",
    ">\n",
    "> Suppose Newton's method starts to manifest quadratic converge from 5th iteration with $e_5 = 0.1$. Guess what will be the error after three more iterations? For simplicity, assume $M=1$. It is more fun to guess without thinking much.\n",
    ">\n",
    "> (Reminder) This is **about atmosphere**, not getting it right.\n",
    "> \n",
    "> 1. Think for a short time.\n",
    "> 2. Share your guess with your pair.\n",
    "> 3. Feel free to say out loud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- Revisit computation of $\\sqrt 2$, and see if it shows quadratic convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\\# iterations</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.685786e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.331865e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.229813e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.002453e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.120928e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.442917e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    \\# iterations         error\n",
       "1             1.0  3.685786e+00\n",
       "2             2.0  1.331865e+00\n",
       "3             3.0  3.229813e-01\n",
       "4             4.0  3.002453e-02\n",
       "5             5.0  3.120928e-04\n",
       "6             6.0  3.442917e-08\n",
       "7             7.0  2.220446e-16\n",
       "8             8.0  0.000000e+00\n",
       "9             8.0  0.000000e+00\n",
       "10            8.0  0.000000e+00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "N = 10\n",
    "df = pd.DataFrame(columns=['\\# iterations', 'error'])\n",
    "\n",
    "for i in range(1, N+1):\n",
    "    appr, iter = newton(f, fp, x0, max_iter=i)\n",
    "    err = np.abs(appr - sol)\n",
    "    df.loc[i] = [iter, appr - sol]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "- If this did not surprise you, look at the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"This example shows how computer arithmetic can be limited by the precision of floating point numbers. The output must be 1 from mathematical point of view.\n",
    "\n",
    "    Suggested parameter\n",
    "        N = 10, 100, 170, 171\n",
    "\"\"\"\n",
    "\n",
    "N = 171\n",
    "\n",
    "prod = 1.\n",
    "\n",
    "for i in range(1, N+1):\n",
    "    prod = prod * i\n",
    "\n",
    "for i in range(1, N+1):\n",
    "    prod = prod / i\n",
    "\n",
    "print(prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: What to be careful of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- $f'$ must exists and $f'(x_i)\\neq 0$.\n",
    "- Newton's method may diverge. \n",
    "- $f'(r)=0$ (Multiple roots) slow down to a linear convergence.\n",
    "  - Modified Newton's method recovers quadratic convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $f'$ must be available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** (Divergence of Newton's method)\n",
    "\n",
    "- Newton's method may diverge while it converges fast if it does. (i.e., only locally convergent)\n",
    "- If it happens to be $f'(x_i)=0$, the method breaks down.\n",
    "\n",
    "<!-- | | | |\n",
    "|---|---|---|\n",
    "|![Divergence of Newton's method](https://amsi.org.au/ESA_Senior_Years/imageSenior/2a_numerical_methods_graph_7.png) | ![Oscillation of Newton's method](https://i.stack.imgur.com/yPC4a.png)| ![Failure of Newton's method](https://mmerevise.co.uk/app/uploads/2021/07/Method-Fail-2-e1650551922512.png.webp) | -->\n",
    "\n",
    "![Divergence of Newton's method](https://amsi.org.au/ESA_Senior_Years/imageSenior/2a_numerical_methods_graph_7.png) \n",
    "\n",
    "![Oscillation of Newton's method](https://i.stack.imgur.com/yPC4a.png)\n",
    "\n",
    "![Failure of Newton's method](https://mmerevise.co.uk/app/uploads/2021/07/Method-Fail-2-e1650551922512.png.webp)\n",
    "\n",
    "\n",
    "Figure: https://amsi.org.au/, StackExchange, MME\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "- Local convergence can still be useful.\n",
    "  - In applications of time evolution, next state can be formulated as a solution to an equation.\n",
    "  - Previous state serves as a good initial guess for the next state.\n",
    "\n",
    "![Time evolution: Phase field crystal equation](https://www.mdpi.com/crystals/crystals-12-01271/article_deploy/html/images/crystals-12-01271-g006-550.jpg)\n",
    "\n",
    "Figure: MDPI (Crystal phase field morphology; an example of time evolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $f'(r)=0$ slows down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Slow down of Newton's method)\n",
    "\n",
    "Implement Newton's method for $x^2=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda x: x*x\n",
    "gp = lambda x: 2*x\n",
    "\n",
    "sol = 0.\n",
    "\n",
    "x0 = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\\# iterations</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.156250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.039062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.019531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.009766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    \\# iterations     error\n",
       "1             1.0  5.000000\n",
       "2             2.0  2.500000\n",
       "3             3.0  1.250000\n",
       "4             4.0  0.625000\n",
       "5             5.0  0.312500\n",
       "6             6.0  0.156250\n",
       "7             7.0  0.078125\n",
       "8             8.0  0.039062\n",
       "9             9.0  0.019531\n",
       "10           10.0  0.009766"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "N = 10\n",
    "df = pd.DataFrame(columns=['\\# iterations', 'error'])\n",
    "\n",
    "for i in range(1, N+1):\n",
    "    appr, iter = newton(g, gp, x0, max_iter=i)\n",
    "    err = np.abs(appr - sol)\n",
    "    df.loc[i] = [iter, appr - sol]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is happening?**\n",
    "\n",
    "Geogebra interactive module: suggested settings\n",
    "\n",
    "1. x^2 - 2\n",
    "2. x^2\n",
    "\n",
    "[Newton's method: Geogebra interactive module](https://www.geogebra.org/m/n6KXp4hE)\n",
    "\n",
    "Creator: Lenore Horner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematics makes it clearer.\n",
    "\n",
    "- $g(x)=x^2$\n",
    "- $g'(x)=2x$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_{i+1} & =x_i-\\frac{f\\left(x_i\\right)}{f^{\\prime}\\left(x_i\\right)} \\\\\n",
    "& =x_i-\\frac{x_i^2}{2 x_i} \\\\\n",
    "& =\\frac{x_i}{2}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Linear convergence for multiple roots)\n",
    "\n",
    "Assume that the $(m + 1)$-times continuously differentiable function $f$ on $[a,b]$ has a multiplicity $m$ root at $r$. Then Newton’s Method is locally convergent to $r$, and the error $e_i$ at step $i$ satisfies\n",
    "\n",
    "$$\n",
    "\\lim _{i \\rightarrow \\infty} \\frac{e_{i+1}}{e_i}=S,\n",
    "$$\n",
    "\n",
    "where $S=(m-1)/m$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remedy: Modified Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** (Modified Newton's method)\n",
    "\n",
    "If $f$ is $(m + 1)$-times continuously differentiable on $[a,b]$, which contains a root $r$ of multiplicity $m > 1$, then *Modified Newton’s Method* \n",
    "\n",
    "$$\n",
    "x_{i+1}=x_i-\\frac{m f\\left(x_i\\right)}{f^{\\prime}\\left(x_i\\right)}\n",
    "$$\n",
    "\n",
    "converges locally and quadratically to $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secant method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation**\n",
    "\n",
    "Newton's method is great. But it requires $f'(x)$ as well as $f(x)$.\n",
    "\n",
    "How can we overcome this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method\n",
    "\n",
    "**Idea**: Replace $f'(x_i)$ in Newton's method with something similar.\n",
    "\n",
    "**Geometric intuition**\n",
    "\n",
    "![Secant method](https://mathworld.wolfram.com/images/eps-svg/SecantMethod_800.svg)\n",
    "\n",
    "Figure: Wolfram MathWorld.\n",
    "\n",
    "[Secant method: Geogebra interactive module](https://www.geogebra.org/m/vpk4geyu)\n",
    "\n",
    "Author: Marian Choy\n",
    "\n",
    "\n",
    "> ***Algorithm*** (Secant method)\n",
    ">\n",
    "> Given $x_0, x_1\\in\\mathbb{R}$, compute, for $i\\ge 1$,\n",
    ">\n",
    "> $$ x_{i+1}=x_{i}-f\\left(x_{i}\\right)\\frac{\\left(x_{i}-x_{i-1}\\right)}{f\\left(x_{i}\\right)-f\\left(x_{i-1}\\right)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "- If the secant method converges, its rate of convergence is the *golden ratio* ($\\approx 1.618$).\n",
    "- User must feed **two initial guesses**.\n",
    "- It requires **only the function evaluation**, but not the derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis\n",
    "\n",
    "In favor of more computational activities, we skip the proof the *superlinear* convergence (i.e., a convergence rate that is faster the linear: $e_{k+1} \\approx C e_k^\\alpha$ with $\\alpha>1$) of the secant method.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed point iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method\n",
    "\n",
    "**Terminology**\n",
    "\n",
    "It is also called *Picard iteration* or *functional iteration*.\n",
    "\n",
    "**Geometric interpretation**\n",
    "\n",
    "\"A picture paints a thousand words.\" \n",
    "\n",
    "![Fixed point iteration](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Cosine_fixed_point.svg/1920px-Cosine_fixed_point.svg.png)\n",
    "\n",
    "Figure: Wikipedia\n",
    "\n",
    "[Fixed point iteration: Geogebra interactive module](https://www.geogebra.org/m/qUbg7Z6W) \n",
    "\n",
    "Author: stuart.cork\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Algorithm** (Fixed point iteration - general)\n",
    ">\n",
    "> Given a function $f:\\mathbb{R}\\to\\mathbb{R}$ and an initial guess $x_0\\in\\mathbb{R}$, compute, for $i\\ge 0$,\n",
    ">\n",
    "> $$ x_{i+1} = f(x_i). $$\n",
    "\n",
    "> **Algorithm** (Fixed point iteration - root finding for $f$)\n",
    ">\n",
    "> Given a function $f:\\mathbb{R}\\to\\mathbb{R}$ and an initial guess $x_0\\in\\mathbb{R}$, set $g(x)=x+f(x)$, compute, for $i\\ge 0$, \n",
    ">\n",
    "> $$ x_{i+1} = g(x_i). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "- If repeated applications of a function $g$ converges to $x$, then it solves $x=g(x)$. (Some condition on $f$ is needed: see Analysis below.)\n",
    "- If converges, the fixed point iteration method converges *linearly*: there exists $C>0$ such that $e_{k+1}\\approx \\lambda e_k$ with $0<\\lambda<1$. \n",
    "- If you want to solve the equation $f(x)=0$, set $g(x):=x+f(x)$ and apply the fixed point iteration to $g$. Then, the fixed point $x$ satisfies \n",
    "    $$x = g(x)=x+f(x) \\quad \\text{implies} \\quad f(x)=0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition** (Fixed point)\n",
    "> $x$ is called a *fixed point* of the function $g$ if $g(x)=x$.\n",
    "\n",
    "\n",
    "> **Definition** (Contractive/Contraction mapping)\n",
    "> A function $g:D \\to \\mathbb{R}$ is called *contractive* or a *contractive mapping/contraction* if there is $\\lambda\\in[0,1)$ such that $|g(x)-g(y)|\\le \\lambda|x-y|$ for all $x,y\\in D$.\n",
    "\n",
    "> **Theorem** (Contraction mapping is continuous)\n",
    "> If $g:D\\to \\mathbb{R}$ is contractive, it is continuous.\n",
    "\n",
    "> **Theorem** (Absolute convergence implies convergence)\n",
    "> If $\\sum_{i=1}^\\infty x_i$ is absolutely convergent, i.e., $\\sum_{i=1}^\\infty |x_i| < \\infty$, then $\\sum_{i=1}^\\infty x_i$ also converges.\n",
    "\n",
    "> **Theorem** (Contraction Mapping Theorem)\n",
    "> Let $D$ be a closed subset of $\\mathbb{R}$. If $g:D \\to D$ is a contraction, then it has a unique fixed point. Moreover, this fixed point is the limit of the functional iteration starting with any initial guess.\n",
    "\n",
    "[Proof of contraction mapping theorem 1](https://jhparkyb.github.io/resources/notes/na/pf_ContractionMappingThm1_lp3000.png)\n",
    "\n",
    "[Proof of contraction mapping theorem 2](https://jhparkyb.github.io/resources/notes/na/pf_ContractionMappingThm2_lp3001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof outline\n",
    "\n",
    "1. $x_i= (x_i - x_{i-1}) + (x_{i-1} - x_{i-2}) + \\cdots + (x_{1} - x_{0}) + x_0$ absolutely converges, hence converges.\n",
    "   - $|x_i - x_{i-1}| \\le \\lambda^{i-1} |x_1 - x_0|$\n",
    "2. Pass $x_{i+1}=g(x_i)$ to the limit $i\\to \\infty$.\n",
    "3. Uniqueness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Question** \n",
    ">\n",
    "> The above proof outline did not use one condition and used another condition implicitly. What are they? \n",
    ">\n",
    "> (Reminder) This is **about atmosphere**, not getting it right.\n",
    "> \n",
    "> 1. Think for a short time.\n",
    "> 2. Share your guess with your pair.\n",
    "> 3. Type your answer in clicker.\n",
    "> 4. Feel free to say out loud.\n",
    "\n",
    "(Homework questions will ask you what happens if you ignore them.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparisons of root-finding methods\n",
    "\n",
    "| | Bisection | Newton | Secant | Fixed point |\n",
    "|---|---|---|---|---|\n",
    "| need $f(x)$ | O | O | O | O |\n",
    "| need $f'(x)$ | - | O | - | - |\n",
    "| rate of convergence | 1 | 2 | 1.618 | 1 |\n",
    "| rate of convergence <br> per two function eval's | 1 <br> (with smaller contraction constant) | 2 | $1.618^2\\approx 2.618$ | 1 <br> (with smaller contraction constant) |\n",
    "| global convergence | yes <br> if $f(a)f(b)<0$ | no | no | practially no |\n",
    "| solution boxed | yes | no | no | generally, no |  \n",
    "| generalization <br> to high dimensions <br> (intellectual effort) | awkward | yes, <br> but gradient may be unavailable  | yes, <br> but not very trival <br> (called quasi-Newton methods)| yes |\n",
    "| generalization <br> to high dimensions <br> (numerical aspects) | N/A | demanding | depends | depends |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Styles of algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Mathematics- or idea-oriented pseudo-algorithm | Coding-oriented pseudo-algorithm |\n",
    "|---|---|\n",
    "|Given an initial guess $x_0$, <br> compute <br> $ x_{i+1} = x_i - \\frac{f(x_i)}{f'(x_i)}$ for $i\\ge 0$. | Input (or Data): $x_0$, $f$, $f'$ <br> Set: $Tol>0$, $x \\gets x_0$ <br> While $\\|x - x_{pre}\\| > Tol$: <br> $ \\quad \\quad x_{pre} \\gets x $ <br> $\\quad \\quad x \\gets x - \\frac{f(x)}{f'(x)}$ |\n",
    "| Focus on the essence | Also consider some details in implementation. In particular, this usually includes *stopping criteria*. |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History of Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In favor of more computational activities, we skip the proof the quadratic convergence of Newton's method. But since Newton's method is highly relevant even these days and since it inspires many other methods, we include some history about it.\n",
    "\n",
    "> **Historical note**\n",
    ">\n",
    "> 1. Babylonians (1894 BC - 539 BC) used the method to approximate square roots: $\\sqrt{2}$ accurately up to seventh decimal place. (Ref: [2, 3])\n",
    ">       ![Babylonian clay tablet](https://projectlovelace.net/static_prod/img/YBC7289.jpg)\n",
    ">\n",
    ">       Figure: Project Lovelace\n",
    "> 1. In 1669, the method was employed by Newton for the cubic equation $3x^3 -2x-5 = 0$. (Ref: [1])\n",
    "> 1. In 1690, Raphson described the method for a general cubic equation $x^3 — bx = c$. (Ref: [1])\n",
    "> 1. In 1818, Fourier proved the quadratic convergence of the method. (Ref: [1])\n",
    "> 1. In 1829, Cauchy proved a convergence theorem which does not assume the existence of a solution. (existence of a solution is a consequence; but it assumes some other conditions on the iterates) (Ref: [1])\n",
    "> 1. In 1939, Kantorovich proved a convergence theorem in a very general setting. (Ref: [1])\n",
    "> 1. In 1948, Kantorovich proved an improved version, which is now called Kantorovich's theorem or the Newton-Kantorovich theorem: existence of a solution is not assumed and the convergence is quadratic in a very general setting. (Ref: [1])\n",
    "> \n",
    "> Reference\n",
    "> \n",
    "> [1] Brezinski (2001) Numerical Analysis: Historical Developments in the 20th Century. p. 242\n",
    "> \n",
    "> [2] Sauer (2017) Numerical Analysis p. 41\n",
    "> \n",
    "> [3] Wikipedia (Babylonia) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wilkinson's polonomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example** (Wilkinson's polynomial)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(x) & =\\prod_{i=1}^{20}(x-i)=(x-1)(x-2) \\cdots(x-20) \\\\\n",
    "& =x^{20}-210 x^{19}+20615 x^{18}+\\cdots+2432902008176640000\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15324.993485094896\n",
      "0.9999999999998672\n"
     ]
    }
   ],
   "source": [
    "wilkinson_coeff= np.array([    \n",
    "                     1.,\n",
    "                  -210.,\n",
    "                 20615.,\n",
    "              -1256850.,\n",
    "              53327946.,\n",
    "           -1672280820.,\n",
    "           40171771630.,\n",
    "         -756111184500.,\n",
    "        11310276995381.,\n",
    "      -135585182899530.,\n",
    "      1307535010540395.,\n",
    "    -10142299865511450.,\n",
    "     63030812099294896.,\n",
    "   -311333643161390656.,\n",
    "   1206647803780373248.,\n",
    "  -3599979517947607040.,\n",
    "   8037811822645051392.,\n",
    " -12870931245150988288.,\n",
    "  13803759753640704000.,\n",
    "  -8752948036761600000.,\n",
    "   2432902008176640000.])\n",
    "\n",
    "appr_roots = np.roots(wilkinson_coeff)\n",
    "\n",
    "def poly_eval(coeff, x):\n",
    "    \"\"\"\n",
    "    Return the value of Wilkinson's polynomial at x.\n",
    "    \"\"\"\n",
    "    n = len(coeff)\n",
    "    sum = 0.\n",
    "    for i in range(n):\n",
    "        sum += coeff[n-1-i]*x**i\n",
    "    return sum\n",
    "\n",
    "print(poly_eval(wilkinson_coeff, appr_roots[19]))\n",
    "print(appr_roots[19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "This work is licensed under [Creative Commons Attribution-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-sa/4.0/)\n",
    "Part of the content of this notebook is borrowed from [Elementary Numerical Analysis (with Python)](https://lemesurierb.people.cofc.edu/elementary-numerical-analysis-python/preface.html) written by Brenton LeMesurier, College of Charleston and University of Northern Colorado. Thanks to Dr. LeMesurier for sharing excellent notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \n",
    "[proof of contraction mapping theorem 1](https://jhparkyb.github.io/resources/notes/na/104ABoardWork_RootFinding015.png)\n",
    "\n",
    "[proof of contraction mapping theorem 2](https://jhparkyb.github.io/resources/notes/na/104ABoardWork_RootFinding016.png)\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptrch",
   "language": "python",
   "name": "ptrch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
